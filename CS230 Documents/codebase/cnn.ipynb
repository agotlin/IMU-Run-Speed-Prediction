{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LUMO - Deep Convolutional Network in TensorFlow\n",
    "\n",
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "import socket\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add model-specific naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user input for filename of saved model, defaults to timestamp\n",
    "name = input(\"String to add to model filename, defaults to time stamp if nothing entered.\")  # Python 3\n",
    "if name == \"\":\n",
    "    name = time.time()\n",
    "#user input to add note to plot\n",
    "plot_note=input(\"Note you'd like to add to plot:\")\n",
    "\n",
    "#user input to load prev model\n",
    "model_to_load=input(\"enter the model name to load - leave blank to start fresh\")\n",
    "\n",
    "#user input to load prev model\n",
    "results_file_name=input(\"name of results file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# File setup                                               #\n",
    "############################################################\n",
    "\n",
    "#Set to true to report on hold out test set\n",
    "FINALTEST=True\n",
    "\n",
    "# Location of file containing CSV of training, test instances\n",
    "myFileLocation='../datasets/big-mod.csv'\n",
    "\n",
    "# Number of epochs between each printed update\n",
    "checkin_frequency =  10\n",
    "\n",
    "############################################################\n",
    "# Hyperparameters                                          #\n",
    "############################################################\n",
    "\n",
    "batch_size = 50\n",
    "kernel_size = 6\n",
    "depth = 40\n",
    "num_hidden = 50\n",
    "reportingMetric_1=\"ACC\" # options are \"MAE\", \"MSE\", \"MAPE\", \"ACC\", \"ACC1\", \"ACC2\"\n",
    "\n",
    "reportingMetric_2=\"ACC2\" # options are \"MAE\", \"MSE\", \"MAPE\", \"ACC\",\"ACC1\", \"ACC2\"\n",
    "\n",
    "reportingMetric_3=\"MAE\" # options are \"MAE\", \"MSE\", \"MAPE\", \"ACC\",\"ACC1\", \"ACC2\"\n",
    "\n",
    "reportingMetric_4=\"\" # options are \"MAE\", \"MSE\", \"MAPE\", \"ACC\",\"ACC1\", \"ACC2\"\n",
    "\n",
    "################Hyperparameters\n",
    "sample_stride=18\n",
    "input_height = 1\n",
    "input_num_timestamps = 36\n",
    "num_channels = 11\n",
    "max_pool_kernel_size=6\n",
    "\n",
    "speed_bucket_size = \"0.1\" #options are \".1\" , \".5\", \"none_use_regression\"\n",
    "label_window_size = 30 # number of timestamps used to label the speed we will be predicting\n",
    "\n",
    "loss_function = \"cross_entropy\" #options are: \"L1_loss\" , \"cross_entropy\" , \"L2_loss\"\n",
    "optimizer_type = \"gradient\" #options are: \"adam\" , \"rmsprop\", \"gradient\"\n",
    "\n",
    "activation_1 = \"relu\" #options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "activation_2 = \"tanh\" #options are \"relu\" , \"tanh\" and \"sigmoid\" - used for final FC layer\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthConvActivation(x,weights,biases):\n",
    "    if activation_1==\"relu\":\n",
    "        result=tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases)) \n",
    "        \n",
    "    elif activation_1==\"tanh\":\n",
    "        result=tf.nn.tanh(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "        \n",
    "    elif activation_1==\"sigmoid\":\n",
    "        result=tf.nn.sigmoid(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    return result\n",
    "\n",
    "def FCActivation(c_flat,f_weights_l1,f_biases_l1):\n",
    "    if activation_2==\"relu\":\n",
    "        result=tf.nn.relu(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "        \n",
    "    elif activation_2==\"tanh\":\n",
    "        result=tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "        \n",
    "    elif activation_2==\"sigmoid\":\n",
    "        result=tf.nn.sigmoid(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def load_results_file(results_file_name):\n",
    "    my_file = Path(\"../results/\"+results_file_name+\".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[\"cnn\",\n",
    "            name,\n",
    "            \"plot_filename\",\n",
    "            myFileLocation,\n",
    "            loss_function,\n",
    "            optimizer_type,\n",
    "            activation_1,\n",
    "            activation_2,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,    \n",
    "            training_epochs,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            batch_size,\n",
    "            kernel_size,\n",
    "            depth,\n",
    "            num_hidden,\n",
    "            max_pool_kernel_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_num_timestamps,\n",
    "            checkin_frequency,\n",
    "            plot_note,\n",
    "            \"none\",\n",
    "            results_file_name,\n",
    "            num_channels,\n",
    "            label_window_size,\n",
    "            sample_stride\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"loss function\",\n",
    "                                    \"optimizer\",\n",
    "                                    \"activation_1\",\n",
    "                                    \"activation_2\",\n",
    "                                    \"last_train_accuracy_1_bucket\",\n",
    "                                    \"last_dev_accuracy_1_bucket\",\n",
    "                                    \"last_train_accuracy_2_buckets\",\n",
    "                                    \"last_dev_accuracy_2_buckets\",\n",
    "                                    \"last_train_MSE\",\n",
    "                                    \"last_dev_MSE\",\n",
    "                                    \"last_train_MAE\",\n",
    "                                    \"last_dev_MAE\",\n",
    "                                    \"last_train_MAPE\",\n",
    "                                    \"last_dev_MAPE\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy\",\n",
    "                                    \"train accuracy\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"kernel_size\",\n",
    "                                    \"depth\",\n",
    "                                    \"num_hidden\",\n",
    "                                    \"max_pool_kernel_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_num_timestamps\",\n",
    "                                    \"checkin_frequency\",\n",
    "                                    \"plot_note\",\n",
    "                                    \"model_loaded\",\n",
    "                                    \"results_file_name\",\n",
    "                                    \"num_channels\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"sample_stride\"])\n",
    "        \n",
    "        df.to_csv(\"../results/\"+results_file_name+\".csv\",index=False )\n",
    "        return df\n",
    "    \n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path,header = 0)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma\n",
    "    \n",
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "    \n",
    "def plot_activity(activity,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['bounce'], 'bounce')\n",
    "    plot_axis(ax1, data['timestamp'], data['braking'], 'braking')\n",
    "    plot_axis(ax2, data['timestamp'], data['pelvic_tilt'], 'pelvic_tilt')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride #(size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = input_num_timestamps):\n",
    "    segments = np.empty((0,window_size,num_channels))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        #print(start)\n",
    "        #print(end)\n",
    "        aa = data[\"age\"][start:end]\n",
    "        bb = data[\"weight\"][start:end]\n",
    "        cc = data[\"height\"][start:end]\n",
    "        dd = data[\"gender\"][start:end]\n",
    "        a = data[\"bounce\"][start:end]\n",
    "        b = data[\"braking\"][start:end]\n",
    "        c = data[\"cadence\"][start:end]\n",
    "        d = data[\"ground_contact\"][start:end]\n",
    "        e = data[\"pelvic_drop\"][start:end]\n",
    "        f = data[\"pelvic_rotation\"][start:end]\n",
    "        g = data[\"pelvic_tilt\"][start:end]\n",
    "        if(end < data.shape[0] and\n",
    "           len(data['timestamp'][start:end]) == window_size and\n",
    "           data['activity_id'][start]==data['activity_id'][end]): \n",
    "            \n",
    "            segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "            \n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            \n",
    "            if speed_bucket_size == \"0.1\":\n",
    "                 labels = np.append(labels,np.around(np.mean(data[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) \n",
    "                    # round to nearest decimal\n",
    "            elif speed_bucket_size == \"0.5\":\n",
    "                 labels = np.append(labels,np.around(2*np.mean(data[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) \n",
    "            elif speed_bucket_size == \"none_use_regression\":\n",
    "                 labels = np.append(labels,np.mean(data[\"gps_speed_true\"][start_labeling:end_labeling]))\n",
    "                    # round to nearest half unit\n",
    "            \n",
    "    num_labels = len(np.unique(labels))\n",
    "    return segments, labels, num_labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    result=depthConvActivation(x,weights,biases)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from CSV into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results of previously run models from file address specified in load_results_file()\n",
    "past_results=load_results_file(results_file_name)\n",
    "print(past_results)\n",
    "\n",
    "# Read and normalize training/test instances\n",
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "dataset['weight'] = feature_normalize(dataset['weight'])\n",
    "dataset['height'] = feature_normalize(dataset['height'])\n",
    "dataset['bounce'] = feature_normalize(dataset['bounce'])\n",
    "dataset['gender'] = feature_normalize(dataset['gender'])\n",
    "dataset['age'] = feature_normalize(dataset['age'])\n",
    "dataset['braking'] = feature_normalize(dataset['braking'])\n",
    "dataset['cadence'] = feature_normalize(dataset['cadence'])\n",
    "dataset['ground_contact'] = feature_normalize(dataset['ground_contact'])\n",
    "dataset['pelvic_drop'] = feature_normalize(dataset['pelvic_drop'])\n",
    "dataset['pelvic_rotation'] = feature_normalize(dataset['pelvic_rotation'])\n",
    "dataset['pelvic_tilt'] = feature_normalize(dataset['pelvic_tilt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape individual data points into windowed training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels, num_labels = segment_signal(dataset)\n",
    "\n",
    "labels_as_numbers = np.asarray(np.unique(labels), dtype = np.float32)\n",
    "labels_as_numbers = np.reshape(labels_as_numbers, (len(labels_as_numbers), 1))\n",
    "labels_as_one_hots = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "\n",
    "reshaped_segments = segments.reshape(len(segments), 1,input_num_timestamps, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train (90%) and dev (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.90\n",
    "print(train_test_split)\n",
    "\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels_as_one_hots[train_test_split]\n",
    "\n",
    "test_dev_x = reshaped_segments[~train_test_split]\n",
    "test_dev_y = labels_as_one_hots[~train_test_split]\n",
    "\n",
    "test_dev_split=np.random.rand(len(test_dev_x)) <.5\n",
    "\n",
    "# In our class, this should be called dev, not test, but use \"test\" for consistency with other code-bases\n",
    "test_x=test_dev_x[test_dev_split]\n",
    "test_y=test_dev_y[test_dev_split]\n",
    "\n",
    "\n",
    "hold_out_test_x=test_dev_x[~test_dev_split]\n",
    "hold_out_test_y=test_dev_y[~test_dev_split]\n",
    "\n",
    "print(\"train x shape:\",train_x.shape)\n",
    "print(\"test x shape:\", test_x.shape)\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TensorFlow objects and model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_num_timestamps,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "##Layer 1 - Depthwise Convolution\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "\n",
    "## Max Pool\n",
    "p = apply_max_pool(c,max_pool_kernel_size,2)\n",
    "\n",
    "##Layer 2 - Depthwise Convolution\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "##Flatten for FC layers \n",
    "shape = c.get_shape().as_list()\n",
    "print(\"SHAPE:\",shape)\n",
    "\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "##Initiate hidden layers weights/biases based on hyperparameter entered\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "\n",
    "## run num_hidden FC layers \n",
    "f = FCActivation(c_flat,f_weights_l1,f_biases_l1)\n",
    "\n",
    "##Final Output weights/biases\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "\n",
    "## Final Layer - Softmax - unlress flagged for regression\n",
    "if speed_bucket_size == \"none_use_regression\":\n",
    "    y_ = tf.matmul(f, out_weights) + out_biases\n",
    "elif speed_bucket_size != \"none_use_regression\":\n",
    "    y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "    \n",
    "numeric_Y = Y @ labels_as_numbers         #actual values\n",
    "numeric_y_ = y_ @ labels_as_numbers       #predicted values\n",
    "\n",
    "y_true_final = [] # placeholder list for (m/s) values of actual y\n",
    "y_pred_final = [] # placeholder list for (m/s) values of predicted y\n",
    "\n",
    "# Specifying loss function from flag hyperparameter\n",
    "if loss_function == \"cross_entropy\":\n",
    "    loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "elif loss_function == \"L1_loss\":\n",
    "    loss = tf.reduce_sum(np.abs(numeric_Y - numeric_y_))\n",
    "elif loss_function == \"L2_loss\":\n",
    "        loss = tf.sqrt(tf.reduce_sum((numeric_Y - numeric_y_) * (numeric_Y - numeric_y_)))\n",
    "\n",
    "# Specifying optimizer from flag hyperparameter\n",
    "if optimizer_type==\"gradient\":\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "elif optimizer_type==\"adam\":\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "elif optimizer_type==\"rmsprop\":\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "#Creating prediction and error variables\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "prediction_within_1_bucket = np.abs(tf.argmax(y_,1) - tf.argmax(Y,1)) <= 1\n",
    "prediction_within_2_bucket = np.abs(tf.argmax(y_,1) - tf.argmax(Y,1)) <= 2\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy_1_bucket = tf.reduce_mean(tf.cast(prediction_within_1_bucket, tf.float32))\n",
    "accuracy_2_buckets = tf.reduce_mean(tf.cast(prediction_within_2_bucket, tf.float32))\n",
    "MSE = tf.reduce_mean((numeric_Y - numeric_y_)*(numeric_Y - numeric_y_))\n",
    "MAE = tf.reduce_mean(np.abs((numeric_Y - numeric_y_)))\n",
    "MAPE = tf.reduce_mean(np.abs((numeric_Y - numeric_y_)))\n",
    "\n",
    "# Instantiate reporting variables and lists to store their histories in:\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# Accuracy (only for display in classification tasks):\n",
    "last_train_accuracy = 0.0\n",
    "last_dev_accuracy = 0.0\n",
    "dev_accuracy_history=np.empty(shape=[1],dtype=float)\n",
    "train_accuracy_history=np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# Accuracy within 1 bucket (only for display in classification tasks):\n",
    "last_train_accuracy_1_bucket = 0.0\n",
    "last_dev_accuracy_1_bucket = 0.0\n",
    "dev_accuracy_1_bucket_history=np.empty(shape=[1],dtype=float)\n",
    "train_accuracy_1_bucket_history=np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# Accuracy withing 2 buckets (only for display in classification tasks):\n",
    "last_train_accuracy_2_buckets = 0.0\n",
    "last_dev_accuracy_2_buckets = 0.0\n",
    "dev_accuracy_2_buckets_history = np.empty(shape=[1],dtype=float)\n",
    "train_accuracy_2_buckets_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# MSE (Mean Squared Error): \n",
    "last_train_MSE=0.0\n",
    "last_dev_MSE=0.0\n",
    "dev_MSE_history=np.empty(shape=[1],dtype=float)\n",
    "train_MSE_history=np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# MAE (Mean Absolute Error): \n",
    "last_train_MAE=0.0\n",
    "last_dev_MAE=0.0\n",
    "dev_MAE_history=np.empty(shape=[1],dtype=float)\n",
    "train_MAE_history=np.empty(shape=[1],dtype=float)\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error): \n",
    "last_train_MAPE=0.0\n",
    "last_dev_MAPE=0.0\n",
    "dev_MAPE_history=np.empty(shape=[1],dtype=float)\n",
    "train_MAPE_history=np.empty(shape=[1],dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model and print ongoing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create instance of saver to save model \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    if model_to_load!=\"\":\n",
    "        saver.restore(session, \"../models/\"+model_to_load+\".ckpt\")\n",
    "    counter =  0\n",
    "    cost_at_epoch=[]\n",
    "    last_checkin_time=time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        cost_at_epoch.append(c)    \n",
    "\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if (counter % checkin_frequency == 0):\n",
    "\n",
    "            #Updated lists of reporting metrics:\n",
    "\n",
    "            #Accuracy (only for display in classification tasks):\n",
    "            last_train_accuracy = session.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_accuracy=session.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "            train_accuracy_history=np.append(train_accuracy_history,last_train_accuracy)\n",
    "            dev_accuracy_history=np.append(dev_accuracy_history,last_dev_accuracy)\n",
    "\n",
    "            #Accuracy within 1 bucket (only for display in classification tasks):\n",
    "            last_train_accuracy_1_bucket = session.run(accuracy_1_bucket, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_accuracy_1_bucket = session.run(accuracy_1_bucket, feed_dict={X: test_x, Y: test_y})\n",
    "            train_accuracy_1_bucket_history = np.append(train_accuracy_1_bucket_history, last_train_accuracy_1_bucket)\n",
    "            dev_accuracy_1_bucket_history = np.append(dev_accuracy_1_bucket_history,last_dev_accuracy_1_bucket)\n",
    "\n",
    "            #Accuracy within 2 buckets (only for display in classification tasks):\n",
    "            last_train_accuracy_2_buckets = session.run(accuracy_2_buckets, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_accuracy_2_buckets = session.run(accuracy_2_buckets, feed_dict={X: test_x, Y: test_y})\n",
    "            train_accuracy_2_buckets_history = np.append(train_accuracy_2_buckets_history,last_train_accuracy_2_buckets)\n",
    "            dev_accuracy_2_buckets_history = np.append(dev_accuracy_2_buckets_history,last_dev_accuracy_2_buckets)\n",
    "\n",
    "            #MSE (Mean Squared Error):\n",
    "            last_train_MSE = session.run(MSE, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_MSE=session.run(MSE, feed_dict={X: test_x, Y: test_y})\n",
    "            train_MSE_history=np.append(train_MSE_history,last_train_MSE)\n",
    "            dev_MSE_history=np.append(dev_MSE_history,last_dev_MSE)\n",
    "\n",
    "            #MAE (Mean Absolute Error):\n",
    "            last_train_MAE = session.run(MAE, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_MAE=session.run(MAE, feed_dict={X: test_x, Y: test_y})\n",
    "            train_MAE_history=np.append(train_MAE_history,last_train_MAE)\n",
    "            dev_MAE_history=np.append(dev_MAE_history,last_dev_MAE)\n",
    "\n",
    "            #MAPE (Mean Absolute Percentage Error):\n",
    "            last_train_MAPE = session.run(MAPE, feed_dict={X: train_x, Y: train_y})\n",
    "            last_dev_MAPE = session.run(MAPE, feed_dict={X: test_x, Y: test_y})\n",
    "            train_MAPE_history=np.append(train_MAPE_history,last_train_MAPE)\n",
    "            dev_MAPE_history=np.append(dev_MAPE_history,last_dev_MAPE)\n",
    "\n",
    "            print(\"Epoch: \",epoch,\"*\",checkin_frequency,\"\\n\"+\"Training Loss: \",c)\n",
    "            print(\"Testing Accuracy:\",last_dev_accuracy,\"\\n\",\"Training Accuracy: \", last_train_accuracy)\n",
    "            print(\"dev MAE:\", last_dev_MAE)\n",
    "            print(\"dev MSE:\", last_dev_MSE)\n",
    "\n",
    "\n",
    "            #print(\"Current Time:\"+str(time.time()))\n",
    "            print(\"Time cost:\"+str(round(time.time()-last_checkin_time,1)))\n",
    "            last_checkin_time=time.time()\n",
    "\n",
    "\n",
    "            if counter > 1:\n",
    "                # Save the variables to disk.\n",
    "                save_path = saver.save(session, \"../models/\"+name+\".ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "                print(\"\\n\")\n",
    "\n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))  \n",
    "   \n",
    "    y_true_final = session.run(numeric_Y, feed_dict={X: test_x, Y: test_y})\n",
    "    y_pred_final = session.run(numeric_y_, feed_dict={X: test_x, Y: test_y})\n",
    "    \n",
    "    if FINALTEST==True:\n",
    "        y_true_final_test = session.run(numeric_Y, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        y_pred_final_test= session.run(numeric_y_, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        \n",
    "        hold_out_MAE=session.run(MAE, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        hold_out_MSE=session.run(MSE, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        hold_out_ACC=session.run(accuracy, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        hold_out_ACC1=session.run(accuracy_1_bucket, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})\n",
    "        hold_out_ACC2=session.run(accuracy_2_buckets, feed_dict={X: hold_out_test_x, Y: hold_out_test_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save detailed results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record pred vs actual in .csv\n",
    "\n",
    "# Record data in a an .csv\n",
    "y_trueVy_pred = np.hstack([y_true_final,y_pred_final])\n",
    "df_y_trueVy_pred = pd.DataFrame(y_trueVy_pred)\n",
    "filepath_predictions = \"../results/\"+results_file_name+\"_Predictions_\"+name+\".csv\"\n",
    "df_y_trueVy_pred.to_csv(filepath_predictions, header = [\"y_true_final\", \"y_pred_final\"], index=False)\n",
    "\n",
    "\n",
    "#Record accuracy metrics\n",
    "\n",
    "# Save results to a .csv\n",
    "df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([dev_accuracy_history,\n",
    "                                                      dev_accuracy_1_bucket_history,\n",
    "                                                      dev_accuracy_2_buckets_history,\n",
    "                                                      dev_MSE_history,\n",
    "                                                      dev_MAE_history,\n",
    "                                                      dev_MAPE_history\n",
    "                                                      \n",
    "                                                     ])))\n",
    "filepath_acc = \"../results/\"+results_file_name+\"_AccuracyPerEpoch_\"+name+\".csv\"\n",
    "df_devAccuracy.to_csv(filepath_acc, header = [\"dev_accuracy_history\",\n",
    "                                              \"dev_accuracy_1_bucket_history\",\n",
    "                                              \"dev_accuracy_2_buckets_history\",\n",
    "                                              \"dev_MSE_history\",\n",
    "                                              \"dev_MAE_history\",\n",
    "                                              \"dev_MAPE_history\"\n",
    "                                             ], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true_final.shape)\n",
    "\n",
    "plt.scatter(y_true_final, y_pred_final, s=10, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_filename=\"CM\"+str(time.time())+\"User-\"+str(socket.gethostname())\n",
    "\n",
    "plt.savefig(\"../CM/\"+plot_filename+\".png\",bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.scatter(y_true_final_test, y_pred_final_test, s=10, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_filename=\"CM\"+str(time.time())+\"User-\"+str(socket.gethostname())\n",
    "\n",
    "plt.savefig(\"../CM/\"+plot_filename+\".png\",bbox_inches='tight')\n",
    "\n",
    "print(\"hold_out_MAE:\",hold_out_MAE)\n",
    "print(\"hold_out_MSE:\",hold_out_MSE)\n",
    "print(\"hold_out_ACC:\",hold_out_ACC)\n",
    "print(\"hold_out_ACC1:\",hold_out_ACC1)\n",
    "print(\"hold_out_ACC2:\",hold_out_ACC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time=time.time()\n",
    "model_time=end_time-start_time\n",
    "time_per_epoch=model_time/training_epochs\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def createLines(metricType):\n",
    "    if (metricType==\"MAE\"):\n",
    "        \n",
    "        test=np.squeeze(dev_MAE_history[1:])\n",
    "        train=np.squeeze(train_MAE_history[1:])\n",
    "        dev=plt2.plot(test,'#6f0000',label='Mean Abs Error',linewidth=1.5)\n",
    "        train=plt2.plot(train,'#ff4646',label='Mean Abs Error',linewidth=.8)\n",
    "        return dev,train\n",
    "        \n",
    "    if (metricType==\"MSE\"):\n",
    "        \n",
    "        test=np.squeeze(dev_MSE_history[1:])\n",
    "        train=np.squeeze(train_MSE_history[1:])\n",
    "        dev=plt2.plot(test,'#315047',label='MSE',linewidth=1.5)\n",
    "        train=plt2.plot(train,'#38efab',label='MSE',linewidth=.8)\n",
    "        return dev,train\n",
    "        \n",
    "    if (metricType==\"ACC\"):\n",
    "        \n",
    "        test=np.squeeze(trainAccuracy[1:])\n",
    "        train=np.squeeze(devAccuracy[1:])\n",
    "        dev=plt2.plot(test,'#011f4b',label='Class. Accuracy',linewidth=1.5)\n",
    "        train=plt2.plot(train,'#380356',label='Class. Accuracy',linewidth=.8)\n",
    "        return dev,train\n",
    "    \n",
    "    if (metricType==\"ACC1\"):\n",
    "        \n",
    "        test=np.squeeze(dev_accuracy_1_bucket_history[1:])\n",
    "        train=np.squeeze(train_accuracy_1_bucket_history[1:])\n",
    "        dev=plt2.plot(test,'#005b96',label='ACC1',linewidth=1.5)\n",
    "        train=plt2.plot(train,'#6b0078',label='ACC1',linewidth=.8)\n",
    "        return dev,train\n",
    "    \n",
    "    if (metricType==\"ACC2\"):\n",
    "        test=np.squeeze(dev_accuracy_2_buckets_history[1:])\n",
    "        train=np.squeeze(train_accuracy_2_buckets_history[1:])\n",
    "        dev=plt2.plot(test,'#6497b1',label='.2 Buffer Accuracy',linewidth=1.5)\n",
    "        train=plt2.plot(train,'#b000b2',label='.2 Buffer Accuracy',linewidth=.8)\n",
    "        return dev, train\n",
    "    \n",
    "\n",
    "trainAccuracy=np.squeeze(train_accuracy_history[1:])\n",
    "devAccuracy =np.squeeze(dev_accuracy_history[1:])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "lines=[]\n",
    "lineLabels=[]\n",
    "plt2=ax1.twinx()\n",
    "\n",
    "if reportingMetric_1 != \"\":\n",
    "    myLines = createLines(reportingMetric_1)\n",
    "    lines += myLines[0]\n",
    "    lines += myLines[1]\n",
    "    lineLabels += [reportingMetric_1+'Dev']\n",
    "    lineLabels += [reportingMetric_1+'Train']\n",
    "\n",
    "if reportingMetric_2 != \"\":\n",
    "    myLines = createLines(reportingMetric_2)\n",
    "    lines += myLines[0]\n",
    "    lines += myLines[1]\n",
    "    lineLabels += [reportingMetric_2+'Dev']\n",
    "    lineLabels += [reportingMetric_2+'Train']\n",
    "\n",
    "if reportingMetric_3 != \"\":\n",
    "    myLines = createLines(reportingMetric_3)\n",
    "    lines += myLines[0]\n",
    "    lines += myLines[1]\n",
    "    lineLabels += [reportingMetric_3+'Dev']\n",
    "    lineLabels += [reportingMetric_3+'Train']\n",
    "    \n",
    "if reportingMetric_4 != \"\":\n",
    "    myLines = createLines(reportingMetric_4)\n",
    "    lines += myLines[0]\n",
    "    lines += myLines[1]\n",
    "    lineLabels += [reportingMetric_4+'Dev']\n",
    "    lineLabels += [reportingMetric_4+'Train']\n",
    "    \n",
    "print(lineLabels)  \n",
    "#lines += ax1.plot(devAccuracy,'#a93226', label='Dev Accuracy', linewidth=1)\n",
    "\n",
    "#lines += plt2.plot(mae_test,'#a22776',label='MAE',linewidth=1)\n",
    "#lines += plt2.plot(mae_train,'#b33334',label='MAE',linewidth=1)\n",
    "#plt2=ax1.twinx()\n",
    "\n",
    "#plt2.plot(np.squeeze(test_accuracy_history),\"b-\")\n",
    "\n",
    "plt.ylim([0, 1]) \n",
    "plt.ylabel('Model Results')\n",
    "plt.xlabel('epoch x'+str(checkin_frequency))\n",
    "plt.title(\"Final Dev Accuracy =\" + str(last_dev_accuracy))\n",
    "extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "plt.legend([extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra,extra],(\n",
    "                                                \n",
    "                                                \"MAE\" +str(last_dev_MAE),\n",
    "                                                \"MSE\" +str(last_dev_MSE),\n",
    "                                                \"MAPE\" +str(last_dev_MAPE),\n",
    "                                                \"ACC1\" +str(last_dev_accuracy_1_bucket),\n",
    "                                                \"ACC2\" +str(last_dev_accuracy_2_buckets),\n",
    "                                                \"Label win size:\" +str(label_window_size),\n",
    "                                                \"Optimization:\" + optimizer_type,\n",
    "                                                \"activation1:\" + activation_1,\n",
    "                                                \"activation2:\"+ activation_2,\n",
    "                                                \"loss: \" + loss_function,\n",
    "                                                \"learning rate: \" + str(learning_rate),\n",
    "                                                \"batch_size: \" + str(batch_size),\n",
    "                                                \"kernel_size: \" + str(kernel_size),\n",
    "                                                \"depth: \"+ str(depth),\n",
    "                                                \"layers : \"+ str(num_hidden),\n",
    "                                                \"max pool kernel size: \"+str(max_pool_kernel_size),\n",
    "                                                \"speed_bucket_size: \" + speed_bucket_size,\n",
    "                                                \"learning rate: \"+str(learning_rate),\n",
    "                                                \"epochs: \"+str(training_epochs),\n",
    "                                                \"input_num_timestamps: \" + str(input_num_timestamps),\n",
    "                                                \"num_labels: \" + str(num_labels),\n",
    "                                                \"num_channels:\" + str(num_channels),\n",
    "                                                \"note:\" + plot_note),\n",
    "          \n",
    "                                                bbox_to_anchor=(1.2, 1),\n",
    "                                                loc=2,\n",
    "                                                borderaxespad=0.)\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax1, lines[0:], lineLabels[0:],bbox_to_anchor=(1,-.1,0,0),\n",
    "             loc=1, frameon=True)\n",
    "\n",
    "ax1.add_artist(leg);\n",
    "\n",
    "plot_filename=str(time.time())+\"User-\"+str(socket.gethostname())\n",
    "\n",
    "plt.savefig(\"../plots/\"+plot_filename+\".png\",bbox_inches='tight')\n",
    "\n",
    "a=[[\"cnn\",\n",
    "    name,\n",
    "    plot_filename,\n",
    "    myFileLocation,\n",
    "    loss_function,\n",
    "    optimizer_type,\n",
    "    activation_1,\n",
    "    activation_2,\n",
    "    last_train_accuracy_1_bucket,\n",
    "    last_dev_accuracy_1_bucket,\n",
    "    last_train_accuracy_2_buckets,\n",
    "    last_dev_accuracy_2_buckets,\n",
    "    last_train_MSE,\n",
    "    last_dev_MSE,\n",
    "    last_train_MAE,\n",
    "    last_dev_MAE,\n",
    "    last_train_MAPE,\n",
    "    last_dev_MAPE,    \n",
    "    training_epochs,\n",
    "    model_time,\n",
    "    last_dev_accuracy,\n",
    "    last_train_accuracy,\n",
    "    batch_size,\n",
    "    kernel_size,\n",
    "    depth,\n",
    "    num_hidden,\n",
    "    max_pool_kernel_size,\n",
    "    learning_rate,\n",
    "    speed_bucket_size,\n",
    "    loss_function,\n",
    "    input_num_timestamps,\n",
    "    checkin_frequency,\n",
    "    plot_note,\n",
    "    model_to_load,\n",
    "    results_file_name,\n",
    "    num_channels,\n",
    "    label_window_size,\n",
    "    sample_stride\n",
    "    ]]\n",
    "        \n",
    "df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                            \"model filename\",\n",
    "                            \"plot filename\",\n",
    "                            \"data filename\",\n",
    "                            \"loss function\",\n",
    "                            \"optimizer\",\n",
    "                            \"activation_1\",\n",
    "                            \"activation_2\",\n",
    "                            \"last_train_accuracy_1_bucket\",\n",
    "                            \"last_dev_accuracy_1_bucket\",\n",
    "                            \"last_train_accuracy_2_buckets\",\n",
    "                            \"last_dev_accuracy_2_buckets\",\n",
    "                            \"last_train_MSE\",\n",
    "                            \"last_dev_MSE\",\n",
    "                            \"last_train_MAE\",\n",
    "                            \"last_dev_MAE\",\n",
    "                            \"last_train_MAPE\",\n",
    "                            \"last_dev_MAPE\",\n",
    "                            \"epochs\",\n",
    "                            \"runtime\",\n",
    "                            \"dev accuracy\",\n",
    "                            \"train accuracy\",\n",
    "                            \"batch_size\",\n",
    "                            \"kernel_size\",\n",
    "                            \"depth\",\n",
    "                            \"num_hidden\",\n",
    "                            \"max_pool_kernel_size\",\n",
    "                            \"learning_rate\",\n",
    "                            \"speed_bucket_size\",\n",
    "                            \"loss_function\",\n",
    "                            \"input_num_timestamps\",\n",
    "                            \"checkin_frequency\",\n",
    "                            \"plot_note\",\n",
    "                            \"model_loaded\",\n",
    "                            \"results_file_name\",\n",
    "                            \"num_channels\",\n",
    "                            \"label_window_size\",\n",
    "                            \"sample_stride\"])\n",
    "\n",
    "\n",
    "past_results=pd.concat([past_results,df])\n",
    "past_results.to_csv(\"../results/\"+results_file_name+\".csv\",index=False )\n",
    "fig.tight_layout(pad=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
