{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumo Run - Deep FFCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here 2\n",
      "made it here 3\n",
      "made it here 4\n"
     ]
    }
   ],
   "source": [
    "print('made it here 1') # these are used for Sherlock to check what is causing the hiccup\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, Input\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers \n",
    "from keras import optimizers\n",
    "\n",
    "print('made it here 2')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print('made it here 3')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import socket\n",
    "import pickle\n",
    "import os.path\n",
    "#import dill #can't find dill\n",
    "\n",
    "print('made it here 4')\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "np.random.seed(7) # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "num_channels = 6 # number of time-series channels of data (i.e. 7 kinematic features) #NOTE: Change to 6 by removing Pelvic Tilt (recommended by Lumo)\n",
    "num_anthropometrics = 4 # number of user anthropometric data elements\n",
    "input_window_size = 36 # number of timestamps used in the input for prediction (i.e. the input window)\n",
    "label_window_size = 30 # number of timestamps used to label the speed we will be predicting\n",
    "speed_bucket_size = '0.1' # how to round the data for classification task. Consider '0.5', '0.1', and 'none_use_regression'\n",
    "\n",
    "previous_model_weights_to_load = \"\" # If non-empty, load weights from a previous model (note: architectures must be identical)\n",
    "model_architecture = 'CNN' # 'FCN', 'CNN'\n",
    "data_input_table_structure = 'Raw_Timeseries' # 'Vectorized_By_Row' 'Raw_Timeseries'\n",
    "myFileDirectory = 'C:/Users/adam/Documents/CS 230/Project/Lumo Data/'\n",
    "myFileName = 'quarter-big'\n",
    "myFileLocation = myFileDirectory + myFileName + '.csv'\n",
    "        # Other data files/folders to potentially use:\n",
    "        # '../datasets/'  |  'C:/Users/adam/Documents/CS 230/Project/Lumo Data/\n",
    "        # 'quarter-big'   |   'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "        # 'SAMPLE_TimeSeries_Longest1000Runs_wAnthro_MultiLabeledSpeed_20180523'  |  'TimeSeries_InputCNN_1000Runs'\n",
    "        \n",
    "# Fully Connected Architecture\n",
    "\n",
    "num_hidden_units_fc_layers = [256, 256, 256, 128, 128, 128]\n",
    "hidden_units_strategy = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers) # document strategy \n",
    "num_hidden_fc_layers = len(num_hidden_units_fc_layers) # document strategy\n",
    "activations_fc_layers = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "activations_strategy = ''.join(str(num) + \"_\" for num in activations_fc_layers) # document strategy\n",
    "dropout_rate_fc_layers = [1.0, 1.0, 1.0, 0.8, 0.8, 0.8]\n",
    "dropout_rates = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers) # document strategy\n",
    "\n",
    "# Convolutional Architecture\n",
    "    \n",
    "sample_stride = 18 # how many timestamps to shift over between each unique training example\n",
    "num_filters = 40 # number of filters in Conv2D layer (aka depth) # we used 40, ex; used 128\n",
    "kernel_size = 6 # kernal size of the Conv2D layer # we use 6, example used 2, I would guess closer to 3\n",
    "activation_conv_layer = \"relu\" # options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "max_pool_kernel_size = 6 # max pooling window size# we use 6, example used 2, I don't agre with 6\n",
    "conv_layer_dropout = 0.2 # dropout ratio for dropout layer # we don't use in our model\n",
    "\n",
    "num_hidden_units_fc_layers_CNN = [50]\n",
    "hidden_units_strategy_CNN = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers_CNN) # document strategy \n",
    "num_hidden_fc_layers_CNN = len(num_hidden_units_fc_layers_CNN) # document strategy\n",
    "activations_fc_layers_CNN = ['tanh']\n",
    "activations_strategy_CNN = ''.join(str(num) + \"_\" for num in activations_fc_layers_CNN) # document strategy\n",
    "dropout_rate_fc_layers_CNN = [1.0]\n",
    "dropout_rates_CNN = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers_CNN) # document strategy\n",
    "\n",
    "num_hidden_units_fc_layers_CNN[0] \n",
    "activations_fc_layers_CNN[0]\n",
    "    \n",
    "# Training strategy\n",
    "\n",
    "batch_size = 50 # we used 50 for CNN, 128 for FCN\n",
    "learning_rate = 0.0001 # we used 0.001 for FCN, 0.0001 for CNN\n",
    "training_epochs = 50\n",
    "optimizer_type = 'gradient' # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "loss_function = 'categorical_crossentropy' # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Automatic Reporting and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 3 most interesting evaluation metrics to report on in final plots\n",
    "\n",
    "accuracy_reporting_metric_1 = 'class_mae' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "accuracy_reporting_metric_2 = 'class_percent_2buckRange' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "accuracy_reporting_metric_3 = 'class_mse' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3\n",
    "\n",
    "plt.style.use('ggplot') # style of matlab plots to produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File naming conventions\n",
    "\n",
    "file_name = strftime(\"%Y%m%d_%H%M%S\", gmtime()) # user input for filename of saved model\n",
    "plot_note = \"\"\n",
    "model_to_lod = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180726\" + \"_\" + model_architecture\n",
    "\n",
    "customize_file_names = False\n",
    "if customize_file_names:\n",
    "    file_name = input(\"String to add to model filename (defaults to time stamp if nothing entered):\")  \n",
    "    results_file_name = input(\"Name of the results file, a table, to store the prediction results\") # name of results file\n",
    "    plot_note = input(\"Note you'd like to add in the legend of the primary learning curves plot:\") #user input to add note to plot\n",
    "    model_to_load = input(\"Enter the model name to load to initialize parameters - leave blank to start fresh\") #user input to load prev model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path,header = 0) # This uses the header row (row 0) as the column names\n",
    "    return data\n",
    "\n",
    "def windows(data, size): # define time windows to create each training example\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride # other common options: (size / 2)\n",
    "    \n",
    "# Used for vectorized input WITH SQL pre-processing\n",
    "def segment_signal_FCN_vector(data_inputs, data_full): \n",
    "    #dataframe_input = data_inputs.loc[:, 'gender':'pelvic_tilt_lag_0'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    dataframe_input = data_inputs.loc[:, 'gender':'pelvic_rotation_lag_0'] # select all columns from gender to pelvic_rotation_lag_0\n",
    "    dataframe_labels = data_full.loc[:, 'gps_speed_lag_7':'gps_speed_lag_3'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    segments = dataframe_input.values\n",
    "    labels_before_avg = dataframe_labels.values\n",
    "    if speed_bucket_size == '0.1':\n",
    "        labels = np.around(np.mean(labels_before_avg, axis=1),decimals=1)\n",
    "    elif speed_bucket_size == 'none_use_regression':\n",
    "        labels = np.mean(labels_before_avg, axis=1)\n",
    "    return segments, labels\n",
    "\n",
    "# Used for CNN/FFCN input WITHOUT SQL pre-processing\n",
    "def segment_signal_w_concat(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    # define segment shape for training example input\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))    \n",
    "        segments_timeseries = np.empty((0, input_window_size, num_channels))\n",
    "        segments_anthro = np.empty((0, num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        aa = data_inputs[\"age\"][start]\n",
    "        bb = data_inputs[\"weight\"][start]\n",
    "        cc = data_inputs[\"height\"][start]\n",
    "        dd = data_inputs[\"gender\"][start]   \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            # Create segment input arrays\n",
    "            if  model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':        \n",
    "                segments_timeseries = np.vstack([segments_timeseries,np.dstack([a,b,c,d,e,f])])\n",
    "                segments_anthro = np.vstack([segments_anthro,np.hstack([aa,bb,cc,dd])])\n",
    "            # Create labels array\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    if  model_architecture == 'FCN':\n",
    "        return segments, labels\n",
    "    elif model_architecture == 'CNN':            \n",
    "        return segments_timeseries, segments_anthro, labels\n",
    "\n",
    "def load_results_file_FCN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            num_hidden_fc_layers,\n",
    "            hidden_units_strategy,\n",
    "            activations_strategy,\n",
    "            dropout_rates\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 4\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"num_hidden_fc_layers\",\n",
    "                                    \"hidden_units_strategy\",\n",
    "                                    \"activations_strategy\",\n",
    "                                    \"dropout_rates\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df\n",
    "\n",
    "def load_results_file_CNN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            hidden_units_strategy_CNN,\n",
    "            num_filters,\n",
    "            kernel_size,\n",
    "            sample_stride,\n",
    "            activation_conv_layer,\n",
    "            activations_strategy_CNN,\n",
    "            max_pool_kernel_size\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"hidden_units_strategy_CNN\",\n",
    "                                    \"num_filters\",\n",
    "                                    \"kernel_size\",\n",
    "                                    \"sample_stride\",\n",
    "                                    \"activation_conv_layer\",\n",
    "                                    \"activations_strategy_CNN\",\n",
    "                                    \"max_pool_kernel_size\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation'] # normalize all columns from gender to pelvic_tilt\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation_lag_0'] # normalize all columns from gender to pelvic_rotation_lag_0\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_file_string_segment = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_segment_timeseries = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_timeseries.npy\"\n",
    "np_array_file_string_segment_anthro = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_anthro.npy\"\n",
    "np_array_file_string_label = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "        \n",
    "if os.path.isfile(np_array_file_string_segment):   # if this file already exists, load the relevant saved np arrays\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments_timeseries = np.load(np_array_file_string_segment_timeseries, allow_pickle=True)\n",
    "        segments_anthro = np.load(np_array_file_string_segment_anthro, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        if  model_architecture == 'FCN':\n",
    "            segments, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset)\n",
    "        elif model_architecture == 'CNN':\n",
    "            segments_timeseries, segments_anthro, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "         labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "         labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "    if  model_architecture == 'FCN':\n",
    "        np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        np.save(np_array_file_string_segment_timeseries, segments_timeseries, allow_pickle=True)\n",
    "        np.save(np_array_file_string_segment_anthro, segments_anthro, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "\n",
    "num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data into training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(labels)) < 0.90 # split data into 90% train, 10% dev, based on lenghto of labels\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "elif model_architecture == 'CNN':\n",
    "    X_train_timeseries = segments_timeseries[train_dev_split]\n",
    "    X_test_timeseries = segments_timeseries[~train_dev_split]\n",
    "    X_train_anthro = segments_anthro[train_dev_split]\n",
    "    X_test_anthro = segments_anthro[~train_dev_split]\n",
    "\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement NN architecture in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnModel():\n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers[0], activation=activations_fc_layers[0], input_shape=(input_window_size*num_channels + num_anthropometrics,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_fc_layers[0]))\n",
    "    # Intermediate layers\n",
    "    for L in range(1, num_hidden_fc_layers):\n",
    "        model.add(Dense(num_hidden_units_fc_layers[L], activation=activations_fc_layers[L]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model\n",
    "\n",
    "def cnnModel_multInput(): # (inputs, outputs):\n",
    "    # CNN over time-series data\n",
    "    input_cnn = Input(shape=(input_window_size, num_channels))\n",
    "    conv1 = Conv1D(num_filters, kernel_size,activation=activation_conv_layer)(input_cnn)\n",
    "    pool1 = MaxPooling1D(pool_size=max_pool_kernel_size, padding='valid', strides=(2))(conv1)\n",
    "    conv2 = Conv1D(num_filters//10, kernel_size, activation=activation_conv_layer)(pool1) # add additional CNN layer\n",
    "    flat1 = Flatten()(conv2)\n",
    "    # Include anthropometric data\n",
    "    input_anthro = Input(shape=(num_anthropometrics,))\n",
    "    # Concatenate result of CNN with antropometric data\n",
    "    merged = concatenate([flat1, input_anthro])\n",
    "    # Add fully connected hident layers after concatenating (at least one)\n",
    "    fc1 = Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])(merged) # add first fully connected layer\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        # NEED TO CORRECT BEFORE USE    \n",
    "        #something like this\n",
    "        fc_L = Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L])(fc_L-1) # add first fully connected layer\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        output = Dense(num_buckets_total, activation='softmax')(fc1) # will need to change with more fc layers\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        output = Dense(1,activation='linear')(fc1)  \n",
    "    model = Model(inputs = [input_cnn, input_anthro], outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  model_architecture == 'FCN':\n",
    "    model = fcnModel()\n",
    "elif model_architecture == 'CNN':\n",
    "    model = cnnModel_multInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 36, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 31, 40)       1480        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 13, 40)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 8, 4)         964         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 36)           0           flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           1850        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 43)           2193        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,487\n",
      "Trainable params: 6,487\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss functions and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def class_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "    # Note: we cannot define RMSE directly in Keras since the loss function is defined for one training example at a time\n",
    "\n",
    "def class_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "\n",
    "def class_mape(y_true, y_pred):\n",
    "    diff = K.abs((K.sum(y_true * labels_to_number,axis=-1,keepdims=True) - K.sum(y_pred * labels_to_number,axis=-1,keepdims=True)) / K.clip(K.abs(K.sum(y_true * labels_to_number,axis=-1,keepdims=True)),K.epsilon(),None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "def class_percent_1buckLow(y_true, y_pred): # percent of times the prediction is 1 bucket below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx())\n",
    "\n",
    "def class_percent_2buckLow(y_true, y_pred): # percent of times the prediction is 2 buckets below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())\n",
    "    \n",
    "def class_percent_1buckHigh(y_true, y_pred): # percent of times the prediction is 1 bucket above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx())    \n",
    "\n",
    "def class_percent_2buckHigh(y_true, y_pred): # percent of times the prediction is 2 buckets above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx())    \n",
    "\n",
    "def class_percent_1buckRange(y_true, y_pred): # percent of times the prediction is within 1 bucket of true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())\n",
    "\n",
    "def class_percent_2buckRange(y_true, y_pred): # percent of times the prediction is within 2 buckets of true value\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())    \n",
    "\n",
    "# For reference, from keras documentation: https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "#def class_categorical_accuracy(y_true, y_pred):\n",
    "    #return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model loss and optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "if optimizer_type == 'adam':\n",
    "    model_optimizer = optimizers.Adam(lr = learning_rate) #, decay, beta_1, beta_2 are HPs\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    model_optimizer = optimizers.RMSprop(lr = learning_rate) #, decay, rho\n",
    "elif optimizer_type == 'gradient':\n",
    "    model_optimizer = optimizers.SGD(lr = learning_rate) #, decay, momentum\n",
    "\n",
    "# Compile model with appropriate loss function\n",
    "if speed_bucket_size != 'none_use_regression': # if performing classification, ALWAYS use cross-entropy loss\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=model_optimizer, metrics=['accuracy',class_percent_1buckRange,class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n",
    "else:                                          # if performing regression, use mean squared error or mean absolute error\n",
    "    if loss_function == 'categorical_crossentropy': raise NameError('Are you sure you want to use cross entropy loss with a regression tasks!?')\n",
    "    model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, load weights from a previous model to start with model\n",
    "\n",
    "if previous_model_weights_to_load != \"\":\n",
    "    model.load_weights(\"../Model Final Parameters/\" + previous_model_weights_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12215 samples, validate on 1383 samples\n",
      "Epoch 1/50\n",
      "12215/12215 [==============================] - 3s 254us/step - loss: 3.8364 - acc: 0.0245 - class_percent_1buckRange: 0.0734 - class_percent_2buckRange: 0.1265 - class_mae: 0.6871 - class_mse: 0.6771 - val_loss: 3.8360 - val_acc: 0.0260 - val_class_percent_1buckRange: 0.0781 - val_class_percent_2buckRange: 0.1302 - val_class_mae: 0.6683 - val_class_mse: 0.6565\n",
      "Epoch 2/50\n",
      "12215/12215 [==============================] - 3s 212us/step - loss: 3.8261 - acc: 0.0254 - class_percent_1buckRange: 0.0752 - class_percent_2buckRange: 0.1276 - class_mae: 0.6856 - class_mse: 0.6736 - val_loss: 3.8259 - val_acc: 0.0268 - val_class_percent_1buckRange: 0.0781 - val_class_percent_2buckRange: 0.1280 - val_class_mae: 0.6672 - val_class_mse: 0.6536\n",
      "Epoch 3/50\n",
      "12215/12215 [==============================] - 3s 237us/step - loss: 3.8171 - acc: 0.0260 - class_percent_1buckRange: 0.0791 - class_percent_2buckRange: 0.1298 - class_mae: 0.6846 - class_mse: 0.6711 - val_loss: 3.8171 - val_acc: 0.0282 - val_class_percent_1buckRange: 0.0788 - val_class_percent_2buckRange: 0.1302 - val_class_mae: 0.6664 - val_class_mse: 0.6513\n",
      "Epoch 4/50\n",
      " 3800/12215 [========>.....................] - ETA: 2s - loss: 3.8143 - acc: 0.0245 - class_percent_1buckRange: 0.0795 - class_percent_2buckRange: 0.1295 - class_mae: 0.6903 - class_mse: 0.6820"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-809936211a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mmodel_architecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'CNN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_timeseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_anthro\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_timeseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_anthro\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    history = model.fit(X_train, y_train, batch_size= batch_size, epochs=training_epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "elif model_architecture == 'CNN':\n",
    "    history = model.fit([X_train_timeseries, X_train_anthro], y_train, batch_size= batch_size, epochs=training_epochs, verbose=1, validation_data=([X_test_timeseries, X_test_anthro], y_test))\n",
    "    \n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a plot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform key results into a np arrary\n",
    "trainAccuracy_1 = np.squeeze(history.history[accuracy_reporting_metric_1])\n",
    "devAccuracy_1 = np.squeeze(history.history[dev_reporting_metric_1])\n",
    "trainAccuracy_2 = np.squeeze(history.history[accuracy_reporting_metric_2])\n",
    "devAccuracy_2 = np.squeeze(history.history[dev_reporting_metric_2])    \n",
    "trainAccuracy_3 = np.squeeze(history.history[accuracy_reporting_metric_3])\n",
    "devAccuracy_3 = np.squeeze(history.history[dev_reporting_metric_3])\n",
    "trainAccuracy_4 = np.squeeze(history.history['acc'])\n",
    "devAccuracy_4 = np.squeeze(history.history['val_acc'])\n",
    "epochs = np.squeeze(range(1,training_epochs + 1))\n",
    "\n",
    "# Save results to a .csv in the \"Learning Curve Results\"\n",
    "df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([epochs,devAccuracy_1, devAccuracy_2, devAccuracy_3, devAccuracy_4])))\n",
    "filepath_acc = \"../Learning Curves/\" + str(file_name) +\"_AccuracyPerEpoch_Data\" + \".csv\"\n",
    "df_devAccuracy.to_csv(filepath_acc, header = [\"Epochs\", dev_reporting_metric_1, dev_reporting_metric_2, dev_reporting_metric_3, 'acc'], index=False)\n",
    "\n",
    "# Declare final values for results\n",
    "final_accuracy_1 = history.history[accuracy_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_dev_1 = history.history[dev_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_2 = history.history[accuracy_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_dev_2 = history.history[dev_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_3 = history.history[accuracy_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_dev_3 = history.history[dev_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_4 = history.history['acc'][training_epochs - 1]\n",
    "final_accuracy_dev_4 = history.history['val_acc'][training_epochs - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "lines=[]\n",
    "\n",
    "lines += ax1.plot(trainAccuracy_4,'#0e128c', label='Train Accuracy 2', linewidth=1) #'#DAF7A6'\n",
    "lines += ax1.plot(devAccuracy_4,'#a3a4cc', label='Dev Accuracy 2', linewidth=1)# '#33FF00',\n",
    "lines += ax1.plot(trainAccuracy_1,'#FF5733', label='Train Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_1,'#C70039', label='Dev Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(trainAccuracy_2,'#9C27B0', label='Train Accuracy 2', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_2,'#7986CB', label='Dev Accuracy 2', linewidth=1)\n",
    "plt.ylim([0.0, 1.0])  # Surpress this for non-classification tasks\n",
    "\n",
    "plt.ylabel('Train vs. Dev Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(dev_reporting_metric_1 + \": \" + str(np.around(final_accuracy_dev_1,4)) + \"\\n\" + \\\n",
    "         dev_reporting_metric_2 + \": \" + str(np.around(final_accuracy_dev_2,4))) \n",
    "extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "plt.legend([extra,extra,extra,extra,extra,extra,extra,extra,extra,extra],(\n",
    "                                                \"loss: \" + loss_function,\n",
    "                                                \"learning rate: \" + str(learning_rate),\n",
    "                                                \"batch_size: \" + str(batch_size),\n",
    "                                                \"speed_bucket_size: \" + speed_bucket_size,\n",
    "                                                \"epochs: \"+str(training_epochs),\n",
    "                                                \"input_window_size: \" + str(input_window_size),\n",
    "                                                \"num_labels: \" + str(len(labels_to_number)),\n",
    "                                                \"evaluation metric 1:\"+accuracy_reporting_metric_1,\n",
    "                                                \"evaluation metric 2:\"+accuracy_reporting_metric_2,\n",
    "                                                \"evaluation metric 3:\"+accuracy_reporting_metric_3,\n",
    "                                                \"note:\" + plot_note),\n",
    "                                                bbox_to_anchor=(1.05, 1),\n",
    "                                                loc=2,\n",
    "                                                borderaxespad=0.)\n",
    "\n",
    "leg = Legend(ax1, lines[0:], ['Train ACC', 'Dev ACC','Train Eval 1','Dev Eval 1'],\n",
    "             loc='bestoutside', frameon=False)\n",
    "ax1.add_artist(leg);\n",
    "plt.savefig(\"../Learning Curves/\" + str(file_name) + \"_AccuracyPerEpoch_Image.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record results of a model in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the most recent run to the results file for documentation\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,  \n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,    \n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        num_hidden_fc_layers,\n",
    "        hidden_units_strategy,\n",
    "        activations_strategy,\n",
    "        dropout_rates\n",
    "       ]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"batch_size\",  \n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"num_hidden_fc_layers\",\n",
    "                                \"hidden_units_strategy\",\n",
    "                                \"activations_strategy\",\n",
    "                                \"dropout_rates\"])\n",
    "    past_results = load_results_file_FCN(results_file_name)\n",
    "elif model_architecture == 'CNN':     \n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,\n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        hidden_units_strategy_CNN,\n",
    "        num_filters,\n",
    "        kernel_size,\n",
    "        sample_stride,\n",
    "        activation_conv_layer,\n",
    "        activations_strategy_CNN,\n",
    "        max_pool_kernel_size]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"batch_size\",\n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"hidden_units_strategy_CNN\",\n",
    "                                \"num_filters\",\n",
    "                                \"kernel_size\",\n",
    "                                \"sample_stride\",\n",
    "                                \"activation_conv_layer\",\n",
    "                                \"activations_strategy_CNN\",\n",
    "                                \"max_pool_kernel_size\"])    \n",
    "    past_results = load_results_file_CNN(results_file_name)\n",
    "past_results=pd.concat([past_results,df])\n",
    "print(past_results)\n",
    "past_results.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) # Consider fixing to put the columns in not-alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Confusion Matrix and Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = y_test\n",
    "y_true_argmax = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.scatter(y_true_argmax, y_pred_argmax, s=3, alpha=0.3)\n",
    "plt.scatter(y_true_argmax, y_true_argmax, s=3, alpha=1)\n",
    "#plt.scatter(y_true, y_pred, s=3, alpha=0.3) # For regression\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "plt.xlabel('Y_True')\n",
    "plt.ylabel('Y_Prediction')\n",
    "plt.savefig(\"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Image.png\")\n",
    "plt.show()\n",
    "\n",
    "# Record data in a .csv\n",
    "y_trueVy_pred = np.vstack([y_true_argmax,y_pred_argmax])\n",
    "df_y_trueVy_pred = pd.DataFrame(np.transpose(y_trueVy_pred))\n",
    "filepath_predictions = \"../Model Final Predictions/\" + str(file_name) + \"_Predictions\" + \".csv\"\n",
    "df_y_trueVy_pred.to_csv(filepath_predictions, header = [\"y_true_argmax\", \"y_pred_argmax\"], index=False)\n",
    "\n",
    "# Create and save a confusion matrix\n",
    "cm = confusion_matrix(y_true_argmax, y_pred_argmax)\n",
    "df_cm = pd.DataFrame (cm)\n",
    "filepath_cm = \"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Data.xlsx\"\n",
    "df_cm.to_excel(filepath_cm, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model parameters (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_model_name = file_name + \"_\" + model_architecture\n",
    "model.save_weights(\"../Model Final Parameters/\" + completed_model_name + '_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
