{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumo Run - Deep FFCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is this script being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_to_run_script = 'local' # 'Sherlock', 'local'\n",
    "runs_per_file = 'train_loop' # 'single', 'train_loop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script is starting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded keras!\n"
     ]
    }
   ],
   "source": [
    "print('Script is starting!') # these are used for Sherlock to check what is causing the hiccup\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, Input, Lambda, Merge\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers \n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "print('Successfully loaded keras!')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import socket\n",
    "import pickle\n",
    "import os.path\n",
    "#import dill #can't find dill\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# from SpeedPrediction_Helper import *\n",
    "\n",
    "np.random.seed(7) # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "num_channels = 6 # number of time-series channels of data (i.e. 7 kinematic features) #NOTE: Change to 6 by removing Pelvic Tilt (recommended by Lumo)\n",
    "num_anthropometrics = 4 # number of user anthropometric data elements\n",
    "input_window_size = 26 # number of timestamps used in the input for prediction (i.e. the input window)\n",
    "label_window_size = 20 # number of timestamps used to label the speed we will be predicting\n",
    "speed_bucket_size = 'none_use_regression' # how to round the data for classification task. Consider '0.5', '0.1', and 'none_use_regression'\n",
    "\n",
    "previous_model_weights_to_load = \"\" # If non-empty, load weights from a previous model (note: architectures must be identical)\n",
    "model_architecture = 'CNN' # 'FCN', 'CNN'\n",
    "data_input_table_structure = 'Raw_Timeseries' # 'Vectorized_By_Row' 'Raw_Timeseries'\n",
    "if machine_to_run_script == 'local':\n",
    "    folder_head_loc = '../';\n",
    "    folder_data_loc = 'C:/Users/adam/Documents/Lumo/Lumo Data/'\n",
    "elif machine_to_run_script == 'Sherlock':\n",
    "    folder_head_loc = '/home/users/agotlin/lumo/'\n",
    "    folder_data_loc = '/home/users/agotlin/SherlockDataFiles/'\n",
    "myFileName = 'TimeSeries_InputRaw_1000Runs_QuarterSample'\n",
    "myFileLocation = folder_data_loc + myFileName + '.csv'\n",
    "    # Other data files/folders to potentially use:\n",
    "    # 'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "    # 'TimeSeries_InputRaw_1000Runs'  |  'TimeSeries_InputRaw_1000Runs_QuarterSample'  |  'TimeSeries_InputRaw_1000Runs_Top10kRowsSample'\n",
    "    \n",
    "# Training strategy\n",
    "batch_size_all = [64, 128] # we used 50 for CNN, 128 for FCN\n",
    "learning_rate_all = [0.0001] # we used 0.001 for FCN, 0.0001 for CNN\n",
    "optimizer_type_all = ['gradient'] # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "loss_function_all = ['mae'] # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'    \n",
    "training_epochs_all = [100]\n",
    "    \n",
    "# Fully Connected Architecture\n",
    "\n",
    "num_hidden_units_fc_layers = [256, 256, 256, 128, 128, 128]\n",
    "hidden_units_strategy = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers) # document strategy \n",
    "num_hidden_fc_layers = len(num_hidden_units_fc_layers) # document strategy\n",
    "activations_fc_layers = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "activations_strategy = ''.join(str(num) + \"_\" for num in activations_fc_layers) # document strategy\n",
    "dropout_rate_fc_layers = [1.0, 1.0, 1.0, 0.8, 0.8, 0.8]\n",
    "dropout_rates = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers) # document strategy\n",
    "\n",
    "# Convolutional Architecture\n",
    "    \n",
    "sample_stride = input_window_size/2 # how many timestamps to shift over between each unique training example # 18, input_window_size/2\n",
    "num_filters = 40 # number of filters in Conv2D layer (aka depth) # we used 40, ex; used 128\n",
    "kernel_size = 5 # kernal size of the Conv2D layer # we use 6, example used 2, I would guess closer to 3\n",
    "activation_conv_layer = \"relu\" # options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "max_pool_kernel_size = 5 # max pooling window size# we use 6, example used 2, I don't agre with 6\n",
    "conv_layer_dropout = 0.2 # dropout ratio for dropout layer # we don't use in our model\n",
    "\n",
    "num_hidden_units_fc_layers_CNN = [50]\n",
    "hidden_units_strategy_CNN = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers_CNN) # document strategy \n",
    "num_hidden_fc_layers_CNN = len(num_hidden_units_fc_layers_CNN) # document strategy\n",
    "activations_fc_layers_CNN = ['tanh']\n",
    "activations_strategy_CNN = ''.join(str(num) + \"_\" for num in activations_fc_layers_CNN) # document strategy\n",
    "dropout_rate_fc_layers_CNN = [1.0]\n",
    "dropout_rates_CNN = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers_CNN) # document strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Automatic Reporting and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 3 most interesting evaluation metrics to report on in final plots\n",
    "if speed_bucket_size != 'none_use_regression': \n",
    "    accuracy_reporting_metric_1 = 'class_mae' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "    accuracy_reporting_metric_2 = 'class_mse' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "    accuracy_reporting_metric_3 = 'class_percent_2buckRange' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3    \n",
    "    accuracy_reporting_metric_4 = 'acc' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_4 = 'val_' + accuracy_reporting_metric_4\n",
    "else:\n",
    "    accuracy_reporting_metric_1 = 'mean_absolute_error' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "    accuracy_reporting_metric_2 = 'mean_squared_error' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "    accuracy_reporting_metric_3 = 'loss' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3\n",
    "    accuracy_reporting_metric_4 = 'loss' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange' # A bandaid since the results reporting script requires 4 metrics\n",
    "    dev_reporting_metric_4 = 'val_' + accuracy_reporting_metric_4    \n",
    "\n",
    "# File naming conventions\n",
    "plot_note = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180911\" + \"_\" + model_architecture\n",
    "\n",
    "# Style of matlab plots to produce\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeedPrediction_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation'] # normalize all columns from gender to pelvic_tilt\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation_lag_0'] # normalize all columns from gender to pelvic_rotation_lag_0\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly normalized data!\n"
     ]
    }
   ],
   "source": [
    "print('Successfuly normalized data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling down C:/Users/adam/Documents/Lumo/Lumo Data/SavedNPArrays/TimeSeries_InputRaw_1000Runs_QuarterSample_CNN_26_20_13.0_label_regr.npy\n"
     ]
    }
   ],
   "source": [
    "np_array_file_string_segment = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_segment_timeseries = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_timeseries.npy\"\n",
    "np_array_file_string_segment_anthro = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_anthro.npy\"\n",
    "np_array_file_string_label = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" +  str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if speed_bucket_size == 'none_use_regression': # A bandaid script since regression and classification were not differentiated in above naming convention\n",
    "    np_array_file_string_segment = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_regr.npy\"\n",
    "    np_array_file_string_segment_timeseries = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_timeseries_regr.npy\"\n",
    "    np_array_file_string_segment_anthro = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_anthro_regr.npy\"\n",
    "    np_array_file_string_label = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" +  str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label_regr.npy\"\n",
    "    \n",
    "if os.path.isfile(np_array_file_string_label):   # if this file already exists, load the relevant SavedNPArrays\n",
    "    print('Pulling down ' + np_array_file_string_label)\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments_timeseries = np.load(np_array_file_string_segment_timeseries, allow_pickle=True)\n",
    "        segments_anthro = np.load(np_array_file_string_segment_anthro, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    print('Creating ' + np_array_file_string_label)\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        if  model_architecture == 'FCN':\n",
    "            segments, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset, model_architecture, speed_bucket_size, input_window_size, num_channels, num_anthropometrics, label_window_size, sample_stride)\n",
    "        elif model_architecture == 'CNN':\n",
    "            segments_timeseries, segments_anthro, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset, model_architecture, speed_bucket_size, input_window_size, num_channels, num_anthropometrics,label_window_size, sample_stride)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "        labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "        labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "        np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "#     else:\n",
    "#         labels_to_number = [0] # A bandaid placeholder to report labels_to_number in confusion matrix script\n",
    "    if  model_architecture == 'FCN':\n",
    "        np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        np.save(np_array_file_string_segment_timeseries, segments_timeseries, allow_pickle=True)\n",
    "        np.save(np_array_file_string_segment_anthro, segments_anthro, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "\n",
    "if speed_bucket_size != 'none_use_regression':\n",
    "    num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed data!\n"
     ]
    }
   ],
   "source": [
    "print('Successfully preprocessed data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data into training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(labels)) < 0.90 # split data into 90% train, 10% dev, based on lenghto of labels\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "elif model_architecture == 'CNN':\n",
    "    X_train_timeseries = segments_timeseries[train_dev_split]\n",
    "    X_test_timeseries = segments_timeseries[~train_dev_split]\n",
    "    X_train_anthro = segments_anthro[train_dev_split]\n",
    "    X_test_anthro = segments_anthro[~train_dev_split]\n",
    "\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement NN architecture in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnModel():\n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers[0], activation=activations_fc_layers[0], input_shape=(input_window_size*num_channels + num_anthropometrics,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_fc_layers[0]))\n",
    "    # Intermediate layers\n",
    "    for L in range(1, num_hidden_fc_layers):\n",
    "        model.add(Dense(num_hidden_units_fc_layers[L], activation=activations_fc_layers[L]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "        \n",
    "    model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "def cnnModel_multInput(): # (inputs, outputs):\n",
    "    # CNN over time-series data\n",
    "    input_cnn = Input(shape=(input_window_size, num_channels))\n",
    "    conv1 = Conv1D(num_filters, kernel_size,activation=activation_conv_layer)(input_cnn)\n",
    "    pool1 = MaxPooling1D(pool_size=max_pool_kernel_size, padding='valid', strides=(2))(conv1)\n",
    "    conv2 = Conv1D(num_filters//10, kernel_size, activation=activation_conv_layer)(pool1) # add additional CNN layer\n",
    "    flat1 = Flatten()(conv2)\n",
    "    # Include anthropometric data\n",
    "    input_anthro = Input(shape=(num_anthropometrics,))\n",
    "    # Concatenate result of CNN with antropometric data\n",
    "    merged = concatenate([flat1, input_anthro])\n",
    "    # Add fully connected hident layers after concatenating (at least one)\n",
    "    fc1 = Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])(merged) # add first fully connected layer\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        None\n",
    "        # NEED TO CORRECT BEFORE USE    \n",
    "        #something like this\n",
    "        #fc_L = Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L])(fc_L-1) # add first fully connected layer\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        output = Dense(num_buckets_total, activation='softmax')(fc1) # will need to change with more fc layers\n",
    "        \n",
    "        #Test Code for Dot Product\n",
    "        #sofma = Dense(num_buckets_total, activation='softmax')(fc1) # will need to change with more fc layers\n",
    "        #output = Lambda(lambda sofma: sofma*labels_to_number)\n",
    "        #output = Merge([sofma, labels_to_number], mode='dot', dot_axes=(1, 1))\n",
    "        \n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        output = Dense(1,activation='linear')(fc1)  \n",
    "    model = Model(inputs = [input_cnn, input_anthro], outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  model_architecture == 'FCN':\n",
    "    model = fcnModel()\n",
    "elif model_architecture == 'CNN':\n",
    "    model = cnnModel_multInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 26, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 22, 40)       1240        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 9, 40)        0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5, 4)         804         max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 20)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 24)           0           flatten_4[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50)           1250        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            51          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,345\n",
      "Trainable params: 3,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss functions and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss functions\n",
    "\n",
    "def class_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "    # Note: we cannot define RMSE directly in Keras since the loss function is defined for one training example at a time\n",
    "\n",
    "def class_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "\n",
    "def class_mape(y_true, y_pred):\n",
    "    diff = K.abs((K.sum(y_true * labels_to_number,axis=-1,keepdims=True) - K.sum(y_pred * labels_to_number,axis=-1,keepdims=True)) / K.clip(K.abs(K.sum(y_true * labels_to_number,axis=-1,keepdims=True)),K.epsilon(),None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "def class_percent_2buckLow(y_true, y_pred): # percent of times the prediction is 2 buckets below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())\n",
    "\n",
    "def class_percent_2buckHigh(y_true, y_pred): # percent of times the prediction is 2 buckets above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx())    \n",
    "\n",
    "def class_percent_2buckRange(y_true, y_pred): # percent of times the prediction is within 2 buckets of true value\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure training run ... model loss and optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show progress if running on local, only show final epoch values when running on Sherlock, don't show anything in a loop\n",
    "if (machine_to_run_script == 'local') & (runs_per_file == 'single'):\n",
    "    verbose_option = 1 # show progress bar\n",
    "elif (machine_to_run_script == 'Sherlock') & (runs_per_file == 'single'):\n",
    "    verbose_option = 2 # one line per epoch # 0 stay silent\n",
    "elif (machine_to_run_script == 'local') & (runs_per_file == 'train_loop'):\n",
    "    verbose_option = 2 # one line per epoch # 0 stay silent\n",
    "elif (machine_to_run_script == 'Sherlock') & (runs_per_file == 'train_loop'):\n",
    "    verbose_option = 0 # silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model 20180917_191016: bs=64, lr=0.0001, opt=gradient, lf=mae, epochs=100\n",
      "Train on 17036 samples, validate on 1909 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 2.5979 - mean_squared_error: 7.2599 - mean_absolute_error: 2.5979 - val_loss: 2.4656 - val_mean_squared_error: 6.6046 - val_mean_absolute_error: 2.4656\n",
      "Epoch 2/100\n",
      " - 2s - loss: 2.3911 - mean_squared_error: 6.2558 - mean_absolute_error: 2.3911 - val_loss: 2.2509 - val_mean_squared_error: 5.6183 - val_mean_absolute_error: 2.2509\n",
      "Epoch 3/100\n",
      " - 2s - loss: 2.1473 - mean_squared_error: 5.1871 - mean_absolute_error: 2.1473 - val_loss: 1.9754 - val_mean_squared_error: 4.4894 - val_mean_absolute_error: 1.9754\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.8123 - mean_squared_error: 3.9038 - mean_absolute_error: 1.8123 - val_loss: 1.5875 - val_mean_squared_error: 3.1137 - val_mean_absolute_error: 1.5875\n",
      "Epoch 5/100\n",
      " - 2s - loss: 1.3745 - mean_squared_error: 2.4684 - mean_absolute_error: 1.3745 - val_loss: 1.1362 - val_mean_squared_error: 1.7833 - val_mean_absolute_error: 1.1362\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.9625 - mean_squared_error: 1.3597 - mean_absolute_error: 0.9625 - val_loss: 0.8119 - val_mean_squared_error: 1.0066 - val_mean_absolute_error: 0.8119\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.7280 - mean_squared_error: 0.8373 - mean_absolute_error: 0.7280 - val_loss: 0.6630 - val_mean_squared_error: 0.7149 - val_mean_absolute_error: 0.6630\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.6232 - mean_squared_error: 0.6409 - mean_absolute_error: 0.6232 - val_loss: 0.5906 - val_mean_squared_error: 0.5959 - val_mean_absolute_error: 0.5906\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.5641 - mean_squared_error: 0.5498 - mean_absolute_error: 0.5641 - val_loss: 0.5441 - val_mean_squared_error: 0.5282 - val_mean_absolute_error: 0.5441\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.5236 - mean_squared_error: 0.4933 - mean_absolute_error: 0.5236 - val_loss: 0.5099 - val_mean_squared_error: 0.4822 - val_mean_absolute_error: 0.5099\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.4932 - mean_squared_error: 0.4538 - mean_absolute_error: 0.4932 - val_loss: 0.4835 - val_mean_squared_error: 0.4486 - val_mean_absolute_error: 0.4835\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.4698 - mean_squared_error: 0.4250 - mean_absolute_error: 0.4698 - val_loss: 0.4629 - val_mean_squared_error: 0.4240 - val_mean_absolute_error: 0.4629\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.4517 - mean_squared_error: 0.4036 - mean_absolute_error: 0.4517 - val_loss: 0.4468 - val_mean_squared_error: 0.4054 - val_mean_absolute_error: 0.4468\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.4374 - mean_squared_error: 0.3873 - mean_absolute_error: 0.4374 - val_loss: 0.4335 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.4335\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.4260 - mean_squared_error: 0.3741 - mean_absolute_error: 0.4260 - val_loss: 0.4231 - val_mean_squared_error: 0.3791 - val_mean_absolute_error: 0.4231\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.4166 - mean_squared_error: 0.3643 - mean_absolute_error: 0.4166 - val_loss: 0.4144 - val_mean_squared_error: 0.3692 - val_mean_absolute_error: 0.4144\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.4088 - mean_squared_error: 0.3557 - mean_absolute_error: 0.4088 - val_loss: 0.4072 - val_mean_squared_error: 0.3610 - val_mean_absolute_error: 0.4072\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.4021 - mean_squared_error: 0.3483 - mean_absolute_error: 0.4021 - val_loss: 0.4010 - val_mean_squared_error: 0.3553 - val_mean_absolute_error: 0.4010\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.3965 - mean_squared_error: 0.3427 - mean_absolute_error: 0.3965 - val_loss: 0.3957 - val_mean_squared_error: 0.3494 - val_mean_absolute_error: 0.3957\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.3916 - mean_squared_error: 0.3372 - mean_absolute_error: 0.3916 - val_loss: 0.3913 - val_mean_squared_error: 0.3458 - val_mean_absolute_error: 0.3913\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.3874 - mean_squared_error: 0.3331 - mean_absolute_error: 0.3874 - val_loss: 0.3875 - val_mean_squared_error: 0.3417 - val_mean_absolute_error: 0.3875\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.3837 - mean_squared_error: 0.3295 - mean_absolute_error: 0.3837 - val_loss: 0.3841 - val_mean_squared_error: 0.3375 - val_mean_absolute_error: 0.3841\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.3804 - mean_squared_error: 0.3259 - mean_absolute_error: 0.3804 - val_loss: 0.3811 - val_mean_squared_error: 0.3342 - val_mean_absolute_error: 0.3811\n",
      "Epoch 24/100\n",
      " - 2s - loss: 0.3775 - mean_squared_error: 0.3227 - mean_absolute_error: 0.3775 - val_loss: 0.3784 - val_mean_squared_error: 0.3316 - val_mean_absolute_error: 0.3784\n",
      "Epoch 25/100\n",
      " - 2s - loss: 0.3748 - mean_squared_error: 0.3200 - mean_absolute_error: 0.3748 - val_loss: 0.3760 - val_mean_squared_error: 0.3295 - val_mean_absolute_error: 0.3760\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.3723 - mean_squared_error: 0.3176 - mean_absolute_error: 0.3723 - val_loss: 0.3737 - val_mean_squared_error: 0.3273 - val_mean_absolute_error: 0.3737\n",
      "Epoch 27/100\n",
      " - 2s - loss: 0.3700 - mean_squared_error: 0.3153 - mean_absolute_error: 0.3700 - val_loss: 0.3716 - val_mean_squared_error: 0.3251 - val_mean_absolute_error: 0.3716\n",
      "Epoch 28/100\n",
      " - 2s - loss: 0.3680 - mean_squared_error: 0.3134 - mean_absolute_error: 0.3680 - val_loss: 0.3696 - val_mean_squared_error: 0.3224 - val_mean_absolute_error: 0.3696\n",
      "Epoch 29/100\n"
     ]
    }
   ],
   "source": [
    "for batch_size,learning_rate,optimizer_type,loss_function,training_epochs in itertools.product(batch_size_all,learning_rate_all,optimizer_type_all,loss_function_all,training_epochs_all):\n",
    "    \n",
    "    # Model run file naming conventions\n",
    "    file_name = strftime(\"%Y%m%d_%H%M%S\", gmtime()) # user input for filename of saved model\n",
    "    print('Starting to train model ' + file_name + ': bs='+str(batch_size)+', lr='+str(learning_rate)+', opt='+optimizer_type+', lf='+loss_function+', epochs='+str(training_epochs))\n",
    "    \n",
    "    # If desired, load weights from a previous model to start with model\n",
    "    if previous_model_weights_to_load != \"\":\n",
    "        model.load_weights(folder_head_loc + \"Model Final Parameters/\" + previous_model_weights_to_load)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        model_optimizer = optimizers.Adam(lr = learning_rate) #, decay, beta_1, beta_2 are HPs\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        model_optimizer = optimizers.RMSprop(lr = learning_rate) #, decay, rho\n",
    "    elif optimizer_type == 'gradient':\n",
    "        model_optimizer = optimizers.SGD(lr = learning_rate) #, decay, momentum\n",
    "    \n",
    "    # Compile model with appropriate loss function\n",
    "    if speed_bucket_size != 'none_use_regression': # if performing classification, ALWAYS use cross-entropy loss\n",
    "        model.compile(loss ='categorical_crossentropy', optimizer=model_optimizer, metrics=['accuracy',class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n",
    "    else:                                          # if performing regression, use mean squared error or mean absolute error\n",
    "        if loss_function == 'categorical_crossentropy': raise NameError('Are you sure you want to use cross entropy loss with a regression tasks!?')\n",
    "        model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    if  model_architecture == 'FCN':\n",
    "        history = model.fit(X_train, y_train, batch_size= batch_size, epochs=training_epochs, verbose=verbose_option, validation_data=(X_test, y_test))\n",
    "    elif model_architecture == 'CNN':\n",
    "        history = model.fit([X_train_timeseries, X_train_anthro], y_train, batch_size= batch_size, epochs=training_epochs, verbose=verbose_option, validation_data=([X_test_timeseries, X_test_anthro], y_test))\n",
    "    end_time=time.time()\n",
    "    \n",
    "    print('Finished training model ' + file_name + '!')\n",
    "        \n",
    "    # Transform key results into a np arrary\n",
    "    trainAccuracy_1 = np.squeeze(history.history[accuracy_reporting_metric_1])\n",
    "    devAccuracy_1 = np.squeeze(history.history[dev_reporting_metric_1])\n",
    "    trainAccuracy_2 = np.squeeze(history.history[accuracy_reporting_metric_2])\n",
    "    devAccuracy_2 = np.squeeze(history.history[dev_reporting_metric_2])    \n",
    "    trainAccuracy_3 = np.squeeze(history.history[accuracy_reporting_metric_3])\n",
    "    devAccuracy_3 = np.squeeze(history.history[dev_reporting_metric_3])\n",
    "    trainAccuracy_4 = np.squeeze(history.history[accuracy_reporting_metric_4])\n",
    "    devAccuracy_4 = np.squeeze(history.history[dev_reporting_metric_4])\n",
    "    epochs = np.squeeze(range(1,training_epochs + 1))\n",
    "    # Declare final values for results\n",
    "    final_accuracy_1 = history.history[accuracy_reporting_metric_1][training_epochs - 1]\n",
    "    final_accuracy_dev_1 = history.history[dev_reporting_metric_1][training_epochs - 1]\n",
    "    final_accuracy_2 = history.history[accuracy_reporting_metric_2][training_epochs - 1]\n",
    "    final_accuracy_dev_2 = history.history[dev_reporting_metric_2][training_epochs - 1]\n",
    "    final_accuracy_3 = history.history[accuracy_reporting_metric_3][training_epochs - 1]\n",
    "    final_accuracy_dev_3 = history.history[dev_reporting_metric_3][training_epochs - 1]\n",
    "    final_accuracy_4 = history.history[accuracy_reporting_metric_4][training_epochs - 1]\n",
    "    final_accuracy_dev_4 = history.history[dev_reporting_metric_4][training_epochs - 1]\n",
    "    # Save results to a .csv in the \"Learning Curve Results\"\n",
    "    df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([epochs,devAccuracy_1, devAccuracy_2, devAccuracy_3, devAccuracy_4])))\n",
    "    filepath_acc = folder_head_loc + \"Learning Curves/\" + str(file_name) +\"_AccuracyPerEpoch_Data\" + \".csv\"\n",
    "    df_devAccuracy.to_csv(filepath_acc, header = [\"Epochs\", dev_reporting_metric_1, dev_reporting_metric_2, dev_reporting_metric_3, 'acc'], index=False)\n",
    "\n",
    "    # Create a plot of Learning Curves\n",
    "    create_learning_curves_from_model(machine_to_run_script\n",
    "                ,trainAccuracy_4\n",
    "                ,devAccuracy_4\n",
    "                ,trainAccuracy_1\n",
    "                ,devAccuracy_1\n",
    "                ,trainAccuracy_2\n",
    "                ,devAccuracy_2\n",
    "                ,dev_reporting_metric_1\n",
    "                ,final_accuracy_dev_1\n",
    "                ,dev_reporting_metric_2\n",
    "                ,final_accuracy_dev_2\n",
    "                ,loss_function\n",
    "                ,learning_rate\n",
    "                ,batch_size\n",
    "                ,speed_bucket_size\n",
    "                ,training_epochs\n",
    "                ,input_window_size\n",
    "                ,labels_to_number\n",
    "                ,accuracy_reporting_metric_1\n",
    "                ,accuracy_reporting_metric_2\n",
    "                ,accuracy_reporting_metric_3\n",
    "                ,plot_note\n",
    "                ,folder_head_loc\n",
    "                ,file_name)\n",
    "\n",
    "    # Add the results of the most recent run to the results file for documentation\n",
    "    populate_results_performance_table(folder_head_loc,\n",
    "                results_file_name,\n",
    "                model_architecture,\n",
    "                file_name,\n",
    "                myFileLocation,\n",
    "                training_epochs,  \n",
    "                end_time,\n",
    "                start_time,\n",
    "                final_accuracy_1,\n",
    "                final_accuracy_dev_1,\n",
    "                final_accuracy_2,\n",
    "                final_accuracy_dev_2,\n",
    "                final_accuracy_3,\n",
    "                final_accuracy_dev_3,\n",
    "                batch_size,    \n",
    "                learning_rate,\n",
    "                speed_bucket_size,\n",
    "                loss_function,\n",
    "                input_window_size,\n",
    "                label_window_size,\n",
    "                optimizer_type,\n",
    "                accuracy_reporting_metric_1,\n",
    "                accuracy_reporting_metric_2,\n",
    "                accuracy_reporting_metric_3,\n",
    "                num_hidden_fc_layers,\n",
    "                hidden_units_strategy,\n",
    "                activations_strategy,\n",
    "                dropout_rates,\n",
    "                hidden_units_strategy_CNN,\n",
    "                num_filters,\n",
    "                kernel_size,\n",
    "                sample_stride,\n",
    "                activation_conv_layer,\n",
    "                activations_strategy_CNN,\n",
    "                max_pool_kernel_size)    \n",
    "        \n",
    "    # Create confusion matrices\n",
    "    if speed_bucket_size != 'none_use_regression': \n",
    "        if  model_architecture == 'FCN':\n",
    "                y_pred = model.predict(X_test)\n",
    "        elif model_architecture == 'CNN':\n",
    "            y_pred = model.predict([X_test_timeseries, X_test_anthro])\n",
    "        y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "        y_true = y_test\n",
    "        y_true_argmax = np.argmax(y_true, axis=1)  \n",
    "        create_conf_matrix_from_model(machine_to_run_script\n",
    "                ,y_true_argmax\n",
    "                ,y_pred_argmax\n",
    "                ,folder_head_loc\n",
    "                ,file_name)\n",
    "    \n",
    "    print('Successfully created plots and figures for model ' + file_name)\n",
    "    \n",
    "    # Save model weights\n",
    "    if machine_to_run_script == 'local':\n",
    "        completed_model_name = file_name + \"_\" + model_architecture\n",
    "        model.save_weights(folder_head_loc + \"Model Final Parameters/\" + completed_model_name + '_weights.h5')\n",
    "        # THIS DOES NOT WORK IN SHERLOCK RIGHT NOW\n",
    "        \n",
    "print('Completed all training runs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
