{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumo Run - Deep FFCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is this script being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_to_run_script = 'local' # 'Sherlock', 'local'\n",
    "runs_per_file = 'train_loop' # 'single', 'train_loop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script is starting!\n",
      "Successfully loaded keras!\n"
     ]
    }
   ],
   "source": [
    "print('Script is starting!') # these are used for Sherlock to check what is causing the hiccup\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, Input, Lambda, Merge\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers \n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "print('Successfully loaded keras!')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import socket\n",
    "import pickle\n",
    "import os.path\n",
    "#import dill #can't find dill\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# from SpeedPrediction_Helper import *\n",
    "\n",
    "np.random.seed(7) # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "num_channels = 6 # number of time-series channels of data (i.e. 7 kinematic features) #NOTE: Change to 6 by removing Pelvic Tilt (recommended by Lumo)\n",
    "num_anthropometrics = 4 # number of user anthropometric data elements\n",
    "input_window_size = 26 # number of timestamps used in the input for prediction (i.e. the input window)\n",
    "label_window_size = 20 # number of timestamps used to label the speed we will be predicting\n",
    "speed_bucket_size = 'none_use_regression' # how to round the data for classification task. Consider '0.5', '0.1', and 'none_use_regression'\n",
    "\n",
    "previous_model_weights_to_load = \"\" # If non-empty, load weights from a previous model (note: architectures must be identical)\n",
    "model_architecture = 'CNN' # 'FCN', 'CNN'\n",
    "data_input_table_structure = 'Raw_Timeseries' # 'Vectorized_By_Row' 'Raw_Timeseries'\n",
    "if machine_to_run_script == 'local':\n",
    "    folder_head_loc = '../';\n",
    "    folder_data_loc = 'C:/Users/adam/Documents/Lumo/Lumo Data/'\n",
    "elif machine_to_run_script == 'Sherlock':\n",
    "    folder_head_loc = '/home/users/agotlin/lumo/'\n",
    "    folder_data_loc = '/home/users/agotlin/SherlockDataFiles/'\n",
    "myFileName = 'TimeSeries_InputRaw_1000Runs_QuarterSample'\n",
    "myFileLocation = folder_data_loc + myFileName + '.csv'\n",
    "    # Other data files/folders to potentially use:\n",
    "    # 'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "    # 'TimeSeries_InputRaw_1000Runs'  |  'TimeSeries_InputRaw_1000Runs_QuarterSample'  |  'TimeSeries_InputRaw_1000Runs_Top10kRowsSample'\n",
    "    \n",
    "# Training strategy\n",
    "batch_size_all = [256] # we used 50 for CNN, 128 for FCN\n",
    "learning_rate_all = [0.0001] # we used 0.001 for FCN, 0.0001 for CNN\n",
    "optimizer_type_all = ['adam'] # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "loss_function_all = ['categorical_crossentropy'] # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'    \n",
    "training_epochs_all = [5]\n",
    "    \n",
    "# Fully Connected Architecture\n",
    "\n",
    "num_hidden_units_fc_layers = [256, 256, 256, 128, 128, 128]\n",
    "hidden_units_strategy = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers) # document strategy \n",
    "num_hidden_fc_layers = len(num_hidden_units_fc_layers) # document strategy\n",
    "activations_fc_layers = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "activations_strategy = ''.join(str(num) + \"_\" for num in activations_fc_layers) # document strategy\n",
    "dropout_rate_fc_layers = [1.0, 1.0, 1.0, 0.8, 0.8, 0.8]\n",
    "dropout_rates = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers) # document strategy\n",
    "\n",
    "# Convolutional Architecture\n",
    "    \n",
    "sample_stride = input_window_size/2 # how many timestamps to shift over between each unique training example # 18, input_window_size/2\n",
    "num_filters = 40 # number of filters in Conv2D layer (aka depth) # we used 40, ex; used 128\n",
    "kernel_size = 3 # kernal size of the Conv2D layer # we use 6, example used 2, I would guess closer to 3\n",
    "activation_conv_layer = \"relu\" # options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "max_pool_kernel_size = 3 # max pooling window size# we use 6, example used 2, I don't agre with 6\n",
    "conv_layer_dropout = 0.2 # dropout ratio for dropout layer # we don't use in our model\n",
    "\n",
    "num_hidden_units_fc_layers_CNN = [50]\n",
    "hidden_units_strategy_CNN = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers_CNN) # document strategy \n",
    "num_hidden_fc_layers_CNN = len(num_hidden_units_fc_layers_CNN) # document strategy\n",
    "activations_fc_layers_CNN = ['tanh']\n",
    "activations_strategy_CNN = ''.join(str(num) + \"_\" for num in activations_fc_layers_CNN) # document strategy\n",
    "dropout_rate_fc_layers_CNN = [1.0]\n",
    "dropout_rates_CNN = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers_CNN) # document strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Automatic Reporting and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 3 most interesting evaluation metrics to report on in final plots\n",
    "if speed_bucket_size != 'none_use_regression': \n",
    "    accuracy_reporting_metric_1 = 'class_mae' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "    accuracy_reporting_metric_2 = 'class_mse' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "    accuracy_reporting_metric_3 = 'class_percent_2buckRange' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3    \n",
    "    accuracy_reporting_metric_4 = 'acc' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_4 = 'val_' + accuracy_reporting_metric_4\n",
    "else:\n",
    "    accuracy_reporting_metric_1 = 'mean_absolute_error' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "    accuracy_reporting_metric_2 = 'mean_squared_error' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "    accuracy_reporting_metric_3 = 'loss' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "    dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3\n",
    "    accuracy_reporting_metric_4 = 'loss' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange' # A bandaid since the results reporting script requires 4 metrics\n",
    "    dev_reporting_metric_4 = 'val_' + accuracy_reporting_metric_4    \n",
    "\n",
    "# File naming conventions\n",
    "plot_note = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180911\" + \"_\" + model_architecture\n",
    "\n",
    "# Style of matlab plots to produce\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeedPrediction_Helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation'] # normalize all columns from gender to pelvic_tilt\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_rotation_lag_0'] # normalize all columns from gender to pelvic_rotation_lag_0\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly normalized data!\n"
     ]
    }
   ],
   "source": [
    "print('Successfuly normalized data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling down C:/Users/adam/Documents/Lumo/Lumo Data/SavedNPArrays/TimeSeries_InputRaw_1000Runs_QuarterSample_CNN_26_20_13.0_label.npy\n"
     ]
    }
   ],
   "source": [
    "np_array_file_string_segment = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_segment_timeseries = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_timeseries.npy\"\n",
    "np_array_file_string_segment_anthro = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_anthro.npy\"\n",
    "np_array_file_string_label = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" +  str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if speed_bucket_size == 'none_use_regression': # A bandaid script since regression and classification were not differentiated in above naming convention\n",
    "    np_array_file_string_segment = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_regr.npy\"\n",
    "    np_array_file_string_segment_timeseries = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_timeseries_regr.npy\"\n",
    "    np_array_file_string_segment_anthro = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment_anthro_regr.npy\"\n",
    "    np_array_file_string_label = folder_data_loc + \"SavedNPArrays/\" + str(myFileName) + \"_\" + model_architecture + \"_\" +  str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label_regr.npy\"\n",
    "    \n",
    "if os.path.isfile(np_array_file_string_label):   # if this file already exists, load the relevant SavedNPArrays\n",
    "    print('Pulling down ' + np_array_file_string_label)\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments_timeseries = np.load(np_array_file_string_segment_timeseries, allow_pickle=True)\n",
    "        segments_anthro = np.load(np_array_file_string_segment_anthro, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    print('Creating ' + np_array_file_string_label)\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        if  model_architecture == 'FCN':\n",
    "            segments, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset, model_architecture, speed_bucket_size, input_window_size, num_channels, num_anthropometrics, label_window_size, sample_stride)\n",
    "        elif model_architecture == 'CNN':\n",
    "            segments_timeseries, segments_anthro, labels = segment_signal_w_concat(dataset_inputs_normalized, dataset, model_architecture, speed_bucket_size, input_window_size, num_channels, num_anthropometrics,label_window_size, sample_stride)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "        labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "        labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "        np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "    else:\n",
    "        labels_to_number = [0] # A bandaid placeholder to report labels_to_number in confusion matrix script\n",
    "    if  model_architecture == 'FCN':\n",
    "        np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    elif model_architecture == 'CNN':\n",
    "        np.save(np_array_file_string_segment_timeseries, segments_timeseries, allow_pickle=True)\n",
    "        np.save(np_array_file_string_segment_anthro, segments_anthro, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "\n",
    "if speed_bucket_size != 'none_use_regression':\n",
    "    num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed data!\n"
     ]
    }
   ],
   "source": [
    "print('Successfully preprocessed data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data into training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(labels)) < 0.90 # split data into 90% train, 10% dev, based on lenghto of labels\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "elif model_architecture == 'CNN':\n",
    "    X_train_timeseries = segments_timeseries[train_dev_split]\n",
    "    X_test_timeseries = segments_timeseries[~train_dev_split]\n",
    "    X_train_anthro = segments_anthro[train_dev_split]\n",
    "    X_test_anthro = segments_anthro[~train_dev_split]\n",
    "\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement NN architecture in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnModel():\n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers[0], activation=activations_fc_layers[0], input_shape=(input_window_size*num_channels + num_anthropometrics,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_fc_layers[0]))\n",
    "    # Intermediate layers\n",
    "    for L in range(1, num_hidden_fc_layers):\n",
    "        model.add(Dense(num_hidden_units_fc_layers[L], activation=activations_fc_layers[L]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "        \n",
    "    model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "def cnnModel_multInput(): # (inputs, outputs):\n",
    "    # CNN over time-series data\n",
    "    input_cnn = Input(shape=(input_window_size, num_channels))\n",
    "    conv1 = Conv1D(num_filters, kernel_size,activation=activation_conv_layer)(input_cnn)\n",
    "    pool1 = MaxPooling1D(pool_size=max_pool_kernel_size, padding='valid', strides=(2))(conv1)\n",
    "    conv2 = Conv1D(num_filters//10, kernel_size, activation=activation_conv_layer)(pool1) # add additional CNN layer\n",
    "    flat1 = Flatten()(conv2)\n",
    "    # Include anthropometric data\n",
    "    input_anthro = Input(shape=(num_anthropometrics,))\n",
    "    # Concatenate result of CNN with antropometric data\n",
    "    merged = concatenate([flat1, input_anthro])\n",
    "    # Add fully connected hident layers after concatenating (at least one)\n",
    "    fc1 = Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])(merged) # add first fully connected layer\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        None\n",
    "        # NEED TO CORRECT BEFORE USE    \n",
    "        #something like this\n",
    "        #fc_L = Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L])(fc_L-1) # add first fully connected layer\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        output = Dense(num_buckets_total, activation='softmax')(fc1) # will need to change with more fc layers\n",
    "        \n",
    "        #Test Code for Dot Product\n",
    "        #sofma = Dense(num_buckets_total, activation='softmax')(fc1) # will need to change with more fc layers\n",
    "        #output = Lambda(lambda sofma: sofma*labels_to_number)\n",
    "        #output = Merge([sofma, labels_to_number], mode='dot', dot_axes=(1, 1))\n",
    "        \n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        output = Dense(1,activation='linear')(fc1)  \n",
    "    model = Model(inputs = [input_cnn, input_anthro], outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  model_architecture == 'FCN':\n",
    "    model = fcnModel()\n",
    "elif model_architecture == 'CNN':\n",
    "    model = cnnModel_multInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 26, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 24, 40)       760         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 11, 40)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 9, 4)         484         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 36)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 40)           0           flatten_3[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           2050        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 46)           2346        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,640\n",
      "Trainable params: 5,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss functions and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss functions\n",
    "\n",
    "def class_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "    # Note: we cannot define RMSE directly in Keras since the loss function is defined for one training example at a time\n",
    "\n",
    "def class_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "\n",
    "def class_mape(y_true, y_pred):\n",
    "    diff = K.abs((K.sum(y_true * labels_to_number,axis=-1,keepdims=True) - K.sum(y_pred * labels_to_number,axis=-1,keepdims=True)) / K.clip(K.abs(K.sum(y_true * labels_to_number,axis=-1,keepdims=True)),K.epsilon(),None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "def class_percent_2buckLow(y_true, y_pred): # percent of times the prediction is 2 buckets below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())\n",
    "\n",
    "def class_percent_2buckHigh(y_true, y_pred): # percent of times the prediction is 2 buckets above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx())    \n",
    "\n",
    "def class_percent_2buckRange(y_true, y_pred): # percent of times the prediction is within 2 buckets of true value\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure training run ... model loss and optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show progress if running on local, only show final epoch values when running on Sherlock, don't show anything in a loop\n",
    "if (machine_to_run_script == 'local') & (runs_per_file == 'single'):\n",
    "    verbose_option = 1 # show progress bar\n",
    "elif (machine_to_run_script == 'Sherlock') & (runs_per_file == 'single'):\n",
    "    verbose_option = 2 # one line per epoch # 0 stay silent\n",
    "elif (machine_to_run_script == 'local') & (runs_per_file == 'train_loop'):\n",
    "    verbose_option = 2 # one line per epoch # 0 stay silent\n",
    "elif (machine_to_run_script == 'Sherlock') & (runs_per_file == 'train_loop'):\n",
    "    verbose_option = 0 # silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model 20180918_220217: bs=256, lr=0.0001, opt=adam, lf=categorical_crossentropy, epochs=5\n",
      "Train on 17067 samples, validate on 1878 samples\n",
      "Epoch 1/5\n",
      " - 2s - loss: 3.7955 - acc: 0.0399 - class_percent_2buckRange: 0.1771 - class_mae: 0.5869 - class_mse: 0.5363 - val_loss: 3.7304 - val_acc: 0.0458 - val_class_percent_2buckRange: 0.2252 - val_class_mae: 0.5829 - val_class_mse: 0.5414\n",
      "Epoch 2/5\n",
      " - 1s - loss: 3.6183 - acc: 0.0633 - class_percent_2buckRange: 0.2808 - class_mae: 0.5543 - class_mse: 0.4910 - val_loss: 3.5212 - val_acc: 0.0660 - val_class_percent_2buckRange: 0.3184 - val_class_mae: 0.5256 - val_class_mse: 0.4663\n",
      "Epoch 3/5\n",
      " - 1s - loss: 3.3847 - acc: 0.0830 - class_percent_2buckRange: 0.3818 - class_mae: 0.4815 - class_mse: 0.4073 - val_loss: 3.3031 - val_acc: 0.0836 - val_class_percent_2buckRange: 0.4015 - val_class_mae: 0.4537 - val_class_mse: 0.3880\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-d8940ba582c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose_option\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_architecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'CNN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_timeseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_anthro\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose_option\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_timeseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_anthro\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mend_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_size,learning_rate,optimizer_type,loss_function,training_epochs in itertools.product(batch_size_all,learning_rate_all,optimizer_type_all,loss_function_all,training_epochs_all):\n",
    "    \n",
    "    # Model run file naming conventions\n",
    "    file_name = strftime(\"%Y%m%d_%H%M%S\", gmtime()) # user input for filename of saved model\n",
    "    print('Starting to train model ' + file_name + ': bs='+str(batch_size)+', lr='+str(learning_rate)+', opt='+optimizer_type+', lf='+loss_function+', epochs='+str(training_epochs))\n",
    "    \n",
    "    # If desired, load weights from a previous model to start with model\n",
    "    if previous_model_weights_to_load != \"\":\n",
    "        model.load_weights(folder_head_loc + \"Model Final Parameters/\" + previous_model_weights_to_load)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        model_optimizer = optimizers.Adam(lr = learning_rate) #, decay, beta_1, beta_2 are HPs\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        model_optimizer = optimizers.RMSprop(lr = learning_rate) #, decay, rho\n",
    "    elif optimizer_type == 'gradient':\n",
    "        model_optimizer = optimizers.SGD(lr = learning_rate) #, decay, momentum\n",
    "    \n",
    "    # Compile model with appropriate loss function\n",
    "    if speed_bucket_size != 'none_use_regression': # if performing classification, ALWAYS use cross-entropy loss\n",
    "        model.compile(loss ='categorical_crossentropy', optimizer=model_optimizer, metrics=['accuracy',class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n",
    "    else:                                          # if performing regression, use mean squared error or mean absolute error\n",
    "        if loss_function == 'categorical_crossentropy': raise NameError('Are you sure you want to use cross entropy loss with a regression tasks!?')\n",
    "        model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    if  model_architecture == 'FCN':\n",
    "        history = model.fit(X_train, y_train, batch_size= batch_size, epochs=training_epochs, verbose=verbose_option, validation_data=(X_test, y_test))\n",
    "    elif model_architecture == 'CNN':\n",
    "        history = model.fit([X_train_timeseries, X_train_anthro], y_train, batch_size= batch_size, epochs=training_epochs, verbose=verbose_option, validation_data=([X_test_timeseries, X_test_anthro], y_test))\n",
    "    end_time=time.time()\n",
    "    \n",
    "    print('Finished training model ' + file_name + '!')\n",
    "        \n",
    "    # Transform key results into a np arrary\n",
    "    trainAccuracy_1 = np.squeeze(history.history[accuracy_reporting_metric_1])\n",
    "    devAccuracy_1 = np.squeeze(history.history[dev_reporting_metric_1])\n",
    "    trainAccuracy_2 = np.squeeze(history.history[accuracy_reporting_metric_2])\n",
    "    devAccuracy_2 = np.squeeze(history.history[dev_reporting_metric_2])    \n",
    "    trainAccuracy_3 = np.squeeze(history.history[accuracy_reporting_metric_3])\n",
    "    devAccuracy_3 = np.squeeze(history.history[dev_reporting_metric_3])\n",
    "    trainAccuracy_4 = np.squeeze(history.history[accuracy_reporting_metric_4])\n",
    "    devAccuracy_4 = np.squeeze(history.history[dev_reporting_metric_4])\n",
    "    epochs = np.squeeze(range(1,training_epochs + 1))\n",
    "    # Declare final values for results\n",
    "    final_accuracy_1 = history.history[accuracy_reporting_metric_1][training_epochs - 1]\n",
    "    final_accuracy_dev_1 = history.history[dev_reporting_metric_1][training_epochs - 1]\n",
    "    final_accuracy_2 = history.history[accuracy_reporting_metric_2][training_epochs - 1]\n",
    "    final_accuracy_dev_2 = history.history[dev_reporting_metric_2][training_epochs - 1]\n",
    "    final_accuracy_3 = history.history[accuracy_reporting_metric_3][training_epochs - 1]\n",
    "    final_accuracy_dev_3 = history.history[dev_reporting_metric_3][training_epochs - 1]\n",
    "    final_accuracy_4 = history.history[accuracy_reporting_metric_4][training_epochs - 1]\n",
    "    final_accuracy_dev_4 = history.history[dev_reporting_metric_4][training_epochs - 1]\n",
    "    # Save results to a .csv in the \"Learning Curve Results\"\n",
    "    df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([epochs,devAccuracy_1, devAccuracy_2, devAccuracy_3, devAccuracy_4])))\n",
    "    filepath_acc = folder_head_loc + \"Learning Curves/\" + str(file_name) +\"_AccuracyPerEpoch_Data\" + \".csv\"\n",
    "    df_devAccuracy.to_csv(filepath_acc, header = [\"Epochs\", dev_reporting_metric_1, dev_reporting_metric_2, dev_reporting_metric_3, 'acc'], index=False)\n",
    "\n",
    "    # Create a plot of Learning Curves\n",
    "    create_learning_curves_from_model(machine_to_run_script\n",
    "                ,trainAccuracy_4\n",
    "                ,devAccuracy_4\n",
    "                ,trainAccuracy_1\n",
    "                ,devAccuracy_1\n",
    "                ,trainAccuracy_2\n",
    "                ,devAccuracy_2\n",
    "                ,dev_reporting_metric_1\n",
    "                ,final_accuracy_dev_1\n",
    "                ,dev_reporting_metric_2\n",
    "                ,final_accuracy_dev_2\n",
    "                ,loss_function\n",
    "                ,learning_rate\n",
    "                ,batch_size\n",
    "                ,speed_bucket_size\n",
    "                ,training_epochs\n",
    "                ,input_window_size\n",
    "                ,labels_to_number\n",
    "                ,accuracy_reporting_metric_1\n",
    "                ,accuracy_reporting_metric_2\n",
    "                ,accuracy_reporting_metric_3\n",
    "                ,plot_note\n",
    "                ,folder_head_loc\n",
    "                ,file_name)\n",
    "\n",
    "    # Add the results of the most recent run to the results file for documentation\n",
    "    populate_results_performance_table(folder_head_loc,\n",
    "                results_file_name,\n",
    "                model_architecture,\n",
    "                file_name,\n",
    "                myFileLocation,\n",
    "                training_epochs,  \n",
    "                end_time,\n",
    "                start_time,\n",
    "                final_accuracy_1,\n",
    "                final_accuracy_dev_1,\n",
    "                final_accuracy_2,\n",
    "                final_accuracy_dev_2,\n",
    "                final_accuracy_3,\n",
    "                final_accuracy_dev_3,\n",
    "                batch_size,    \n",
    "                learning_rate,\n",
    "                speed_bucket_size,\n",
    "                loss_function,\n",
    "                input_window_size,\n",
    "                label_window_size,\n",
    "                optimizer_type,\n",
    "                accuracy_reporting_metric_1,\n",
    "                accuracy_reporting_metric_2,\n",
    "                accuracy_reporting_metric_3,\n",
    "                num_hidden_fc_layers,\n",
    "                hidden_units_strategy,\n",
    "                activations_strategy,\n",
    "                dropout_rates,\n",
    "                hidden_units_strategy_CNN,\n",
    "                num_filters,\n",
    "                kernel_size,\n",
    "                sample_stride,\n",
    "                activation_conv_layer,\n",
    "                activations_strategy_CNN,\n",
    "                max_pool_kernel_size)    \n",
    "        \n",
    "#     # Create confusion matrices\n",
    "#     if speed_bucket_size != 'none_use_regression': \n",
    "#         if  model_architecture == 'FCN':\n",
    "#                 y_pred = model.predict(X_test)\n",
    "#         elif model_architecture == 'CNN':\n",
    "#             y_pred = model.predict([X_test_timeseries, X_test_anthro])\n",
    "#         y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "#         y_true = y_test\n",
    "#         y_true_argmax = np.argmax(y_true, axis=1)  \n",
    "#         create_conf_matrix_from_model(machine_to_run_script\n",
    "#                 ,y_true_argmax\n",
    "#                 ,y_pred_argmax\n",
    "#                 ,folder_head_loc\n",
    "#                 ,file_name)\n",
    "\n",
    "    if  model_architecture == 'FCN':\n",
    "            y_pred = model.predict(X_test)\n",
    "    elif model_architecture == 'CNN':\n",
    "        y_pred = model.predict([X_test_timeseries, X_test_anthro])\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "    y_true = y_test\n",
    "    y_true_argmax = np.argmax(y_true, axis=1)  \n",
    "    create_conf_matrix_from_model(machine_to_run_script\n",
    "            ,y_true_argmax\n",
    "            ,y_pred_argmax\n",
    "            ,folder_head_loc\n",
    "            ,file_name)\n",
    "\n",
    "    print('Successfully created plots and figures for model ' + file_name)\n",
    "    print(datetime.now())\n",
    "    \n",
    "    # Save model weights\n",
    "    if machine_to_run_script == 'local':\n",
    "        completed_model_name = file_name + \"_\" + model_architecture\n",
    "        model.save_weights(folder_head_loc + \"Model Final Parameters/\" + completed_model_name + '_weights.h5')\n",
    "        # THIS DOES NOT WORK IN SHERLOCK RIGHT NOW\n",
    "        \n",
    "print('Completed all training runs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
