{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Code from Primary NN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment_signal method with time tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "    # ACTIVELY WORKING ON THIS METHOD FOR SPEED OPTIMIZATION (RIGHT NOW IT TAKES TOO LONG)\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    TIME0 = time.time()\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    \n",
    "    TIME1 = time.time()\n",
    "    TIME_INIT = TIME1 - TIME0\n",
    "    time_tracker_full_loop = [0]\n",
    "    time_tracker_cols = [0]\n",
    "    time_tracker_segment_stack = [0]\n",
    "    time_tracker_label_stack = [0]\n",
    "    entered_stracking = [0]\n",
    "    count = 0\n",
    "    \n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        \n",
    "        time_tracker_full_loop_start = time.time()\n",
    "        #time_range_tracker = np.hstack([time_range_tracker, (time_tracker_full_loop[count+1] - time_tracker_full_loop[count]) ]) #count+\n",
    "        count += 1\n",
    "        time_tracker_cols_start = time.time()\n",
    "        \n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end]         \n",
    "        \n",
    "        time_tracker_cols_end = time.time()\n",
    "        time_tracker_cols = np.hstack([time_tracker_cols, time_tracker_cols_end - time_tracker_cols_start])\n",
    "        time_tracker_segment_stack_start = time.time()\n",
    "        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            \n",
    "            entered_stracking = np.hstack([entered_stracking, count])\n",
    "            \n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "            \n",
    "            time_tracker_segment_stack_end = time.time()\n",
    "            time_tracker_segment_stack = np.hstack([time_tracker_segment_stack, time_tracker_segment_stack_end - time_tracker_segment_stack_start])\n",
    "            time_tracker_label_stack_start = time.time()\n",
    "            \n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "            \n",
    "            time_tracker_segment_label_end = time.time()\n",
    "            time_tracker_label_stack = np.hstack([time_tracker_label_stack, time_tracker_segment_label_end - time_tracker_label_stack_start])\n",
    "            \n",
    "        else:\n",
    "            entered_stracking = np.hstack([entered_stracking, 0])\n",
    "        time_tracker_full_loop_end = time.time()\n",
    "        time_tracker_full_loop = np.hstack([time_tracker_full_loop, time_tracker_full_loop_end - time_tracker_full_loop_start])\n",
    "        \n",
    "    TIME2 = time.time()\n",
    "    TIME_LOOP = TIME2 - TIME1\n",
    "    print(TIME_INIT)\n",
    "    print(TIME_LOOP)\n",
    "    print(time_tracker_full_loop[0:1000])\n",
    "    print(time_tracker_full_loop[10000:11000])\n",
    "    print(time_tracker_cols[0:1000])\n",
    "    print(time_tracker_cols[10000:11000])\n",
    "    print(time_tracker_segment_stack[0:1000])\n",
    "    print(time_tracker_segment_stack[10000:11000])\n",
    "    print(time_tracker_label_stack[0:1000])\n",
    "    print(time_tracker_label_stack[10000:11000])\n",
    "    \n",
    "    print(entered_stracking)\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugger code for normalizing input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Helper Code\n",
    "\n",
    "# #dataset = read_data(myFileLocation)\n",
    "# dataset_full = dataset\n",
    "# print(dataset[1:4])\n",
    "# print(dataset_full[1:4])\n",
    "# #dataset['gender':'pelvic_tilt_lag_0'] = (dataset - dataset.mean())/dataset.std()\n",
    "# dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt_lag_0'] # normalize all columns from gender to pelvic_tilt_lag_0\n",
    "# dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "# dataset_full.drop([\"gender\":\"pelvic_tilt_lag_0\"])\n",
    "# dataset_full = [dataset_full, dataset_inputs_normalized]\n",
    "# print(dataset[1:4])\n",
    "# print(dataset_full[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuger code for saving segment and labels as NP arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging helper code\n",
    "\n",
    "#     if data_input_table_structure == 'Raw_Timeseries':\n",
    "#         segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "#     elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "#         segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "\n",
    "#     if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "#          labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "#          labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "#          num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)\n",
    "            \n",
    "\n",
    "            \n",
    "# segments2 = np.load('../Saved NP Arrays/TESTPICKLE2.npy', allow_pickle=True)\n",
    "# np.save('../Saved NP Arrays/TESTPICKLE2', segments, allow_pickle=True)\n",
    "# #save_pickle_obj(segments, 'TESTPICKLE')\n",
    "\n",
    "print(segments.shape[0])\n",
    "print(len(segments))\n",
    "#reshaped_segments = segments.reshape(len(segments), 1,input_num_timestamps, num_channels)\n",
    "\n",
    "print(segments[0,0,1:10])\n",
    "print(labels.shape)\n",
    "print(segments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables removed from code for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Variables for CNN model    # THIS IS SILLY. GET RID OF THESE TWO VARIABLES, ADD COMMENTS FOR EASE OF USE/READING INSTEAD\n",
    "if model_architecture == 'CNN':\n",
    "    num_rows_per_example = segments.shape[1] # same as input_window_size\n",
    "    num_cols_per_example = segments.shape[2] # same as total channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution accidentally 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.add(Conv2D(num_filters, (kernel_size,1),input_shape=(input_window_size, num_channels+num_anthropometrics,1),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling2D(pool_size=(max_pool_kernel_size,1),padding='valid',strides=(2,1)))\n",
    "    model.add(Conv2D(num_filters//10, (kernel_size,1),activation=activation_conv_layer)) # add additional CNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dev Split mistakes due to using Conv 2D and switching to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(segments)) < 0.90\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    #reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics, 1)\n",
    "        # Making a change to swith to Conv1D\n",
    "    print(segments.shape)\n",
    "    reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics)\n",
    "        # this line is not needed at all anymore, it does \n",
    "    print(reshapedSegments.shape)\n",
    "    X_train = reshapedSegments[train_dev_split]\n",
    "    X_test = reshapedSegments[~train_dev_split]\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippets from before Concatenated anthropometrics in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        #g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end] \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                #segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f])])\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal_label_only(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return labels\n",
    "\n",
    "# Preprocess data to input into model\n",
    "\n",
    "np_array_file_string_segment = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_label = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if os.path.isfile(np_array_file_string_segment):   # if this file already exists, load the relevant saved np arrays\n",
    "    segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "         labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "         labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "    np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "\n",
    "    \n",
    "def cnnModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(num_filters, (kernel_size),input_shape=(input_window_size, num_channels+num_anthropometrics),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling1D(pool_size=(max_pool_kernel_size),padding='valid',strides=(2)))\n",
    "    model.add(Conv1D(num_filters//10, (kernel_size),activation=activation_conv_layer)) # add additional CNN layer\n",
    "    # model.add(Dropout(dropOutRatio)) # not used in our model # adding a dropout layer for the regularization\n",
    "    model.add(Flatten()) # flatten the output in order to apply the fully connected layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])) # add first fully connected layer\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(dropout_rate_fc_layers_CNN[0]))\n",
    "    # Intermediate fully connected layerslayers\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        model.add(Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L]))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 0.5 speed bucket option (no longer used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In segment signal code\n",
    "\n",
    "#raw timeseries\n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "#vectorized        \n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.around(2*np.mean(labels_before_avg, axis=1),decimals=0)/2\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
