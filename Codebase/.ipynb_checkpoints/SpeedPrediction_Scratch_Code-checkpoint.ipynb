{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Code from Primary NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment_signal method with time tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "    # ACTIVELY WORKING ON THIS METHOD FOR SPEED OPTIMIZATION (RIGHT NOW IT TAKES TOO LONG)\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    TIME0 = time.time()\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    \n",
    "    TIME1 = time.time()\n",
    "    TIME_INIT = TIME1 - TIME0\n",
    "    time_tracker_full_loop = [0]\n",
    "    time_tracker_cols = [0]\n",
    "    time_tracker_segment_stack = [0]\n",
    "    time_tracker_label_stack = [0]\n",
    "    entered_stracking = [0]\n",
    "    count = 0\n",
    "    \n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        \n",
    "        time_tracker_full_loop_start = time.time()\n",
    "        #time_range_tracker = np.hstack([time_range_tracker, (time_tracker_full_loop[count+1] - time_tracker_full_loop[count]) ]) #count+\n",
    "        count += 1\n",
    "        time_tracker_cols_start = time.time()\n",
    "        \n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end]         \n",
    "        \n",
    "        time_tracker_cols_end = time.time()\n",
    "        time_tracker_cols = np.hstack([time_tracker_cols, time_tracker_cols_end - time_tracker_cols_start])\n",
    "        time_tracker_segment_stack_start = time.time()\n",
    "        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            \n",
    "            entered_stracking = np.hstack([entered_stracking, count])\n",
    "            \n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "            \n",
    "            time_tracker_segment_stack_end = time.time()\n",
    "            time_tracker_segment_stack = np.hstack([time_tracker_segment_stack, time_tracker_segment_stack_end - time_tracker_segment_stack_start])\n",
    "            time_tracker_label_stack_start = time.time()\n",
    "            \n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "            \n",
    "            time_tracker_segment_label_end = time.time()\n",
    "            time_tracker_label_stack = np.hstack([time_tracker_label_stack, time_tracker_segment_label_end - time_tracker_label_stack_start])\n",
    "            \n",
    "        else:\n",
    "            entered_stracking = np.hstack([entered_stracking, 0])\n",
    "        time_tracker_full_loop_end = time.time()\n",
    "        time_tracker_full_loop = np.hstack([time_tracker_full_loop, time_tracker_full_loop_end - time_tracker_full_loop_start])\n",
    "        \n",
    "    TIME2 = time.time()\n",
    "    TIME_LOOP = TIME2 - TIME1\n",
    "    print(TIME_INIT)\n",
    "    print(TIME_LOOP)\n",
    "    print(time_tracker_full_loop[0:1000])\n",
    "    print(time_tracker_full_loop[10000:11000])\n",
    "    print(time_tracker_cols[0:1000])\n",
    "    print(time_tracker_cols[10000:11000])\n",
    "    print(time_tracker_segment_stack[0:1000])\n",
    "    print(time_tracker_segment_stack[10000:11000])\n",
    "    print(time_tracker_label_stack[0:1000])\n",
    "    print(time_tracker_label_stack[10000:11000])\n",
    "    \n",
    "    print(entered_stracking)\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugger code for normalizing input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Helper Code\n",
    "\n",
    "# #dataset = read_data(myFileLocation)\n",
    "# dataset_full = dataset\n",
    "# print(dataset[1:4])\n",
    "# print(dataset_full[1:4])\n",
    "# #dataset['gender':'pelvic_tilt_lag_0'] = (dataset - dataset.mean())/dataset.std()\n",
    "# dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt_lag_0'] # normalize all columns from gender to pelvic_tilt_lag_0\n",
    "# dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "# dataset_full.drop([\"gender\":\"pelvic_tilt_lag_0\"])\n",
    "# dataset_full = [dataset_full, dataset_inputs_normalized]\n",
    "# print(dataset[1:4])\n",
    "# print(dataset_full[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuger code for saving segment and labels as NP arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging helper code\n",
    "\n",
    "#     if data_input_table_structure == 'Raw_Timeseries':\n",
    "#         segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "#     elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "#         segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "\n",
    "#     if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "#          labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "#          labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "#          num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)\n",
    "            \n",
    "\n",
    "            \n",
    "# segments2 = np.load('../Saved NP Arrays/TESTPICKLE2.npy', allow_pickle=True)\n",
    "# np.save('../Saved NP Arrays/TESTPICKLE2', segments, allow_pickle=True)\n",
    "# #save_pickle_obj(segments, 'TESTPICKLE')\n",
    "\n",
    "print(segments.shape[0])\n",
    "print(len(segments))\n",
    "#reshaped_segments = segments.reshape(len(segments), 1,input_num_timestamps, num_channels)\n",
    "\n",
    "print(segments[0,0,1:10])\n",
    "print(labels.shape)\n",
    "print(segments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables removed from code for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Variables for CNN model    # THIS IS SILLY. GET RID OF THESE TWO VARIABLES, ADD COMMENTS FOR EASE OF USE/READING INSTEAD\n",
    "if model_architecture == 'CNN':\n",
    "    num_rows_per_example = segments.shape[1] # same as input_window_size\n",
    "    num_cols_per_example = segments.shape[2] # same as total channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution accidentally 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.add(Conv2D(num_filters, (kernel_size,1),input_shape=(input_window_size, num_channels+num_anthropometrics,1),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling2D(pool_size=(max_pool_kernel_size,1),padding='valid',strides=(2,1)))\n",
    "    model.add(Conv2D(num_filters//10, (kernel_size,1),activation=activation_conv_layer)) # add additional CNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dev Split mistakes due to using Conv 2D and switching to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(segments)) < 0.90\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    #reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics, 1)\n",
    "        # Making a change to swith to Conv1D\n",
    "    print(segments.shape)\n",
    "    reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics)\n",
    "        # this line is not needed at all anymore, it does \n",
    "    print(reshapedSegments.shape)\n",
    "    X_train = reshapedSegments[train_dev_split]\n",
    "    X_test = reshapedSegments[~train_dev_split]\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippets from before Concatenated anthropometrics in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        #g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end] \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                #segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f])])\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal_label_only(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return labels\n",
    "\n",
    "# Preprocess data to input into model\n",
    "\n",
    "np_array_file_string_segment = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_label = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if os.path.isfile(np_array_file_string_segment):   # if this file already exists, load the relevant saved np arrays\n",
    "    segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "         labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "         labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "    np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "\n",
    "    \n",
    "def cnnModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(num_filters, (kernel_size),input_shape=(input_window_size, num_channels+num_anthropometrics),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling1D(pool_size=(max_pool_kernel_size),padding='valid',strides=(2)))\n",
    "    model.add(Conv1D(num_filters//10, (kernel_size),activation=activation_conv_layer)) # add additional CNN layer\n",
    "    # model.add(Dropout(dropOutRatio)) # not used in our model # adding a dropout layer for the regularization\n",
    "    model.add(Flatten()) # flatten the output in order to apply the fully connected layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])) # add first fully connected layer\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(dropout_rate_fc_layers_CNN[0]))\n",
    "    # Intermediate fully connected layerslayers\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        model.add(Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L]))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 0.5 speed bucket option (no longer used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In segment signal code\n",
    "\n",
    "#raw timeseries\n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "#vectorized        \n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.around(2*np.mean(labels_before_avg, axis=1),decimals=0)/2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old data file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Other data files/folders to potentially use:\n",
    "        #folder_data_loc = 'C:/Users/adam/Documents/CS 230/Project/Lumo Data/'\n",
    "        # '../datasets/'  |  'C:/Users/adam/Documents/CS 230/Project/Lumo Data/\n",
    "        # 'quarter-big'   |   'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "        # 'SAMPLE_TimeSeries_Longest1000Runs_wAnthro_MultiLabeledSpeed_20180523'  |  'TimeSeries_InputCNN_1000Runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search (did not work due to multi-input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODE TO USE GRIDSEARCH\n",
    "\n",
    "batch_sizes_grid = [64,128]\n",
    "epochs_grid = [5,10]\n",
    "param_grid = dict(batch_size=batch_sizes_grid, epochs=epochs_grid)\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    model = KerasClassifier(build_fn=fcnModel, verbose=1)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring = 'mae', n_jobs=1)\n",
    "    print('Starting to run grid search')\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "    \n",
    "elif model_architecture == 'CNN':     \n",
    "    model = KerasClassifier(build_fn=cnnModel_multInput, verbose=2)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "    grid_result = grid.fit([X_train_timeseries, X_train_anthro], y_train,validation_data=([X_test_timeseries, X_test_anthro], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results performance table (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_file_FCN(folder_head_loc, results_file_name):\n",
    "    my_file = Path(folder_head_loc + \"Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            num_hidden_fc_layers,\n",
    "            hidden_units_strategy,\n",
    "            activations_strategy,\n",
    "            dropout_rates\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 3\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"num_hidden_fc_layers\",\n",
    "                                    \"hidden_units_strategy\",\n",
    "                                    \"activations_strategy\",\n",
    "                                    \"dropout_rates\"])\n",
    "        \n",
    "        df.to_csv(folder_head_loc + \"Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df\n",
    "\n",
    "def load_results_file_CNN(folder_head_loc, results_file_name):\n",
    "    my_file = Path(folder_head_loc + \"Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        #print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        #print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            hidden_units_strategy_CNN,\n",
    "            num_filters,\n",
    "            kernel_size,\n",
    "            sample_stride,\n",
    "            activation_conv_layer,\n",
    "            activations_strategy_CNN,\n",
    "            max_pool_kernel_size\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"hidden_units_strategy_CNN\",\n",
    "                                    \"num_filters\",\n",
    "                                    \"kernel_size\",\n",
    "                                    \"sample_stride\",\n",
    "                                    \"activation_conv_layer\",\n",
    "                                    \"activations_strategy_CNN\",\n",
    "                                    \"max_pool_kernel_size\"])\n",
    "        \n",
    "        df.to_csv(folder_head_loc + \"Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One bucket range metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def class_percent_1buckLow(y_true, y_pred): # percent of times the prediction is 1 bucket below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx())\n",
    "\n",
    "    \n",
    "def class_percent_1buckHigh(y_true, y_pred): # percent of times the prediction is 1 bucket above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx())    \n",
    "\n",
    "\n",
    "def class_percent_1buckRange(y_true, y_pred): # percent of times the prediction is within 1 bucket of true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())\n",
    "\n",
    "# For reference, from keras documentation: https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "#def class_categorical_accuracy(y_true, y_pred):\n",
    "    #return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File naming conventions\n",
    "\n",
    "plot_note = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180911\" + \"_\" + model_architecture\n",
    "\n",
    "customize_file_names = False\n",
    "if customize_file_names:\n",
    "    file_name = input(\"String to add to model filename (defaults to time stamp if nothing entered):\")  \n",
    "    results_file_name = input(\"Name of the results file, a table, to store the prediction results\") # name of results file\n",
    "    plot_note = input(\"Note you'd like to add in the legend of the primary learning curves plot:\") #user input to add note to plot\n",
    "    model_to_load = input(\"Enter the model name to load to initialize parameters - leave blank to start fresh\") #user input to load prev model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runs_per_file == 'single':\n",
    "    batch_size = 64 # we used 50 for CNN, 128 for FCN\n",
    "    learning_rate = 0.0001 # we used 0.001 for FCN, 0.0001 for CNN\n",
    "    optimizer_type = 'gradient' # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "    loss_function = 'categorical_crossentropy' # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'    \n",
    "    training_epochs = 3\n",
    "elif runs_per_file == 'train_loop':\n",
    "    batch_size_all = [64,128] # we used 50 for CNN, 128 for FCN\n",
    "    learning_rate_all = [0.0001,0.0005,0.001] # we used 0.001 for FCN, 0.0001 for CNN\n",
    "    optimizer_type_all = ['gradient','adam'] # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "    loss_function_all = ['categorical_crossentropy'] # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'    \n",
    "    training_epochs_all = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrtix method (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confusion Matrix\n",
    "def create_conf_matrix_from_model(machine_to_run_script\n",
    "        ,y_true_argmax\n",
    "        ,y_pred_argmax\n",
    "        ,folder_head_loc\n",
    "        ,file_name\n",
    "        ,speed_bucket_size):\n",
    "    # Plot results\n",
    "    if machine_to_run_script == 'local':\n",
    "        plt.scatter(y_true_argmax, y_pred_argmax, s=3, alpha=0.3)\n",
    "        plt.scatter(y_true_argmax, y_true_argmax, s=3, alpha=1)\n",
    "        #plt.scatter(y_true, y_pred, s=3, alpha=0.3) # For regression\n",
    "        plt.xlim([0,50])\n",
    "        plt.ylim([0,50])\n",
    "        plt.xlabel('Y_True')\n",
    "        plt.ylabel('Y_Prediction')\n",
    "        plt.savefig(folder_head_loc + \"Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Image.png\")\n",
    "        plt.show()\n",
    "    # Record data in a .csv\n",
    "    y_trueVy_pred = np.vstack([y_true_argmax,y_pred_argmax])\n",
    "    df_y_trueVy_pred = pd.DataFrame(np.transpose(y_trueVy_pred))\n",
    "    filepath_predictions = folder_head_loc + \"Model Final Predictions/\" + str(file_name) + \"_Predictions\" + \".csv\"\n",
    "    df_y_trueVy_pred.to_csv(filepath_predictions, header = [\"y_true_argmax\", \"y_pred_argmax\"], index=False)\n",
    "    # Create and save a confusion matrix\n",
    "    if speed_bucket_size != 'none_use_regression':\n",
    "        cm = confusion_matrix(y_true_argmax, y_pred_argmax)\n",
    "        df_cm = pd.DataFrame (cm)\n",
    "        filepath_cm = folder_head_loc + \"Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Data.xlsx\"\n",
    "        df_cm.to_excel(filepath_cm, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
