{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumo Run - Deep FFCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here 2\n",
      "made it here 3\n",
      "made it here 4\n"
     ]
    }
   ],
   "source": [
    "print('made it here 1') # these are used for Sherlock to check what is causing the hiccup\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import regularizers \n",
    "from keras import optimizers\n",
    "\n",
    "print('made it here 2')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print('made it here 3')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import socket\n",
    "import pickle\n",
    "import os.path\n",
    "#import dill #can't find dill\n",
    "\n",
    "print('made it here 4')\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "np.random.seed(7) # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "num_channels = 7 # number of time-series channels of data (i.e. 7 kinematic features) #NOTE: Change to 6 by removing Pelvic Tilt (recommended by Lumo)\n",
    "num_anthropometrics = 4 # number of user anthropometric data elements\n",
    "input_window_size = 11 # number of timestamps used in the input for prediction (i.e. the input window)\n",
    "label_window_size = 5 # number of timestamps used to label the speed we will be predicting\n",
    "speed_bucket_size = '0.1' # how to round the data for classification task. Consider '0.5', '0.1', and 'none_use_regression'\n",
    "\n",
    "previous_model_weights_to_load = \"\" # If non-empty, load weights from a previous model (note: architectures must be identical)\n",
    "model_architecture = 'FCN' # 'FCN', 'CNN'\n",
    "data_input_table_structure = 'Vectorized_By_Row' # 'Vectorized_By_Row' 'Raw_Timeseries'\n",
    "myFileDirectory = 'C:/Users/adam/Documents/CS 230/Project/Lumo Data/'\n",
    "myFileName = 'TimeSeries_InputVector_100runs'\n",
    "myFileLocation = myFileDirectory + myFileName + '.csv'\n",
    "        # Other data files/folders to potentially use:\n",
    "        # '../datasets/'  |  'C:/Users/adam/Documents/CS 230/Project/Lumo Data/\n",
    "        # 'quarter-big'   |   'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "        # 'SAMPLE_TimeSeries_Longest1000Runs_wAnthro_MultiLabeledSpeed_20180523'\n",
    "        \n",
    "# Fully Connected Architecture\n",
    "\n",
    "num_hidden_units_fc_layers = [256, 256, 256, 128, 128, 128]\n",
    "hidden_units_strategy = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers) # document strategy \n",
    "num_hidden_fc_layers = len(num_hidden_units_fc_layers) # document strategy\n",
    "activations_fc_layers = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "activations_strategy = ''.join(str(num) + \"_\" for num in activations_fc_layers) # document strategy\n",
    "dropout_rate_fc_layers = [1.0, 1.0, 1.0, 0.8, 0.8, 0.8]\n",
    "dropout_rates = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers) # document strategy\n",
    "\n",
    "# Convolutional Architecture\n",
    "    \n",
    "sample_stride = 18 # how many timestamps to shift over between each unique training example\n",
    "num_filters = 40 # number of filters in Conv2D layer (aka depth) # we used 40, ex; used 128\n",
    "kernel_size = 6 # kernal size of the Conv2D layer # we use 6, example used 2, I would guess closer to 3\n",
    "activation_conv_layer = \"relu\" # options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "max_pool_kernel_size = 6 # max pooling window size# we use 6, example used 2, I don't agre with 6\n",
    "conv_layer_dropout = 0.2 # dropout ratio for dropout layer # we don't use in our model\n",
    "\n",
    "num_hidden_units_fc_layers_CNN = [50]\n",
    "hidden_units_strategy_CNN = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers_CNN) # document strategy \n",
    "num_hidden_fc_layers_CNN = len(num_hidden_units_fc_layers_CNN) # document strategy\n",
    "activations_fc_layers_CNN = ['tanh']\n",
    "activations_strategy_CNN = ''.join(str(num) + \"_\" for num in activations_fc_layers_CNN) # document strategy\n",
    "dropout_rate_fc_layers_CNN = [1.0]\n",
    "dropout_rates_CNN = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers_CNN) # document strategy\n",
    "\n",
    "num_hidden_units_fc_layers_CNN[0] \n",
    "activations_fc_layers_CNN[0]\n",
    "    \n",
    "# Training strategy\n",
    "\n",
    "batch_size = 128 # we used 50 for CNN, 128 for FCN\n",
    "learning_rate = 0.001 # we used 0.001 for FCN, 0.0001 for CNN\n",
    "training_epochs = 5\n",
    "optimizer_type = 'adam' # options are: \"adam\" , \"rmsprop\", \"gradient\" # adam for FCN, gradient for CNN\n",
    "loss_function = 'categorical_crossentropy' # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Automatic Reporting and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 3 most interesting evaluation metrics to report on in final plots\n",
    "\n",
    "accuracy_reporting_metric_1 = 'class_mae' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "accuracy_reporting_metric_2 = 'class_percent_2buckRange' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "accuracy_reporting_metric_3 = 'class_mse' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3\n",
    "\n",
    "plt.style.use('ggplot') # style of matlab plots to produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File naming conventions\n",
    "\n",
    "file_name = strftime(\"%Y%m%d_%H%M%S\", gmtime()) # user input for filename of saved model\n",
    "plot_note = \"\"\n",
    "model_to_lod = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180726\" + \"_\" + model_architecture\n",
    "\n",
    "customize_file_names = False\n",
    "if customize_file_names:\n",
    "    file_name = input(\"String to add to model filename (defaults to time stamp if nothing entered):\")  \n",
    "    results_file_name = input(\"Name of the results file, a table, to store the prediction results\") # name of results file\n",
    "    plot_note = input(\"Note you'd like to add in the legend of the primary learning curves plot:\") #user input to add note to plot\n",
    "    model_to_load = input(\"Enter the model name to load to initialize parameters - leave blank to start fresh\") #user input to load prev model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path,header = 0) # This uses the header row (row 0) as the column names\n",
    "    return data\n",
    "\n",
    "def windows(data, size): # define time windows to create each training example\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride # other common options: (size / 2)\n",
    "    \n",
    "# Used for vectorized input WITH SQL pre-processing\n",
    "def segment_signal_FCN_vector(data_inputs, data_full): \n",
    "    dataframe_input = data_inputs.loc[:, 'gender':'pelvic_tilt_lag_0'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    dataframe_labels = data_full.loc[:, 'gps_speed_lag_7':'gps_speed_lag_3'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    segments = dataframe_input.values\n",
    "    labels_before_avg = dataframe_labels.values\n",
    "    if speed_bucket_size == '0.1':\n",
    "        labels = np.around(np.mean(labels_before_avg, axis=1),decimals=1)\n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.around(2*np.mean(labels_before_avg, axis=1),decimals=0)/2\n",
    "    elif speed_bucket_size == 'none_use_regression':\n",
    "        labels = np.mean(labels_before_avg, axis=1)\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end]         \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal_label_only(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return labels\n",
    "\n",
    "def load_results_file_FCN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            num_hidden_fc_layers,\n",
    "            hidden_units_strategy,\n",
    "            activations_strategy,\n",
    "            dropout_rates\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 4\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"num_hidden_fc_layers\",\n",
    "                                    \"hidden_units_strategy\",\n",
    "                                    \"activations_strategy\",\n",
    "                                    \"dropout_rates\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df\n",
    "\n",
    "def load_results_file_CNN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            hidden_units_strategy_CNN,\n",
    "            num_filters,\n",
    "            kernel_size,\n",
    "            sample_stride,\n",
    "            activation_conv_layer,\n",
    "            activations_strategy_CNN,\n",
    "            max_pool_kernel_size\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"batch_size\",\n",
    "#                                     \"num_hidden_fc_layers\",\n",
    "#                                     \"activations_strategy\",\n",
    "#                                     \"dropout_rates\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"hidden_units_strategy_CNN\",\n",
    "                                    \"num_filters\",\n",
    "                                    \"kernel_size\",\n",
    "                                    \"sample_stride\",\n",
    "                                    \"activation_conv_layer\",\n",
    "                                    \"activations_strategy_CNN\",\n",
    "                                    \"max_pool_kernel_size\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt'] # normalize all columns from gender to pelvic_tilt\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt_lag_0'] # normalize all columns from gender to pelvic_tilt_lag_0\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_file_string_segment = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_label = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if os.path.isfile(np_array_file_string_segment):   # if this file already exists, load the relevant saved np arrays\n",
    "    segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "         labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "         labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "    np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "\n",
    "num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.  1.1 1.2 1.3 1.4 1.5 1.6 1.7\n",
      " 1.8 1.9 2.  2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.  3.1 3.2 3.3 3.4 3.5\n",
      " 3.6 3.7 3.8 3.9 4.  4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.  5.1 5.2 5.3\n",
      " 5.4 5.7 5.8 5.9 6.1]\n",
      "(127015, 59)\n",
      "(127015, 81)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Debugger Helper Code\n",
    "\n",
    "print(labels_to_number)\n",
    "print(labels.shape)\n",
    "print(segments.shape)\n",
    "print(labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data into training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(segments)) < 0.90\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics, 1)\n",
    "    X_train = reshapedSegments[train_dev_split]\n",
    "    X_test = reshapedSegments[~train_dev_split]\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127015,)\n",
      "(12827, 81)\n",
      "(114188, 59)\n",
      "(12827, 59)\n"
     ]
    }
   ],
   "source": [
    "print(train_dev_split.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement NN architecture in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnModel():\n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers[0], activation=activations_fc_layers[0], input_shape=(input_window_size*num_channels + num_anthropometrics,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_fc_layers[0]))\n",
    "    # Intermediate layers\n",
    "    for L in range(1, num_hidden_fc_layers):\n",
    "        model.add(Dense(num_hidden_units_fc_layers[L], activation=activations_fc_layers[L]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model\n",
    "        \n",
    "def cnnModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_filters, (kernel_size,1),input_shape=(input_window_size, num_channels+num_anthropometrics,1),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling2D(pool_size=(max_pool_kernel_size,1),padding='valid',strides=(2,1)))\n",
    "    model.add(Conv2D(num_filters//10, (kernel_size,1),activation=activation_conv_layer)) # add additional CNN layer\n",
    "    # model.add(Dropout(dropOutRatio)) # not used in our model # adding a dropout layer for the regularization\n",
    "    model.add(Flatten()) # flatten the output in order to apply the fully connected layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])) # add first fully connected layer\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(dropout_rate_fc_layers_CNN[0]))\n",
    "    # Intermediate fully connected layerslayers\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        model.add(Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L]))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  model_architecture == 'FCN':\n",
    "    model = fcnModel()\n",
    "elif model_architecture == 'CNN':\n",
    "    model = cnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               20992     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 59)                7611      \n",
      "=================================================================\n",
      "Total params: 230,715\n",
      "Trainable params: 228,411\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25082384  0.14781248 -0.09895195 -0.02805245  0.00349556  0.20034566\n",
      "  0.34993213  0.6650031   0.53891783]\n",
      "(127015, 59)\n",
      "(127015, 81)\n"
     ]
    }
   ],
   "source": [
    "# Debuging helper code\n",
    "\n",
    "print(segments[0,1:10])\n",
    "print(labels.shape)\n",
    "print(segments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss functions and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def class_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "    # Note: we cannot define RMSE directly in Keras since the loss function is defined for one training example at a time\n",
    "\n",
    "def class_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "\n",
    "def class_mape(y_true, y_pred):\n",
    "    diff = K.abs((K.sum(y_true * labels_to_number,axis=-1,keepdims=True) - K.sum(y_pred * labels_to_number,axis=-1,keepdims=True)) / K.clip(K.abs(K.sum(y_true * labels_to_number,axis=-1,keepdims=True)),K.epsilon(),None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "def class_percent_1buckLow(y_true, y_pred): # percent of times the prediction is 1 bucket below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx())\n",
    "\n",
    "def class_percent_2buckLow(y_true, y_pred): # percent of times the prediction is 2 buckets below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())\n",
    "    \n",
    "def class_percent_1buckHigh(y_true, y_pred): # percent of times the prediction is 1 bucket above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx())    \n",
    "\n",
    "def class_percent_2buckHigh(y_true, y_pred): # percent of times the prediction is 2 buckets above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx())    \n",
    "\n",
    "def class_percent_1buckRange(y_true, y_pred): # percent of times the prediction is within 1 bucket of true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())\n",
    "\n",
    "def class_percent_2buckRange(y_true, y_pred): # percent of times the prediction is within 2 buckets of true value\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())    \n",
    "\n",
    "# For reference, from keras documentation: https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "#def class_categorical_accuracy(y_true, y_pred):\n",
    "    #return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model loss and optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "if optimizer_type == 'adam':\n",
    "    model_optimizer = optimizers.Adam(lr = learning_rate) #, decay, beta_1, beta_2 are HPs\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    model_optimizer = optimizers.RMSprop(lr = learning_rate) #, decay, rho\n",
    "elif optimizer_type == 'gradient':\n",
    "    model_optimizer = optimizers.SGD(lr = learning_rate) #, decay, momentum\n",
    "\n",
    "# Compile model with appropriate loss function\n",
    "if speed_bucket_size != 'none_use_regression': # if performing classification, ALWAYS use cross-entropy loss\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=model_optimizer, metrics=['accuracy',class_percent_1buckRange,class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n",
    "else:                                          # if performing regression, use mean squared error or mean absolute error\n",
    "    if loss_function == 'categorical_crossentropy': raise NameError('Are you sure you want to use cross entropy loss with a regression tasks!?')\n",
    "    model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugger code\n",
    "\n",
    "#model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics=['accuracy',class_percent_1buckRange,class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, load weights from a previous model to start with model\n",
    "\n",
    "if previous_model_weights_to_load != \"\":\n",
    "    model.load_weights(\"../Model Final Parameters/\" + previous_model_weights_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114188 samples, validate on 12827 samples\n",
      "Epoch 1/5\n",
      "114188/114188 [==============================] - 40s 350us/step - loss: 3.8375 - acc: 0.0827 - class_percent_1buckRange: 0.2421 - class_percent_2buckRange: 0.3815 - class_mae: 0.4230 - class_mse: 0.3957 - val_loss: 2.7112 - val_acc: 0.1332 - val_class_percent_1buckRange: 0.3864 - val_class_percent_2buckRange: 0.5912 - val_class_mae: 0.3228 - val_class_mse: 0.2849\n",
      "Epoch 2/5\n",
      "114188/114188 [==============================] - 36s 315us/step - loss: 2.6733 - acc: 0.1484 - class_percent_1buckRange: 0.4140 - class_percent_2buckRange: 0.6129 - class_mae: 0.2993 - class_mse: 0.2525 - val_loss: 2.3739 - val_acc: 0.1905 - val_class_percent_1buckRange: 0.5028 - val_class_percent_2buckRange: 0.7090 - val_class_mae: 0.2437 - val_class_mse: 0.2109\n",
      "Epoch 3/5\n",
      "114188/114188 [==============================] - 36s 315us/step - loss: 2.5055 - acc: 0.1704 - class_percent_1buckRange: 0.4603 - class_percent_2buckRange: 0.6687 - class_mae: 0.2606 - class_mse: 0.1921 - val_loss: 2.2719 - val_acc: 0.2098 - val_class_percent_1buckRange: 0.5466 - val_class_percent_2buckRange: 0.7470 - val_class_mae: 0.2030 - val_class_mse: 0.1032\n",
      "Epoch 4/5\n",
      "114188/114188 [==============================] - 35s 306us/step - loss: 2.4322 - acc: 0.1848 - class_percent_1buckRange: 0.4898 - class_percent_2buckRange: 0.6973 - class_mae: 0.2412 - class_mse: 0.1627 - val_loss: 2.2170 - val_acc: 0.2202 - val_class_percent_1buckRange: 0.5658 - val_class_percent_2buckRange: 0.7703 - val_class_mae: 0.1854 - val_class_mse: 0.0819\n",
      "Epoch 5/5\n",
      "114188/114188 [==============================] - 33s 285us/step - loss: 2.3914 - acc: 0.1911 - class_percent_1buckRange: 0.5057 - class_percent_2buckRange: 0.7137 - class_mae: 0.2296 - class_mse: 0.1473 - val_loss: 2.2113 - val_acc: 0.2212 - val_class_percent_1buckRange: 0.5707 - val_class_percent_2buckRange: 0.7673 - val_class_mae: 0.1852 - val_class_mse: 0.0819\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size= batch_size, epochs=training_epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a plot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform key results into a np arrary\n",
    "trainAccuracy_1 = np.squeeze(history.history[accuracy_reporting_metric_1])\n",
    "devAccuracy_1 = np.squeeze(history.history[dev_reporting_metric_1])\n",
    "trainAccuracy_2 = np.squeeze(history.history[accuracy_reporting_metric_2])\n",
    "devAccuracy_2 = np.squeeze(history.history[dev_reporting_metric_2])    \n",
    "trainAccuracy_3 = np.squeeze(history.history[accuracy_reporting_metric_3])\n",
    "devAccuracy_3 = np.squeeze(history.history[dev_reporting_metric_3])\n",
    "trainAccuracy_4 = np.squeeze(history.history['acc'])\n",
    "devAccuracy_4 = np.squeeze(history.history['val_acc'])\n",
    "epochs = np.squeeze(range(1,training_epochs + 1))\n",
    "\n",
    "# Save results to a .csv in the \"Learning Curve Results\"\n",
    "df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([epochs,devAccuracy_1, devAccuracy_2, devAccuracy_3, devAccuracy_4])))\n",
    "filepath_acc = \"../Learning Curves/\" + str(file_name) +\"_AccuracyPerEpoch_Data\" + \".csv\"\n",
    "df_devAccuracy.to_csv(filepath_acc, header = [\"Epochs\", dev_reporting_metric_1, dev_reporting_metric_2, dev_reporting_metric_3, 'acc'], index=False)\n",
    "\n",
    "# Declare final values for results\n",
    "final_accuracy_1 = history.history[accuracy_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_dev_1 = history.history[dev_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_2 = history.history[accuracy_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_dev_2 = history.history[dev_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_3 = history.history[accuracy_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_dev_3 = history.history[dev_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_4 = history.history['acc'][training_epochs - 1]\n",
    "final_accuracy_dev_4 = history.history['val_acc'][training_epochs - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "lines=[]\n",
    "\n",
    "lines += ax1.plot(trainAccuracy_4,'#0e128c', label='Train Accuracy 2', linewidth=1) #'#DAF7A6'\n",
    "lines += ax1.plot(devAccuracy_4,'#a3a4cc', label='Dev Accuracy 2', linewidth=1)# '#33FF00',\n",
    "lines += ax1.plot(trainAccuracy_1,'#FF5733', label='Train Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_1,'#C70039', label='Dev Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(trainAccuracy_2,'#9C27B0', label='Train Accuracy 2', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_2,'#7986CB', label='Dev Accuracy 2', linewidth=1)\n",
    "plt.ylim([0.0, 1.0])  # Surpress this for non-classification tasks\n",
    "\n",
    "plt.ylabel('Train vs. Dev Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(dev_reporting_metric_1 + \": \" + str(np.around(final_accuracy_dev_1,4)) + \"\\n\" + \\\n",
    "         dev_reporting_metric_2 + \": \" + str(np.around(final_accuracy_dev_2,4))) \n",
    "extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "plt.legend([extra,extra,extra,extra,extra,extra,extra,extra,extra,extra],(\n",
    "                                                \"loss: \" + loss_function,\n",
    "                                                \"learning rate: \" + str(learning_rate),\n",
    "                                                \"batch_size: \" + str(batch_size),\n",
    "                                                \"speed_bucket_size: \" + speed_bucket_size,\n",
    "                                                \"epochs: \"+str(training_epochs),\n",
    "                                                \"input_window_size: \" + str(input_window_size),\n",
    "                                                \"num_labels: \" + str(len(labels_to_number)),\n",
    "                                                \"evaluation metric 1:\"+accuracy_reporting_metric_1,\n",
    "                                                \"evaluation metric 2:\"+accuracy_reporting_metric_2,\n",
    "                                                \"evaluation metric 3:\"+accuracy_reporting_metric_3,\n",
    "                                                \"note:\" + plot_note),\n",
    "                                                bbox_to_anchor=(1.05, 1),\n",
    "                                                loc=2,\n",
    "                                                borderaxespad=0.)\n",
    "\n",
    "leg = Legend(ax1, lines[0:], ['Train ACC', 'Dev ACC','Train Eval 1','Dev Eval 1'],\n",
    "             loc='bestoutside', frameon=False)\n",
    "ax1.add_artist(leg);\n",
    "plt.savefig(\"../Learning Curves/\" + str(file_name) + \"_AccuracyPerEpoch_Image.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record results of a model in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the most recent run to the results file for documentation\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,  \n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,    \n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        num_hidden_fc_layers,\n",
    "        hidden_units_strategy,\n",
    "        activations_strategy,\n",
    "        dropout_rates\n",
    "       ]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"batch_size\",  \n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"num_hidden_fc_layers\",\n",
    "                                \"hidden_units_strategy\",\n",
    "                                \"activations_strategy\",\n",
    "                                \"dropout_rates\"])\n",
    "    past_results = load_results_file_FCN(results_file_name)\n",
    "elif model_architecture == 'CNN':     \n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,\n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        hidden_units_strategy_CNN,\n",
    "        num_filters,\n",
    "        kernel_size,\n",
    "        sample_stride,\n",
    "        activation_conv_layer,\n",
    "        activations_strategy_CNN,\n",
    "        max_pool_kernel_size]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"batch_size\",\n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"hidden_units_strategy_CNN\",\n",
    "                                \"num_filters\",\n",
    "                                \"kernel_size\",\n",
    "                                \"sample_stride\",\n",
    "                                \"activation_conv_layer\",\n",
    "                                \"activations_strategy_CNN\",\n",
    "                                \"max_pool_kernel_size\"])    \n",
    "    past_results = load_results_file_CNN(results_file_name)\n",
    "past_results=pd.concat([past_results,df])\n",
    "print(past_results)\n",
    "past_results.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) # Consider fixing to put the columns in not-alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Confusion Matrix and Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = y_test\n",
    "y_true_argmax = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.scatter(y_true_argmax, y_pred_argmax, s=3, alpha=0.3)\n",
    "plt.scatter(y_true_argmax, y_true_argmax, s=3, alpha=1)\n",
    "#plt.scatter(y_true, y_pred, s=3, alpha=0.3) # For regression\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "plt.xlabel('Y_True')\n",
    "plt.ylabel('Y_Prediction')\n",
    "plt.savefig(\"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Image.png\")\n",
    "plt.show()\n",
    "\n",
    "# Record data in a .csv\n",
    "y_trueVy_pred = np.vstack([y_true_argmax,y_pred_argmax])\n",
    "df_y_trueVy_pred = pd.DataFrame(np.transpose(y_trueVy_pred))\n",
    "filepath_predictions = \"../Model Final Predictions/\" + str(file_name) + \"_Predictions\" + \".csv\"\n",
    "df_y_trueVy_pred.to_csv(filepath_predictions, header = [\"y_true_argmax\", \"y_pred_argmax\"], index=False)\n",
    "\n",
    "# Create and save a confusion matrix\n",
    "cm = confusion_matrix(y_true_argmax, y_pred_argmax)\n",
    "df_cm = pd.DataFrame (cm)\n",
    "filepath_cm = \"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Data.xlsx\"\n",
    "df_cm.to_excel(filepath_cm, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model parameters (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_model_name = file_name + \"_\" + model_architecture\n",
    "model.save_weights(\"../Model Final Parameters/\" + completed_model_name + '_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
