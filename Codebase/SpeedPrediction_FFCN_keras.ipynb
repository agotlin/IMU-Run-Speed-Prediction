{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumo Run - Deep FFCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it here 1\n",
      "made it here 2\n",
      "made it here 3\n",
      "made it here 4\n"
     ]
    }
   ],
   "source": [
    "print('made it here 1') # these are used for Sherlock to check what is causing the hiccup\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import regularizers \n",
    "from keras import optimizers\n",
    "\n",
    "print('made it here 2')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print('made it here 3')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from IPython import embed\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "import socket\n",
    "import pickle\n",
    "import os.path\n",
    "#import dill #can't find dill\n",
    "\n",
    "print('made it here 4')\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "np.random.seed(7) # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "\n",
    "num_channels = 7 # number of time-series channels of data (i.e. 7 kinematic features) #NOTE: Change to 6 by removing Pelvic Tilt (recommended by Lumo)\n",
    "num_anthropometrics = 4 # number of user anthropometric data elements\n",
    "input_window_size = 11 # number of timestamps used in the input for prediction (i.e. the input window)\n",
    "label_window_size = 5 # number of timestamps used to label the speed we will be predicting\n",
    "speed_bucket_size = '0.1' # how to round the data for classification task. Consider '0.5', '0.1', and 'none_use_regression'\n",
    "\n",
    "previous_model_weights_to_load = \"\" # If non-empty, load weights from a previous model (note: architectures must be identical)\n",
    "model_architecture = 'FCN' # 'FCN', 'CNN'\n",
    "data_input_table_structure = 'Vectorized_By_Row' # 'Vectorized_By_Row' 'Raw_Timeseries'\n",
    "myFileDirectory = 'C:/Users/adam/Documents/CS 230/Project/Lumo Data/'\n",
    "myFileName = 'TimeSeries_InputVector_100runs'\n",
    "myFileLocation = myFileDirectory + myFileName + '.csv'\n",
    "        # Other data files/folders to potentially use:\n",
    "        # '../datasets/'  |  'C:/Users/adam/Documents/CS 230/Project/Lumo Data/\n",
    "        # 'quarter-big'   |   'TimeSeries_InputVector_100runs'   |   'TimeSeries_InputVector_15runs'\n",
    "        # 'SAMPLE_TimeSeries_Longest1000Runs_wAnthro_MultiLabeledSpeed_20180523'\n",
    "        \n",
    "# Fully Connected Architecture\n",
    "\n",
    "num_hidden_units_fc_layers = [256, 256, 256, 128, 128, 128]\n",
    "hidden_units_strategy = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers) # document strategy \n",
    "num_hidden_fc_layers = len(num_hidden_units_fc_layers) # document strategy\n",
    "activations_fc_layers = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "activations_strategy = ''.join(str(num) + \"_\" for num in activations_fc_layers) # document strategy\n",
    "dropout_rate_fc_layers = [1.0, 1.0, 1.0, 0.8, 0.8, 0.8]\n",
    "dropout_rates = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers) # document strategy\n",
    "\n",
    "# Convolutional Architecture\n",
    "    \n",
    "sample_stride = 18 # how many timestamps to shift over between each unique training example\n",
    "num_filters = 40 # number of filters in Conv2D layer (aka depth) # we used 40, ex; used 128\n",
    "kernel_size = 6 # kernal size of the Conv2D layer # we use 6, example used 2, I would guess closer to 3\n",
    "activation_conv_layer = \"relu\" # options are \"relu\" , \"tanh\" and \"sigmoid\" - used for depthwise_conv\n",
    "max_pool_kernel_size = 6 # max pooling window size# we use 6, example used 2, I don't agre with 6\n",
    "conv_layer_dropout = 0.2 # dropout ratio for dropout layer # we don't use in our model\n",
    "\n",
    "num_hidden_units_fc_layers_CNN = [50]\n",
    "hidden_units_strategy_CNN = ''.join(str(num) + \"_\" for num in num_hidden_units_fc_layers_CNN) # document strategy \n",
    "num_hidden_fc_layers_CNN = len(num_hidden_units_fc_layers_CNN) # document strategy\n",
    "activations_fc_layers_CNN = ['tanh']\n",
    "activations_strategy_CNN = ''.join(str(num) + \"_\" for num in activations_fc_layers_CNN) # document strategy\n",
    "dropout_rate_fc_layers_CNN = [1.0]\n",
    "dropout_rates_CNN = ''.join(str(num) + \"_\" for num in dropout_rate_fc_layers_CNN) # document strategy\n",
    "\n",
    "num_hidden_units_fc_layers_CNN[0] \n",
    "activations_fc_layers_CNN[0]\n",
    "    \n",
    "# Training strategy\n",
    "\n",
    "batch_size = 50 # we used 50 for CNN, 128 for FCN\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 100\n",
    "optimizer_type = 'gradient' # options are: \"adam\" , \"rmsprop\", \"gradient\"\n",
    "loss_function = 'categorical_crossentropy' # Other options (from keras defaults or custom) include: 'categorical_crossentropy' ,'mse', 'mae', 'class_mse', 'class_mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Automatic Reporting and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 3 most interesting evaluation metrics to report on in final plots\n",
    "\n",
    "accuracy_reporting_metric_1 = 'class_mae' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_1 = 'val_' + accuracy_reporting_metric_1\n",
    "accuracy_reporting_metric_2 = 'class_percent_2buckRange' # options: 'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_2 = 'val_' + accuracy_reporting_metric_2\n",
    "accuracy_reporting_metric_3 = 'class_mse' # options: s'acc', 'class_percent_1buckRange', 'class_percent_2buckRange'\n",
    "dev_reporting_metric_3 = 'val_' + accuracy_reporting_metric_3\n",
    "\n",
    "plt.style.use('ggplot') # style of matlab plots to produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File naming conventions\n",
    "\n",
    "file_name = strftime(\"%Y%m%d_%H%M%S\", gmtime()) # user input for filename of saved model\n",
    "plot_note = \"\"\n",
    "model_to_lod = \"\"\n",
    "results_file_name = \"Default_Model_Results_Table_20180726\" + \"_\" + model_architecture\n",
    "\n",
    "customize_file_names = False\n",
    "if customize_file_names:\n",
    "    file_name = input(\"String to add to model filename (defaults to time stamp if nothing entered):\")  \n",
    "    results_file_name = input(\"Name of the results file, a table, to store the prediction results\") # name of results file\n",
    "    plot_note = input(\"Note you'd like to add in the legend of the primary learning curves plot:\") #user input to add note to plot\n",
    "    model_to_load = input(\"Enter the model name to load to initialize parameters - leave blank to start fresh\") #user input to load prev model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for data processing and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path,header = 0) # This uses the header row (row 0) as the column names\n",
    "    return data\n",
    "\n",
    "def windows(data, size): # define time windows to create each training example\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride # other common options: (size / 2)\n",
    "    \n",
    "# Used for vectorized input WITH SQL pre-processing\n",
    "def segment_signal_FCN_vector(data_inputs, data_full): \n",
    "    dataframe_input = data_inputs.loc[:, 'gender':'pelvic_tilt_lag_0'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    dataframe_labels = data_full.loc[:, 'gps_speed_lag_7':'gps_speed_lag_3'] # select all columns from gender to pelvic_tilt_lag_0\n",
    "    segments = dataframe_input.values\n",
    "    labels_before_avg = dataframe_labels.values\n",
    "    if speed_bucket_size == '0.1':\n",
    "        labels = np.around(np.mean(labels_before_avg, axis=1),decimals=1)\n",
    "    elif speed_bucket_size == '0.5':\n",
    "        labels = np.around(2*np.mean(labels_before_avg, axis=1),decimals=0)/2\n",
    "    elif speed_bucket_size == 'none_use_regression':\n",
    "        labels = np.mean(labels_before_avg, axis=1)\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    if  model_architecture == 'FCN':\n",
    "        segments = np.empty((0,input_window_size*num_channels + num_anthropometrics))\n",
    "    elif model_architecture == 'CNN':\n",
    "        segments = np.empty((0, input_window_size, num_channels + num_anthropometrics))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):\n",
    "        a = data_inputs[\"bounce\"][start:end]\n",
    "        b = data_inputs[\"braking\"][start:end]\n",
    "        c = data_inputs[\"cadence\"][start:end]\n",
    "        d = data_inputs[\"ground_contact\"][start:end]\n",
    "        e = data_inputs[\"pelvic_drop\"][start:end]\n",
    "        f = data_inputs[\"pelvic_rotation\"][start:end]\n",
    "        g = data_inputs[\"pelvic_tilt\"][start:end]\n",
    "        if model_architecture == 'FCN':\n",
    "            aa = data_inputs[\"age\"][start]\n",
    "            bb = data_inputs[\"weight\"][start]\n",
    "            cc = data_inputs[\"height\"][start]\n",
    "            dd = data_inputs[\"gender\"][start] \n",
    "        elif model_architecture == 'CNN':\n",
    "            aa = data_inputs[\"age\"][start:end]\n",
    "            bb = data_inputs[\"weight\"][start:end]\n",
    "            cc = data_inputs[\"height\"][start:end]\n",
    "            dd = data_inputs[\"gender\"][start:end]         \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            if model_architecture == 'FCN':\n",
    "                segments_toadd = np.vstack([np.dstack([a,b,c,d,e,f,g])])\n",
    "                segments_toadd_reshape = segments_toadd.reshape(input_window_size * num_channels)\n",
    "                segments = np.vstack([segments,np.hstack([aa,bb,cc,dd,segments_toadd_reshape])])\n",
    "            elif model_architecture == 'CNN':\n",
    "                segments = np.vstack([segments,np.dstack([aa,bb,cc,dd,a,b,c,d,e,f,g])])\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return segments, labels\n",
    "\n",
    "# Used for vectorized input WITHOUT SQL pre-processing\n",
    "def segment_signal_label_only(data_inputs, data_full, input_window_size = input_window_size):\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['timestamp'], input_window_size):        \n",
    "        if(end < data_full.shape[0] and len(data_full['timestamp'][start:end]) == input_window_size and data_full['activity_id'][start]==data_full['activity_id'][end]):\n",
    "            start_labeling = np.int(np.floor(start+(end-start)/2) - np.floor(label_window_size/2))\n",
    "            end_labeling = start_labeling + label_window_size\n",
    "            if speed_bucket_size == '0.1':\n",
    "                labels = np.append(labels,np.around(np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=1)) # round to nearest decimal\n",
    "            elif speed_bucket_size == '0.5':\n",
    "                labels = np.append(labels,np.around(2*np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling]),decimals=0)/2) # round to nearest half unit\n",
    "            elif speed_bucket_size == 'none_use_regression':\n",
    "                labels = np.append(labels,np.mean(data_full[\"gps_speed_true\"][start_labeling:end_labeling])) # no rounding, use regression\n",
    "    return labels\n",
    "\n",
    "def load_results_file_FCN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            num_hidden_fc_layers,\n",
    "            hidden_units_strategy,\n",
    "            activations_strategy,\n",
    "            dropout_rates\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 4\",\n",
    "                                    \"batch_size\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"num_hidden_fc_layers\",\n",
    "                                    \"hidden_units_strategy\",\n",
    "                                    \"activations_strategy\",\n",
    "                                    \"dropout_rates\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df\n",
    "\n",
    "def load_results_file_CNN(results_file_name):\n",
    "    my_file = Path(\"../Model Performance Tables/\" + results_file_name + \".csv\")\n",
    "    if my_file.is_file():\n",
    "        print(\"Found results file\")\n",
    "        prev_results=pd.read_csv(my_file,header=0)\n",
    "        print(list(prev_results.columns.values))\n",
    "        return prev_results\n",
    "    else:\n",
    "        print(\"no results file found - creating file\")\n",
    "        a=[[model_architecture,\n",
    "            file_name,\n",
    "            \"na\",\n",
    "            myFileLocation,\n",
    "            training_epochs,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            speed_bucket_size,\n",
    "            loss_function,\n",
    "            input_window_size,\n",
    "            label_window_size,\n",
    "            optimizer_type,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            hidden_units_strategy_CNN,\n",
    "            num_filters,\n",
    "            kernel_size,\n",
    "            sample_stride,\n",
    "            activation_conv_layer,\n",
    "            activations_strategy_CNN,\n",
    "            max_pool_kernel_size\n",
    "            ]]\n",
    "        \n",
    "        df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                    \"model filename\",\n",
    "                                    \"plot filename\",\n",
    "                                    \"data filename\",\n",
    "                                    \"epochs\",\n",
    "                                    \"runtime\",\n",
    "                                    \"dev accuracy 1\",\n",
    "                                    \"train accuracy 1\",\n",
    "                                    \"dev accuracy 2\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"dev accuracy 3\",\n",
    "                                    \"train accuracy 2\",\n",
    "                                    \"batch_size\",\n",
    "#                                     \"num_hidden_fc_layers\",\n",
    "#                                     \"activations_strategy\",\n",
    "#                                     \"dropout_rates\",\n",
    "                                    \"learning_rate\",\n",
    "                                    \"speed_bucket_size\",\n",
    "                                    \"loss_function\",\n",
    "                                    \"input_window_size\",\n",
    "                                    \"label_window_size\",\n",
    "                                    \"optimizer_type\",\n",
    "                                    \"evaluation_metric_1\",\n",
    "                                    \"evaluation_metric_2\",\n",
    "                                    \"evaluation_metric_3\",\n",
    "                                    \"hidden_units_strategy_CNN\",\n",
    "                                    \"num_filters\",\n",
    "                                    \"kernel_size\",\n",
    "                                    \"sample_stride\",\n",
    "                                    \"activation_conv_layer\",\n",
    "                                    \"activations_strategy_CNN\",\n",
    "                                    \"max_pool_kernel_size\"])\n",
    "        \n",
    "        df.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) \n",
    "        return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(myFileLocation)\n",
    "\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt'] # normalize all columns from gender to pelvic_tilt\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    dataset_inputs = dataset.loc[:, 'gender':'pelvic_tilt_lag_0'] # normalize all columns from gender to pelvic_tilt_lag_0\n",
    "    dataset_inputs_normalized = (dataset_inputs - dataset_inputs.mean())/dataset_inputs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_file_string_segment = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_segment.npy\"\n",
    "np_array_file_string_label = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label.npy\"\n",
    "np_array_file_string_label2num = \"../Saved NP Arrays/\" + str(myFileName) + \"_\" + str(input_window_size) + \"_\" + str(label_window_size) + \"_\" + str(sample_stride) + \"_label2num.npy\"\n",
    "\n",
    "if os.path.isfile(np_array_file_string_segment):   # if this file already exists, load the relevant saved np arrays\n",
    "    segments = np.load(np_array_file_string_segment, allow_pickle=True)\n",
    "    labels = np.load(np_array_file_string_label, allow_pickle=True)\n",
    "    labels_to_number = np.load(np_array_file_string_label2num, allow_pickle=True)\n",
    "else:    # if this file does not exist, run segment_signal method and create np arrays for future use\n",
    "    if data_input_table_structure == 'Raw_Timeseries':\n",
    "        segments, labels = segment_signal(dataset_inputs_normalized, dataset)\n",
    "    elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "        segments, labels = segment_signal_FCN_vector(dataset_inputs_normalized, dataset)\n",
    "    if speed_bucket_size != 'none_use_regression': # if not using regression, convert to one-hot vector labels\n",
    "         labels_to_number = np.unique(labels) # Caches \"labels_to_number\" in order to use in rmse calculation for classification\n",
    "         labels = np.asarray(pd.get_dummies(labels), dtype = np.int8) # one-hot labels to classify nearest bucket\n",
    "    np.save(np_array_file_string_segment, segments, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label, labels, allow_pickle=True)\n",
    "    np.save(np_array_file_string_label2num, labels_to_number, allow_pickle=True)\n",
    "\n",
    "num_buckets_total = len(labels[1]) # total number of classification buckets that exist in the dataset (here, classification bucket == classification class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data into training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_split = np.random.rand(len(segments)) < 0.90\n",
    "if data_input_table_structure == 'Raw_Timeseries':\n",
    "    reshapedSegments = segments.reshape(segments.shape[0], input_window_size, num_channels+num_anthropometrics, 1)\n",
    "    X_train = reshapedSegments[train_dev_split]\n",
    "    X_test = reshapedSegments[~train_dev_split]\n",
    "elif data_input_table_structure == 'Vectorized_By_Row':\n",
    "    X_train = segments[train_dev_split]\n",
    "    X_test = segments[~train_dev_split]\n",
    "y_train = labels[train_dev_split]\n",
    "y_test = labels[~train_dev_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127015,)\n",
      "(12820, 81)\n"
     ]
    }
   ],
   "source": [
    "print(train_dev_split.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement NN architecture in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnModel():\n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers[0], activation=activations_fc_layers[0], input_shape=(input_window_size*num_channels + num_anthropometrics,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate_fc_layers[0]))\n",
    "    # Intermediate layers\n",
    "    for L in range(1, num_hidden_fc_layers):\n",
    "        model.add(Dense(num_hidden_units_fc_layers[L], activation=activations_fc_layers[L]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model\n",
    "        \n",
    "def cnnModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_filters, (kernel_size,1),input_shape=(input_window_size, num_channels+num_anthropometrics,1),activation=activation_conv_layer))\n",
    "    model.add(MaxPooling2D(pool_size=(max_pool_kernel_size,1),padding='valid',strides=(2,1)))\n",
    "    model.add(Conv2D(num_filters//10, (kernel_size,1),activation=activation_conv_layer)) # add additional CNN layer\n",
    "    # model.add(Dropout(dropOutRatio)) # not used in our model # adding a dropout layer for the regularization\n",
    "    model.add(Flatten()) # flatten the output in order to apply the fully connected layer\n",
    "    model.add(Dense(num_hidden_units_fc_layers_CNN[0], activation=activations_fc_layers_CNN[0])) # add first fully connected layer\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(dropout_rate_fc_layers_CNN[0]))\n",
    "    # Intermediate fully connected layerslayers\n",
    "    for L in range(1, num_hidden_fc_layers_CNN):\n",
    "        model.add(Dense(num_hidden_units_fc_layers_CNN[L], activation=activations_fc_layers_CNN[L]))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_fc_layers_CNN[L]))\n",
    "    # Last hidden layer\n",
    "    if speed_bucket_size != 'none_use_regression': # if classification, use softmax for last layer\n",
    "        model.add(Dense(num_buckets_total, activation='softmax'))\n",
    "    else:                                          # if regression, use linear for last layer\n",
    "        model.add(Dense(1,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  model_architecture == 'FCN':\n",
    "    model = fcnModel()\n",
    "elif model_architecture == 'CNN':\n",
    "    model = cnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               20992     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 59)                7611      \n",
      "=================================================================\n",
      "Total params: 230,715\n",
      "Trainable params: 228,411\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-f9ff61bdbb0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Debuging helper code\n",
    "\n",
    "# print(segments[0,0,1:10])\n",
    "# print(labels.shape)\n",
    "# print(segments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss functions and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def class_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "    # Note: we cannot define RMSE directly in Keras since the loss function is defined for one training example at a time\n",
    "\n",
    "def class_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.sum(y_pred * labels_to_number,axis=-1,keepdims=True) - K.sum(y_true * labels_to_number,axis=-1,keepdims=True)), axis=-1)\n",
    "\n",
    "def class_mape(y_true, y_pred):\n",
    "    diff = K.abs((K.sum(y_true * labels_to_number,axis=-1,keepdims=True) - K.sum(y_pred * labels_to_number,axis=-1,keepdims=True)) / K.clip(K.abs(K.sum(y_true * labels_to_number,axis=-1,keepdims=True)),K.epsilon(),None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "def class_percent_1buckLow(y_true, y_pred): # percent of times the prediction is 1 bucket below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx())\n",
    "\n",
    "def class_percent_2buckLow(y_true, y_pred): # percent of times the prediction is 2 buckets below the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())\n",
    "    \n",
    "def class_percent_1buckHigh(y_true, y_pred): # percent of times the prediction is 1 bucket above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx())    \n",
    "\n",
    "def class_percent_2buckHigh(y_true, y_pred): # percent of times the prediction is 2 buckets above the true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx())    \n",
    "\n",
    "def class_percent_1buckRange(y_true, y_pred): # percent of times the prediction is within 1 bucket of true value\n",
    "    return K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())\n",
    "\n",
    "def class_percent_2buckRange(y_true, y_pred): # percent of times the prediction is within 2 buckets of true value\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+1.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())-2.0), K.floatx()) + \\\n",
    "    K.cast(K.equal(K.cast(K.argmax(y_true, axis=-1), K.floatx()), K.cast(K.argmax(y_pred, axis=-1),K.floatx())+2.0), K.floatx())    \n",
    "\n",
    "# For reference, from keras documentation: https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "#def class_categorical_accuracy(y_true, y_pred):\n",
    "    #return K.cast(K.equal(K.argmax(y_true, axis=-1),K.argmax(y_pred, axis=-1)),K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model loss and optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "if optimizer_type == 'adam':\n",
    "    model_optimizer = optimizers.Adam(lr = learning_rate) #, decay, beta_1, beta_2 are HPs\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    model_optimizer = optimizers.RMSprop(lr = learning_rate) #, decay, rho\n",
    "elif optimizer_type == 'gradient':\n",
    "    model_optimizer = optimizers.SGD(lr = learning_rate) #, decay, momentum\n",
    "\n",
    "# Compile model with appropriate loss function\n",
    "if speed_bucket_size != 'none_use_regression': # if performing classification, ALWAYS use cross-entropy loss\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer=model_optimizer, metrics=['accuracy',class_percent_1buckRange,class_percent_2buckRange, class_mae, class_mse]) # class_percent_1buckLow,class_percent_1buckHigh,class_percent_2buckLow, class_percent_2buckHigh,'class_mape'\n",
    "else:                                          # if performing regression, use mean squared error or mean absolute error\n",
    "    if loss_function == 'categorical_crossentropy': raise NameError('Are you sure you want to use cross entropy loss with a regression tasks!?')\n",
    "    model.compile(loss = loss_function, optimizer=model_optimizer, metrics=['mse','mae']) # options: 'mse','mae', 'mape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, load weights from a previous model to start with model\n",
    "\n",
    "if previous_model_weights_to_load != \"\":\n",
    "    model.load_weights(\"../Model Final Parameters/\" + previous_model_weights_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114195 samples, validate on 12820 samples\n",
      "Epoch 1/100\n",
      "114195/114195 [==============================] - 51s 449us/step - loss: 6.6496 - acc: 0.0182 - class_percent_1buckRange: 0.0560 - class_percent_2buckRange: 0.0918 - class_mae: 0.7194 - class_mse: 0.9021 - val_loss: 4.1775 - val_acc: 0.0022 - val_class_percent_1buckRange: 0.0050 - val_class_percent_2buckRange: 0.0087 - val_class_mae: 0.4100 - val_class_mse: 0.3500\n",
      "Epoch 2/100\n",
      "114195/114195 [==============================] - 49s 431us/step - loss: 6.5026 - acc: 0.0194 - class_percent_1buckRange: 0.0575 - class_percent_2buckRange: 0.0958 - class_mae: 0.7028 - class_mse: 0.8710 - val_loss: 4.1210 - val_acc: 0.0027 - val_class_percent_1buckRange: 0.0065 - val_class_percent_2buckRange: 0.0119 - val_class_mae: 0.4078 - val_class_mse: 0.3485\n",
      "Epoch 3/100\n",
      "114195/114195 [==============================] - 48s 424us/step - loss: 6.3926 - acc: 0.0202 - class_percent_1buckRange: 0.0587 - class_percent_2buckRange: 0.0980 - class_mae: 0.6900 - class_mse: 0.8453 - val_loss: 4.0717 - val_acc: 0.0033 - val_class_percent_1buckRange: 0.0083 - val_class_percent_2buckRange: 0.0146 - val_class_mae: 0.4063 - val_class_mse: 0.3475\n",
      "Epoch 4/100\n",
      "114195/114195 [==============================] - 51s 446us/step - loss: 6.2764 - acc: 0.0210 - class_percent_1buckRange: 0.0616 - class_percent_2buckRange: 0.1022 - class_mae: 0.6757 - class_mse: 0.8176 - val_loss: 4.0274 - val_acc: 0.0040 - val_class_percent_1buckRange: 0.0105 - val_class_percent_2buckRange: 0.0183 - val_class_mae: 0.4058 - val_class_mse: 0.3471\n",
      "Epoch 5/100\n",
      "114195/114195 [==============================] - 55s 480us/step - loss: 6.1636 - acc: 0.0222 - class_percent_1buckRange: 0.0646 - class_percent_2buckRange: 0.1060 - class_mae: 0.6633 - class_mse: 0.7966 - val_loss: 3.9835 - val_acc: 0.0057 - val_class_percent_1buckRange: 0.0152 - val_class_percent_2buckRange: 0.0246 - val_class_mae: 0.4046 - val_class_mse: 0.3454\n",
      "Epoch 6/100\n",
      "114195/114195 [==============================] - 51s 447us/step - loss: 6.0485 - acc: 0.0234 - class_percent_1buckRange: 0.0669 - class_percent_2buckRange: 0.1104 - class_mae: 0.6503 - class_mse: 0.7665 - val_loss: 3.9403 - val_acc: 0.0090 - val_class_percent_1buckRange: 0.0224 - val_class_percent_2buckRange: 0.0354 - val_class_mae: 0.4041 - val_class_mse: 0.3455\n",
      "Epoch 7/100\n",
      "114195/114195 [==============================] - 44s 389us/step - loss: 5.9406 - acc: 0.0232 - class_percent_1buckRange: 0.0694 - class_percent_2buckRange: 0.1138 - class_mae: 0.6402 - class_mse: 0.7506 - val_loss: 3.9030 - val_acc: 0.0121 - val_class_percent_1buckRange: 0.0334 - val_class_percent_2buckRange: 0.0527 - val_class_mae: 0.4038 - val_class_mse: 0.3450\n",
      "Epoch 8/100\n",
      "114195/114195 [==============================] - 45s 396us/step - loss: 5.8594 - acc: 0.0240 - class_percent_1buckRange: 0.0708 - class_percent_2buckRange: 0.1162 - class_mae: 0.6289 - class_mse: 0.7338 - val_loss: 3.8654 - val_acc: 0.0145 - val_class_percent_1buckRange: 0.0411 - val_class_percent_2buckRange: 0.0644 - val_class_mae: 0.4031 - val_class_mse: 0.3442\n",
      "Epoch 9/100\n",
      "114195/114195 [==============================] - 45s 391us/step - loss: 5.7683 - acc: 0.0249 - class_percent_1buckRange: 0.0748 - class_percent_2buckRange: 0.1215 - class_mae: 0.6194 - class_mse: 0.7133 - val_loss: 3.8318 - val_acc: 0.0229 - val_class_percent_1buckRange: 0.0665 - val_class_percent_2buckRange: 0.1015 - val_class_mae: 0.4024 - val_class_mse: 0.3432lass_percent_1buckRange: 0.0748 - class_percent_2buck\n",
      "Epoch 10/100\n",
      "114195/114195 [==============================] - 41s 356us/step - loss: 5.6743 - acc: 0.0264 - class_percent_1buckRange: 0.0776 - class_percent_2buckRange: 0.1272 - class_mae: 0.6081 - class_mse: 0.6965 - val_loss: 3.7958 - val_acc: 0.0355 - val_class_percent_1buckRange: 0.1032 - val_class_percent_2buckRange: 0.1541 - val_class_mae: 0.4020 - val_class_mse: 0.3423\n",
      "Epoch 11/100\n",
      "114195/114195 [==============================] - 39s 345us/step - loss: 5.6009 - acc: 0.0261 - class_percent_1buckRange: 0.0815 - class_percent_2buckRange: 0.1343 - class_mae: 0.5983 - class_mse: 0.6774 - val_loss: 3.7626 - val_acc: 0.0536 - val_class_percent_1buckRange: 0.1530 - val_class_percent_2buckRange: 0.2304 - val_class_mae: 0.4016 - val_class_mse: 0.3415\n",
      "Epoch 12/100\n",
      "114195/114195 [==============================] - 43s 378us/step - loss: 5.5017 - acc: 0.0292 - class_percent_1buckRange: 0.0867 - class_percent_2buckRange: 0.1406 - class_mae: 0.5894 - class_mse: 0.6610 - val_loss: 3.7296 - val_acc: 0.0773 - val_class_percent_1buckRange: 0.2228 - val_class_percent_2buckRange: 0.3377 - val_class_mae: 0.4002 - val_class_mse: 0.3396\n",
      "Epoch 13/100\n",
      "114195/114195 [==============================] - 40s 352us/step - loss: 5.4339 - acc: 0.0310 - class_percent_1buckRange: 0.0902 - class_percent_2buckRange: 0.1465 - class_mae: 0.5812 - class_mse: 0.6480 - val_loss: 3.6960 - val_acc: 0.0931 - val_class_percent_1buckRange: 0.2726 - val_class_percent_2buckRange: 0.4105 - val_class_mae: 0.3997 - val_class_mse: 0.3385\n",
      "Epoch 14/100\n",
      "114195/114195 [==============================] - 41s 361us/step - loss: 5.3621 - acc: 0.0332 - class_percent_1buckRange: 0.0958 - class_percent_2buckRange: 0.1540 - class_mae: 0.5701 - class_mse: 0.6268 - val_loss: 3.6717 - val_acc: 0.1012 - val_class_percent_1buckRange: 0.2941 - val_class_percent_2buckRange: 0.4466 - val_class_mae: 0.3988 - val_class_mse: 0.3376\n",
      "Epoch 15/100\n",
      "114195/114195 [==============================] - 42s 370us/step - loss: 5.2862 - acc: 0.0339 - class_percent_1buckRange: 0.0993 - class_percent_2buckRange: 0.1605 - class_mae: 0.5641 - class_mse: 0.6200 - val_loss: 3.6471 - val_acc: 0.1010 - val_class_percent_1buckRange: 0.2967 - val_class_percent_2buckRange: 0.4521 - val_class_mae: 0.3982 - val_class_mse: 0.3363\n",
      "Epoch 16/100\n",
      "114195/114195 [==============================] - 46s 406us/step - loss: 5.2124 - acc: 0.0354 - class_percent_1buckRange: 0.1023 - class_percent_2buckRange: 0.1678 - class_mae: 0.5561 - class_mse: 0.6029 - val_loss: 3.6173 - val_acc: 0.1013 - val_class_percent_1buckRange: 0.2979 - val_class_percent_2buckRange: 0.4525 - val_class_mae: 0.3974 - val_class_mse: 0.3350\n",
      "Epoch 17/100\n",
      "114195/114195 [==============================] - 62s 545us/step - loss: 5.1423 - acc: 0.0375 - class_percent_1buckRange: 0.1078 - class_percent_2buckRange: 0.1741 - class_mae: 0.5506 - class_mse: 0.5978 - val_loss: 3.5904 - val_acc: 0.1004 - val_class_percent_1buckRange: 0.2963 - val_class_percent_2buckRange: 0.4510 - val_class_mae: 0.3971 - val_class_mse: 0.3342\n",
      "Epoch 18/100\n",
      "114195/114195 [==============================] - 58s 511us/step - loss: 5.0930 - acc: 0.0390 - class_percent_1buckRange: 0.1147 - class_percent_2buckRange: 0.1852 - class_mae: 0.5434 - class_mse: 0.5840 - val_loss: 3.5661 - val_acc: 0.1001 - val_class_percent_1buckRange: 0.2938 - val_class_percent_2buckRange: 0.4510 - val_class_mae: 0.3962 - val_class_mse: 0.3330\n",
      "Epoch 19/100\n",
      "114195/114195 [==============================] - 58s 505us/step - loss: 5.0234 - acc: 0.0405 - class_percent_1buckRange: 0.1194 - class_percent_2buckRange: 0.1933 - class_mae: 0.5378 - class_mse: 0.5782 - val_loss: 3.5458 - val_acc: 0.0991 - val_class_percent_1buckRange: 0.2925 - val_class_percent_2buckRange: 0.4491 - val_class_mae: 0.3948 - val_class_mse: 0.3311\n",
      "Epoch 20/100\n",
      "114195/114195 [==============================] - 68s 595us/step - loss: 4.9563 - acc: 0.0436 - class_percent_1buckRange: 0.1259 - class_percent_2buckRange: 0.2005 - class_mae: 0.5284 - class_mse: 0.5592 - val_loss: 3.5191 - val_acc: 0.1002 - val_class_percent_1buckRange: 0.2936 - val_class_percent_2buckRange: 0.4519 - val_class_mae: 0.3942 - val_class_mse: 0.3302\n",
      "Epoch 21/100\n",
      "114195/114195 [==============================] - 43s 378us/step - loss: 4.9056 - acc: 0.0438 - class_percent_1buckRange: 0.1284 - class_percent_2buckRange: 0.2085 - class_mae: 0.5237 - class_mse: 0.5535 - val_loss: 3.5002 - val_acc: 0.0991 - val_class_percent_1buckRange: 0.2915 - val_class_percent_2buckRange: 0.4491 - val_class_mae: 0.3936 - val_class_mse: 0.3292\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114195/114195 [==============================] - 44s 385us/step - loss: 4.8439 - acc: 0.0469 - class_percent_1buckRange: 0.1362 - class_percent_2buckRange: 0.2201 - class_mae: 0.5195 - class_mse: 0.5493 - val_loss: 3.4747 - val_acc: 0.0998 - val_class_percent_1buckRange: 0.2924 - val_class_percent_2buckRange: 0.4509 - val_class_mae: 0.3923 - val_class_mse: 0.3274\n",
      "Epoch 23/100\n",
      "114195/114195 [==============================] - 45s 392us/step - loss: 4.7906 - acc: 0.0489 - class_percent_1buckRange: 0.1413 - class_percent_2buckRange: 0.2283 - class_mae: 0.5130 - class_mse: 0.5380 - val_loss: 3.4573 - val_acc: 0.0977 - val_class_percent_1buckRange: 0.2906 - val_class_percent_2buckRange: 0.4491 - val_class_mae: 0.3913 - val_class_mse: 0.3261\n",
      "Epoch 24/100\n",
      "114195/114195 [==============================] - 43s 378us/step - loss: 4.7489 - acc: 0.0497 - class_percent_1buckRange: 0.1468 - class_percent_2buckRange: 0.2361 - class_mae: 0.5073 - class_mse: 0.5290 - val_loss: 3.4376 - val_acc: 0.0990 - val_class_percent_1buckRange: 0.2918 - val_class_percent_2buckRange: 0.4505 - val_class_mae: 0.3900 - val_class_mse: 0.3244\n",
      "Epoch 25/100\n",
      "114195/114195 [==============================] - 45s 396us/step - loss: 4.6903 - acc: 0.0538 - class_percent_1buckRange: 0.1527 - class_percent_2buckRange: 0.2449 - class_mae: 0.5034 - class_mse: 0.5214 - val_loss: 3.4169 - val_acc: 0.0990 - val_class_percent_1buckRange: 0.2921 - val_class_percent_2buckRange: 0.4506 - val_class_mae: 0.3896 - val_class_mse: 0.3238\n",
      "Epoch 26/100\n",
      "114195/114195 [==============================] - 42s 372us/step - loss: 4.6298 - acc: 0.0531 - class_percent_1buckRange: 0.1578 - class_percent_2buckRange: 0.2535 - class_mae: 0.4993 - class_mse: 0.5153 - val_loss: 3.3972 - val_acc: 0.1000 - val_class_percent_1buckRange: 0.2945 - val_class_percent_2buckRange: 0.4530 - val_class_mae: 0.3885 - val_class_mse: 0.3224\n",
      "Epoch 27/100\n",
      "114195/114195 [==============================] - 46s 400us/step - loss: 4.5831 - acc: 0.0564 - class_percent_1buckRange: 0.1644 - class_percent_2buckRange: 0.2643 - class_mae: 0.4915 - class_mse: 0.5033 - val_loss: 3.3690 - val_acc: 0.1015 - val_class_percent_1buckRange: 0.2985 - val_class_percent_2buckRange: 0.4582 - val_class_mae: 0.3856 - val_class_mse: 0.3191\n",
      "Epoch 28/100\n",
      "114195/114195 [==============================] - 69s 608us/step - loss: 4.5390 - acc: 0.0590 - class_percent_1buckRange: 0.1704 - class_percent_2buckRange: 0.2709 - class_mae: 0.4895 - class_mse: 0.5034 - val_loss: 3.3573 - val_acc: 0.1011 - val_class_percent_1buckRange: 0.2970 - val_class_percent_2buckRange: 0.4559 - val_class_mae: 0.3863 - val_class_mse: 0.3198\n",
      "Epoch 29/100\n",
      "114195/114195 [==============================] - 54s 473us/step - loss: 4.4970 - acc: 0.0598 - class_percent_1buckRange: 0.1728 - class_percent_2buckRange: 0.2782 - class_mae: 0.4856 - class_mse: 0.4947 - val_loss: 3.3395 - val_acc: 0.1010 - val_class_percent_1buckRange: 0.2965 - val_class_percent_2buckRange: 0.4563 - val_class_mae: 0.3846 - val_class_mse: 0.3177\n",
      "Epoch 30/100\n",
      "114195/114195 [==============================] - 54s 475us/step - loss: 4.4482 - acc: 0.0612 - class_percent_1buckRange: 0.1789 - class_percent_2buckRange: 0.2855 - class_mae: 0.4807 - class_mse: 0.4893 - val_loss: 3.3206 - val_acc: 0.1005 - val_class_percent_1buckRange: 0.2966 - val_class_percent_2buckRange: 0.4564 - val_class_mae: 0.3834 - val_class_mse: 0.3164\n",
      "Epoch 31/100\n",
      "114195/114195 [==============================] - 51s 445us/step - loss: 4.4145 - acc: 0.0620 - class_percent_1buckRange: 0.1840 - class_percent_2buckRange: 0.2940 - class_mae: 0.4786 - class_mse: 0.4853 - val_loss: 3.3070 - val_acc: 0.0998 - val_class_percent_1buckRange: 0.2945 - val_class_percent_2buckRange: 0.4534 - val_class_mae: 0.3837 - val_class_mse: 0.3165\n",
      "Epoch 32/100\n",
      "114195/114195 [==============================] - 53s 466us/step - loss: 4.3634 - acc: 0.0645 - class_percent_1buckRange: 0.1890 - class_percent_2buckRange: 0.3015 - class_mae: 0.4727 - class_mse: 0.4747 - val_loss: 3.2884 - val_acc: 0.1014 - val_class_percent_1buckRange: 0.2970 - val_class_percent_2buckRange: 0.4572 - val_class_mae: 0.3811 - val_class_mse: 0.3138\n",
      "Epoch 33/100\n",
      "114195/114195 [==============================] - 56s 486us/step - loss: 4.3200 - acc: 0.0666 - class_percent_1buckRange: 0.1938 - class_percent_2buckRange: 0.3096 - class_mae: 0.4675 - class_mse: 0.4659 - val_loss: 3.2712 - val_acc: 0.1002 - val_class_percent_1buckRange: 0.2948 - val_class_percent_2buckRange: 0.4546 - val_class_mae: 0.3810 - val_class_mse: 0.3135\n",
      "Epoch 34/100\n",
      "114195/114195 [==============================] - 52s 458us/step - loss: 4.2909 - acc: 0.0678 - class_percent_1buckRange: 0.1991 - class_percent_2buckRange: 0.3172 - class_mae: 0.4659 - class_mse: 0.4650 - val_loss: 3.2562 - val_acc: 0.1004 - val_class_percent_1buckRange: 0.2960 - val_class_percent_2buckRange: 0.4563 - val_class_mae: 0.3792 - val_class_mse: 0.3116\n",
      "Epoch 35/100\n",
      "114195/114195 [==============================] - 55s 485us/step - loss: 4.2404 - acc: 0.0681 - class_percent_1buckRange: 0.2012 - class_percent_2buckRange: 0.3217 - class_mae: 0.4618 - class_mse: 0.4595 - val_loss: 3.2378 - val_acc: 0.1007 - val_class_percent_1buckRange: 0.2970 - val_class_percent_2buckRange: 0.4590 - val_class_mae: 0.3766 - val_class_mse: 0.3091\n",
      "Epoch 36/100\n",
      "114195/114195 [==============================] - 55s 480us/step - loss: 4.2100 - acc: 0.0701 - class_percent_1buckRange: 0.2062 - class_percent_2buckRange: 0.3282 - class_mae: 0.4581 - class_mse: 0.4511 - val_loss: 3.2271 - val_acc: 0.1003 - val_class_percent_1buckRange: 0.2973 - val_class_percent_2buckRange: 0.4587 - val_class_mae: 0.3768 - val_class_mse: 0.3091\n",
      "Epoch 37/100\n",
      "114195/114195 [==============================] - 53s 463us/step - loss: 4.1625 - acc: 0.0718 - class_percent_1buckRange: 0.2099 - class_percent_2buckRange: 0.3350 - class_mae: 0.4549 - class_mse: 0.4479 - val_loss: 3.2129 - val_acc: 0.1013 - val_class_percent_1buckRange: 0.2986 - val_class_percent_2buckRange: 0.4607 - val_class_mae: 0.3755 - val_class_mse: 0.3079\n",
      "Epoch 38/100\n",
      "114195/114195 [==============================] - 53s 464us/step - loss: 4.1321 - acc: 0.0732 - class_percent_1buckRange: 0.2133 - class_percent_2buckRange: 0.3393 - class_mae: 0.4510 - class_mse: 0.4426 - val_loss: 3.2011 - val_acc: 0.1011 - val_class_percent_1buckRange: 0.2986 - val_class_percent_2buckRange: 0.4612 - val_class_mae: 0.3751 - val_class_mse: 0.3075\n",
      "Epoch 39/100\n",
      "114195/114195 [==============================] - 52s 457us/step - loss: 4.0923 - acc: 0.0750 - class_percent_1buckRange: 0.2184 - class_percent_2buckRange: 0.3459 - class_mae: 0.4480 - class_mse: 0.4368 - val_loss: 3.1841 - val_acc: 0.1019 - val_class_percent_1buckRange: 0.3004 - val_class_percent_2buckRange: 0.4630 - val_class_mae: 0.3731 - val_class_mse: 0.3053\n",
      "Epoch 40/100\n",
      "114195/114195 [==============================] - 56s 493us/step - loss: 4.0539 - acc: 0.0760 - class_percent_1buckRange: 0.2216 - class_percent_2buckRange: 0.3520 - class_mae: 0.4454 - class_mse: 0.4326 - val_loss: 3.1715 - val_acc: 0.1016 - val_class_percent_1buckRange: 0.3016 - val_class_percent_2buckRange: 0.4648 - val_class_mae: 0.3726 - val_class_mse: 0.3049\n",
      "Epoch 41/100\n",
      "114195/114195 [==============================] - 54s 477us/step - loss: 4.0233 - acc: 0.0753 - class_percent_1buckRange: 0.2233 - class_percent_2buckRange: 0.3562 - class_mae: 0.4433 - class_mse: 0.4309 - val_loss: 3.1613 - val_acc: 0.1016 - val_class_percent_1buckRange: 0.3025 - val_class_percent_2buckRange: 0.4662 - val_class_mae: 0.3719 - val_class_mse: 0.3041\n",
      "Epoch 42/100\n",
      "114195/114195 [==============================] - 55s 485us/step - loss: 3.9937 - acc: 0.0782 - class_percent_1buckRange: 0.2258 - class_percent_2buckRange: 0.3591 - class_mae: 0.4399 - class_mse: 0.4249 - val_loss: 3.1458 - val_acc: 0.1020 - val_class_percent_1buckRange: 0.3043 - val_class_percent_2buckRange: 0.4681 - val_class_mae: 0.3700 - val_class_mse: 0.3022\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114195/114195 [==============================] - 56s 488us/step - loss: 3.9620 - acc: 0.0779 - class_percent_1buckRange: 0.2298 - class_percent_2buckRange: 0.3653 - class_mae: 0.4373 - class_mse: 0.4236 - val_loss: 3.1344 - val_acc: 0.1016 - val_class_percent_1buckRange: 0.3044 - val_class_percent_2buckRange: 0.4687 - val_class_mae: 0.3691 - val_class_mse: 0.3012\n",
      "Epoch 44/100\n",
      "114195/114195 [==============================] - 54s 473us/step - loss: 3.9347 - acc: 0.0802 - class_percent_1buckRange: 0.2325 - class_percent_2buckRange: 0.3695 - class_mae: 0.4350 - class_mse: 0.4206 - val_loss: 3.1243 - val_acc: 0.1022 - val_class_percent_1buckRange: 0.3056 - val_class_percent_2buckRange: 0.4702 - val_class_mae: 0.3683 - val_class_mse: 0.3004\n",
      "Epoch 45/100\n",
      "114195/114195 [==============================] - 55s 485us/step - loss: 3.8977 - acc: 0.0797 - class_percent_1buckRange: 0.2364 - class_percent_2buckRange: 0.3733 - class_mae: 0.4319 - class_mse: 0.4149 - val_loss: 3.1098 - val_acc: 0.1020 - val_class_percent_1buckRange: 0.3053 - val_class_percent_2buckRange: 0.4699 - val_class_mae: 0.3673 - val_class_mse: 0.2995\n",
      "Epoch 46/100\n",
      "114195/114195 [==============================] - 54s 476us/step - loss: 3.8666 - acc: 0.0817 - class_percent_1buckRange: 0.2375 - class_percent_2buckRange: 0.3756 - class_mae: 0.4293 - class_mse: 0.4117 - val_loss: 3.1018 - val_acc: 0.1017 - val_class_percent_1buckRange: 0.3049 - val_class_percent_2buckRange: 0.4695 - val_class_mae: 0.3670 - val_class_mse: 0.2991\n",
      "Epoch 47/100\n",
      "114195/114195 [==============================] - 58s 510us/step - loss: 3.8363 - acc: 0.0825 - class_percent_1buckRange: 0.2414 - class_percent_2buckRange: 0.3827 - class_mae: 0.4262 - class_mse: 0.4070 - val_loss: 3.0912 - val_acc: 0.1023 - val_class_percent_1buckRange: 0.3049 - val_class_percent_2buckRange: 0.4705 - val_class_mae: 0.3664 - val_class_mse: 0.2982\n",
      "Epoch 48/100\n",
      "114195/114195 [==============================] - 60s 524us/step - loss: 3.8063 - acc: 0.0849 - class_percent_1buckRange: 0.2414 - class_percent_2buckRange: 0.3855 - class_mae: 0.4246 - class_mse: 0.4054 - val_loss: 3.0806 - val_acc: 0.1026 - val_class_percent_1buckRange: 0.3057 - val_class_percent_2buckRange: 0.4721 - val_class_mae: 0.3652 - val_class_mse: 0.2969\n",
      "Epoch 49/100\n",
      "114195/114195 [==============================] - 56s 487us/step - loss: 3.7842 - acc: 0.0848 - class_percent_1buckRange: 0.2457 - class_percent_2buckRange: 0.3886 - class_mae: 0.4219 - class_mse: 0.4018 - val_loss: 3.0737 - val_acc: 0.1020 - val_class_percent_1buckRange: 0.3041 - val_class_percent_2buckRange: 0.4701 - val_class_mae: 0.3663 - val_class_mse: 0.2978\n",
      "Epoch 50/100\n",
      "114195/114195 [==============================] - 58s 504us/step - loss: 3.7452 - acc: 0.0855 - class_percent_1buckRange: 0.2498 - class_percent_2buckRange: 0.3944 - class_mae: 0.4178 - class_mse: 0.3954 - val_loss: 3.0621 - val_acc: 0.1030 - val_class_percent_1buckRange: 0.3056 - val_class_percent_2buckRange: 0.4725 - val_class_mae: 0.3647 - val_class_mse: 0.2964\n",
      "Epoch 51/100\n",
      "114195/114195 [==============================] - 55s 485us/step - loss: 3.7316 - acc: 0.0847 - class_percent_1buckRange: 0.2478 - class_percent_2buckRange: 0.3929 - class_mae: 0.4174 - class_mse: 0.3941 - val_loss: 3.0546 - val_acc: 0.1034 - val_class_percent_1buckRange: 0.3064 - val_class_percent_2buckRange: 0.4718 - val_class_mae: 0.3648 - val_class_mse: 0.2962\n",
      "Epoch 52/100\n",
      "114195/114195 [==============================] - 53s 464us/step - loss: 3.6938 - acc: 0.0870 - class_percent_1buckRange: 0.2532 - class_percent_2buckRange: 0.3999 - class_mae: 0.4142 - class_mse: 0.3899 - val_loss: 3.0455 - val_acc: 0.1033 - val_class_percent_1buckRange: 0.3076 - val_class_percent_2buckRange: 0.4743 - val_class_mae: 0.3640 - val_class_mse: 0.2956\n",
      "Epoch 53/100\n",
      "114195/114195 [==============================] - 55s 484us/step - loss: 3.6737 - acc: 0.0875 - class_percent_1buckRange: 0.2564 - class_percent_2buckRange: 0.4034 - class_mae: 0.4122 - class_mse: 0.3868 - val_loss: 3.0353 - val_acc: 0.1042 - val_class_percent_1buckRange: 0.3100 - val_class_percent_2buckRange: 0.4774 - val_class_mae: 0.3627 - val_class_mse: 0.2944\n",
      "Epoch 54/100\n",
      "114195/114195 [==============================] - 55s 479us/step - loss: 3.6472 - acc: 0.0878 - class_percent_1buckRange: 0.2549 - class_percent_2buckRange: 0.4041 - class_mae: 0.4117 - class_mse: 0.3879 - val_loss: 3.0228 - val_acc: 0.1052 - val_class_percent_1buckRange: 0.3109 - val_class_percent_2buckRange: 0.4796 - val_class_mae: 0.3609 - val_class_mse: 0.2929\n",
      "Epoch 55/100\n",
      "114195/114195 [==============================] - 57s 500us/step - loss: 3.6239 - acc: 0.0894 - class_percent_1buckRange: 0.2586 - class_percent_2buckRange: 0.4078 - class_mae: 0.4082 - class_mse: 0.3816 - val_loss: 3.0108 - val_acc: 0.1074 - val_class_percent_1buckRange: 0.3154 - val_class_percent_2buckRange: 0.4862 - val_class_mae: 0.3587 - val_class_mse: 0.2909\n",
      "Epoch 56/100\n",
      "114195/114195 [==============================] - 56s 492us/step - loss: 3.5990 - acc: 0.0902 - class_percent_1buckRange: 0.2610 - class_percent_2buckRange: 0.4122 - class_mae: 0.4063 - class_mse: 0.3791 - val_loss: 3.0075 - val_acc: 0.1068 - val_class_percent_1buckRange: 0.3153 - val_class_percent_2buckRange: 0.4860 - val_class_mae: 0.3595 - val_class_mse: 0.2913\n",
      "Epoch 57/100\n",
      "114195/114195 [==============================] - 58s 512us/step - loss: 3.5748 - acc: 0.0919 - class_percent_1buckRange: 0.2653 - class_percent_2buckRange: 0.4173 - class_mae: 0.4043 - class_mse: 0.3774 - val_loss: 2.9996 - val_acc: 0.1066 - val_class_percent_1buckRange: 0.3149 - val_class_percent_2buckRange: 0.4860 - val_class_mae: 0.3587 - val_class_mse: 0.2905\n",
      "Epoch 58/100\n",
      "114195/114195 [==============================] - 61s 536us/step - loss: 3.5491 - acc: 0.0915 - class_percent_1buckRange: 0.2654 - class_percent_2buckRange: 0.4197 - class_mae: 0.4015 - class_mse: 0.3720 - val_loss: 2.9904 - val_acc: 0.1072 - val_class_percent_1buckRange: 0.3172 - val_class_percent_2buckRange: 0.4893 - val_class_mae: 0.3576 - val_class_mse: 0.2897\n",
      "Epoch 59/100\n",
      "114195/114195 [==============================] - 61s 536us/step - loss: 3.5340 - acc: 0.0921 - class_percent_1buckRange: 0.2674 - class_percent_2buckRange: 0.4201 - class_mae: 0.4001 - class_mse: 0.3698 - val_loss: 2.9862 - val_acc: 0.1068 - val_class_percent_1buckRange: 0.3179 - val_class_percent_2buckRange: 0.4903 - val_class_mae: 0.3579 - val_class_mse: 0.2898\n",
      "Epoch 60/100\n",
      "114195/114195 [==============================] - 60s 522us/step - loss: 3.5130 - acc: 0.0923 - class_percent_1buckRange: 0.2699 - class_percent_2buckRange: 0.4262 - class_mae: 0.3983 - class_mse: 0.3696 - val_loss: 2.9751 - val_acc: 0.1073 - val_class_percent_1buckRange: 0.3199 - val_class_percent_2buckRange: 0.4934 - val_class_mae: 0.3565 - val_class_mse: 0.2885\n",
      "Epoch 61/100\n",
      " 23800/114195 [=====>........................] - ETA: 43s - loss: 3.4879 - acc: 0.0929 - class_percent_1buckRange: 0.2697 - class_percent_2buckRange: 0.4245 - class_mae: 0.3977 - class_mse: 0.3705"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size= batch_size, epochs=training_epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a plot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform key results into a np arrary\n",
    "trainAccuracy_1 = np.squeeze(history.history[accuracy_reporting_metric_1])\n",
    "devAccuracy_1 = np.squeeze(history.history[dev_reporting_metric_1])\n",
    "trainAccuracy_2 = np.squeeze(history.history[accuracy_reporting_metric_2])\n",
    "devAccuracy_2 = np.squeeze(history.history[dev_reporting_metric_2])    \n",
    "trainAccuracy_3 = np.squeeze(history.history[accuracy_reporting_metric_3])\n",
    "devAccuracy_3 = np.squeeze(history.history[dev_reporting_metric_3])\n",
    "trainAccuracy_4 = np.squeeze(history.history['acc'])\n",
    "devAccuracy_4 = np.squeeze(history.history['val_acc'])\n",
    "epochs = np.squeeze(range(1,training_epochs + 1))\n",
    "\n",
    "# Save results to a .csv in the \"Learning Curve Results\"\n",
    "df_devAccuracy = pd.DataFrame(np.transpose(np.vstack([epochs,devAccuracy_1, devAccuracy_2, devAccuracy_3, devAccuracy_4])))\n",
    "filepath_acc = \"../Learning Curves/\" + str(file_name) +\"_AccuracyPerEpoch_Data\" + \".csv\"\n",
    "df_devAccuracy.to_csv(filepath_acc, header = [\"Epochs\", dev_reporting_metric_1, dev_reporting_metric_2, dev_reporting_metric_3, 'acc'], index=False)\n",
    "\n",
    "# Declare final values for results\n",
    "final_accuracy_1 = history.history[accuracy_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_dev_1 = history.history[dev_reporting_metric_1][training_epochs - 1]\n",
    "final_accuracy_2 = history.history[accuracy_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_dev_2 = history.history[dev_reporting_metric_2][training_epochs - 1]\n",
    "final_accuracy_3 = history.history[accuracy_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_dev_3 = history.history[dev_reporting_metric_3][training_epochs - 1]\n",
    "final_accuracy_4 = history.history['acc'][training_epochs - 1]\n",
    "final_accuracy_dev_4 = history.history['val_acc'][training_epochs - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "lines=[]\n",
    "\n",
    "lines += ax1.plot(trainAccuracy_4,'#0e128c', label='Train Accuracy 2', linewidth=1) #'#DAF7A6'\n",
    "lines += ax1.plot(devAccuracy_4,'#a3a4cc', label='Dev Accuracy 2', linewidth=1)# '#33FF00',\n",
    "lines += ax1.plot(trainAccuracy_1,'#FF5733', label='Train Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_1,'#C70039', label='Dev Accuracy 1', linewidth=1)\n",
    "lines += ax1.plot(trainAccuracy_2,'#9C27B0', label='Train Accuracy 2', linewidth=1)\n",
    "lines += ax1.plot(devAccuracy_2,'#7986CB', label='Dev Accuracy 2', linewidth=1)\n",
    "plt.ylim([0.0, 1.0])  # Surpress this for non-classification tasks\n",
    "\n",
    "plt.ylabel('Train vs. Dev Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(dev_reporting_metric_1 + \": \" + str(np.around(final_accuracy_dev_1,4)) + \"\\n\" + \\\n",
    "         dev_reporting_metric_2 + \": \" + str(np.around(final_accuracy_dev_2,4))) \n",
    "extra = Rectangle((0, 0), 1, 1, fc=\"w\", fill=False, edgecolor='none', linewidth=0)\n",
    "plt.legend([extra,extra,extra,extra,extra,extra,extra,extra,extra,extra],(\n",
    "                                                \"loss: \" + loss_function,\n",
    "                                                \"learning rate: \" + str(learning_rate),\n",
    "                                                \"batch_size: \" + str(batch_size),\n",
    "                                                \"speed_bucket_size: \" + speed_bucket_size,\n",
    "                                                \"epochs: \"+str(training_epochs),\n",
    "                                                \"input_window_size: \" + str(input_window_size),\n",
    "                                                \"num_labels: \" + str(len(labels_to_number)),\n",
    "                                                \"evaluation metric 1:\"+accuracy_reporting_metric_1,\n",
    "                                                \"evaluation metric 2:\"+accuracy_reporting_metric_2,\n",
    "                                                \"evaluation metric 3:\"+accuracy_reporting_metric_3,\n",
    "                                                \"note:\" + plot_note),\n",
    "                                                bbox_to_anchor=(1.05, 1),\n",
    "                                                loc=2,\n",
    "                                                borderaxespad=0.)\n",
    "\n",
    "leg = Legend(ax1, lines[0:], ['Train ACC', 'Dev ACC','Train Eval 1','Dev Eval 1'],\n",
    "             loc='bestoutside', frameon=False)\n",
    "ax1.add_artist(leg);\n",
    "plt.savefig(\"../Learning Curves/\" + str(file_name) + \"_AccuracyPerEpoch_Image.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record results of a model in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the most recent run to the results file for documentation\n",
    "\n",
    "if  model_architecture == 'FCN':\n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,  \n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,    \n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        num_hidden_fc_layers,\n",
    "        hidden_units_strategy,\n",
    "        activations_strategy,\n",
    "        dropout_rates\n",
    "       ]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"batch_size\",  \n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"num_hidden_fc_layers\",\n",
    "                                \"hidden_units_strategy\",\n",
    "                                \"activations_strategy\",\n",
    "                                \"dropout_rates\"])\n",
    "    past_results = load_results_file_FCN(results_file_name)\n",
    "elif model_architecture == 'CNN':     \n",
    "    a=[[model_architecture,\n",
    "        file_name,\n",
    "        \"na\",\n",
    "        myFileLocation,\n",
    "        training_epochs,\n",
    "        end_time - start_time,\n",
    "        final_accuracy_1,\n",
    "        final_accuracy_dev_1,\n",
    "        final_accuracy_2,\n",
    "        final_accuracy_dev_2,\n",
    "        final_accuracy_3,\n",
    "        final_accuracy_dev_3,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        speed_bucket_size,\n",
    "        loss_function,\n",
    "        input_window_size,\n",
    "        label_window_size,\n",
    "        optimizer_type,\n",
    "        accuracy_reporting_metric_1,\n",
    "        accuracy_reporting_metric_2,\n",
    "        accuracy_reporting_metric_3,\n",
    "        hidden_units_strategy_CNN,\n",
    "        num_filters,\n",
    "        kernel_size,\n",
    "        sample_stride,\n",
    "        activation_conv_layer,\n",
    "        activations_strategy_CNN,\n",
    "        max_pool_kernel_size]]\n",
    "    df=pd.DataFrame(a, columns=[\"model type\",\n",
    "                                \"model filename\",\n",
    "                                \"plot filename\",\n",
    "                                \"data filename\",\n",
    "                                \"epochs\",\n",
    "                                \"runtime\",\n",
    "                                \"dev accuracy 1\",\n",
    "                                \"train accuracy 1\",\n",
    "                                \"dev accuracy 2\",\n",
    "                                \"train accuracy 2\",\n",
    "                                \"dev accuracy 3\",\n",
    "                                \"train accuracy 3\",\n",
    "                                \"batch_size\",\n",
    "                                \"learning_rate\",\n",
    "                                \"speed_bucket_size\",\n",
    "                                \"loss_function\",\n",
    "                                \"input_window_size\",\n",
    "                                \"label_window_size\",\n",
    "                                \"optimizer_type\",\n",
    "                                \"evaluation_metric_1\",\n",
    "                                \"evaluation_metric_2\",\n",
    "                                \"evaluation_metric_3\",\n",
    "                                \"hidden_units_strategy_CNN\",\n",
    "                                \"num_filters\",\n",
    "                                \"kernel_size\",\n",
    "                                \"sample_stride\",\n",
    "                                \"activation_conv_layer\",\n",
    "                                \"activations_strategy_CNN\",\n",
    "                                \"max_pool_kernel_size\"])    \n",
    "    past_results = load_results_file_CNN(results_file_name)\n",
    "past_results=pd.concat([past_results,df])\n",
    "print(past_results)\n",
    "past_results.to_csv(\"../Model Performance Tables/\" + results_file_name + \".csv\",index=False ) # Consider fixing to put the columns in not-alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Confusion Matrix and Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = y_test\n",
    "y_true_argmax = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.scatter(y_true_argmax, y_pred_argmax, s=3, alpha=0.3)\n",
    "plt.scatter(y_true_argmax, y_true_argmax, s=3, alpha=1)\n",
    "#plt.scatter(y_true, y_pred, s=3, alpha=0.3) # For regression\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "plt.xlabel('Y_True')\n",
    "plt.ylabel('Y_Prediction')\n",
    "plt.savefig(\"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Image.png\")\n",
    "plt.show()\n",
    "\n",
    "# Record data in a .csv\n",
    "y_trueVy_pred = np.vstack([y_true_argmax,y_pred_argmax])\n",
    "df_y_trueVy_pred = pd.DataFrame(np.transpose(y_trueVy_pred))\n",
    "filepath_predictions = \"../Model Final Predictions/\" + str(file_name) + \"_Predictions\" + \".csv\"\n",
    "df_y_trueVy_pred.to_csv(filepath_predictions, header = [\"y_true_argmax\", \"y_pred_argmax\"], index=False)\n",
    "\n",
    "# Create and save a confusion matrix\n",
    "cm = confusion_matrix(y_true_argmax, y_pred_argmax)\n",
    "df_cm = pd.DataFrame (cm)\n",
    "filepath_cm = \"../Confusion Matrices/\" + str(file_name) + \"_ConfusionMatrix_Data.xlsx\"\n",
    "df_cm.to_excel(filepath_cm, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model parameters (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_model_name = file_name + \"_\" + model_architecture\n",
    "model.save_weights(\"../Model Final Parameters/\" + completed_model_name + '_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
